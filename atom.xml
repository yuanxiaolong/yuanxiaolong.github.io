<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title><![CDATA[Sam小龙]]></title>
  <subtitle><![CDATA[最爱碧颖]]></subtitle>
  <link href="/atom.xml" rel="self"/>
  <link href="http://yoursite.com/"/>
  <updated>2016-07-06T13:21:24.000Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name><![CDATA[yuanxiaolong]]></name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title><![CDATA[parquet data storeage]]></title>
    <link href="http://yoursite.com/blog/2016/07/06/parquet-data-storeage/"/>
    <id>http://yoursite.com/blog/2016/07/06/parquet-data-storeage/</id>
    <published>2016-07-06T13:19:30.000Z</published>
    <updated>2016-07-06T13:21:24.000Z</updated>
    <content type="html"><![CDATA[<p>本文介绍如何将 textfile 转换成为 parquetfile 的过程</p>
<a id="more"></a>
<p>parquet 格式在 impala 中使用效率奇高，本身结合hive使用也十分快。因此转换成 parquet存储格式是十分有必要的。</p>
<p>转换的方式有2种：<br>1.将原始的 textfile 转换成hive的外部表，再从hive中 insert overwrite into <your-parquet-table> select cols from <your-textfile-table><br>2.将原始textfile文件，通过MR程序，先转换成parquet文件，再用hive外部表挂载到此文件上，或其他应用方式。</your-textfile-table></your-parquet-table></p>
<p>第一种方法较方便，不做介绍。第二种较复杂，但每次转换的量是可控的，所以也有应用场景。</p>
<h2 id="u83B7_u53D6parquet_schema"><a href="#u83B7_u53D6parquet_schema" class="headerlink" title="获取parquet schema"></a>获取parquet schema</h2><p>1.有textfile，则先将textfile获取几条数据，insert到textfile hive表中，然后再利用第一种方法，转成parquet hive表。因为数据不是很多，所以转换很快。</p>
<p>2.在 git上获取工具 parquet-tools</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/apache/parquet-mr.git&#10;git checkout parquet-1.5.0</span><br></pre></td></tr></table></figure>
<p>3.修改顶层pom.xml 的hadoop版本跟自己的版本一致，然后注释掉 Twitter 仓库，加快下载速度</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#60;!--&#10;&#60;pluginRepositories&#62;&#10;    &#60;pluginRepository&#62;&#10;      &#60;id&#62;Twitter public Maven repo&#60;/id&#62;&#10;      &#60;url&#62;http://maven.twttr.com&#60;/url&#62;&#10;    &#60;/pluginRepository&#62;&#10;  &#60;/pluginRepositories&#62;&#10;&#8212;&#62;</span><br></pre></td></tr></table></figure>
<p>4.编译子工程（如果添加-Plocal则表示读取本地文件，如果不加，则可以读取hdfs文件，视情况而定）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd ./parquet-tools&#10;mvn clean package [-Plocal]</span><br></pre></td></tr></table></figure>
<p>5.成功后 解压并执行文件</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar zxf parquet-tools-1.5.0-bin.tar.gz &#38;&#38; cd parquet-tools-1.5.0</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./parquet-schema /Users/mfw/Downloads/tmp/front_access_pa2/dt=20160101/000000_0&#10;message hive_schema &#123;&#10;  optional binary remote_addr (UTF8);&#10;  optional binary upstream_addr (UTF8);&#10;  optional binary http_x_forwarded_for (UTF8);&#10;  optional binary visit_time (UTF8);&#10;  optional binary request_uri (UTF8);&#10;  optional binary request_method (UTF8);&#10;  optional binary server_protocol (UTF8);&#10;  optional int32 status;&#10;  optional int32 body_bytes_sent;&#10;  optional float request_time;&#10;  optional int64 uid;&#10;  optional binary uuid (UTF8);&#10;  optional binary user_agent (UTF8);&#10;  optional binary refer (UTF8);&#10;  optional binary request_body (UTF8);&#10;&#125;</span><br></pre></td></tr></table></figure>
<p>这里我们就获取到了 parquet schema 的结构 其中 <code>hive_schema</code> 可以随意写。<br>值得注意的是，为了简便于我们后面 mapreduce的编码，建议把这里的 int float 等都换成 binary ，然后对应的hive表的字段都用 string类型</p>
<h2 id="u7F16_u5199mr"><a href="#u7F16_u5199mr" class="headerlink" title="编写mr"></a>编写mr</h2><p>1.先在pom.xml中添加依赖，将工程打包成包含依赖的 fat jar</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#60;dependencies&#62;&#10;&#10;        &#60;!-- hadoop --&#62;&#10;        &#60;dependency&#62;&#10;            &#60;groupId&#62;org.apache.hadoop&#60;/groupId&#62;&#10;            &#60;artifactId&#62;hadoop-common&#60;/artifactId&#62;&#10;            &#60;version&#62;2.6.0&#60;/version&#62;&#10;        &#60;/dependency&#62;&#10;&#10;        &#60;dependency&#62;&#10;            &#60;groupId&#62;org.apache.hadoop&#60;/groupId&#62;&#10;            &#60;artifactId&#62;hadoop-hdfs&#60;/artifactId&#62;&#10;            &#60;version&#62;2.6.0&#60;/version&#62;&#10;        &#60;/dependency&#62;&#10;&#10;        &#60;dependency&#62;&#10;            &#60;groupId&#62;org.apache.hadoop&#60;/groupId&#62;&#10;            &#60;artifactId&#62;hadoop-client&#60;/artifactId&#62;&#10;            &#60;version&#62;2.6.0&#60;/version&#62;&#10;        &#60;/dependency&#62;&#10;&#10;        &#60;!-- parquet --&#62;&#10;        &#60;dependency&#62;&#10;            &#60;groupId&#62;com.twitter&#60;/groupId&#62;&#10;            &#60;artifactId&#62;parquet-hadoop&#60;/artifactId&#62;&#10;            &#60;version&#62;1.5.0&#60;/version&#62;&#10;        &#60;/dependency&#62;&#10;        &#60;dependency&#62;&#10;            &#60;groupId&#62;com.twitter&#60;/groupId&#62;&#10;            &#60;artifactId&#62;parquet-column&#60;/artifactId&#62;&#10;            &#60;version&#62;1.5.0&#60;/version&#62;&#10;        &#60;/dependency&#62;&#10;        &#60;dependency&#62;&#10;            &#60;groupId&#62;com.twitter&#60;/groupId&#62;&#10;            &#60;artifactId&#62;parquet-common&#60;/artifactId&#62;&#10;            &#60;version&#62;1.5.0&#60;/version&#62;&#10;        &#60;/dependency&#62;&#10;        &#60;dependency&#62;&#10;            &#60;groupId&#62;com.twitter&#60;/groupId&#62;&#10;            &#60;artifactId&#62;parquet-format&#60;/artifactId&#62;&#10;            &#60;version&#62;2.1.0&#60;/version&#62;&#10;        &#60;/dependency&#62;&#10;&#10;        &#60;!-- Logging --&#62;&#10;        &#60;dependency&#62;&#10;            &#60;groupId&#62;org.slf4j&#60;/groupId&#62;&#10;            &#60;artifactId&#62;slf4j-api&#60;/artifactId&#62;&#10;            &#60;version&#62;1.7.2&#60;/version&#62;&#10;        &#60;/dependency&#62;&#10;&#10;        &#60;dependency&#62;&#10;            &#60;groupId&#62;log4j&#60;/groupId&#62;&#10;            &#60;artifactId&#62;log4j&#60;/artifactId&#62;&#10;            &#60;version&#62;1.2.16&#60;/version&#62;&#10;        &#60;/dependency&#62;&#10;&#10;        &#60;dependency&#62;&#10;            &#60;groupId&#62;org.slf4j&#60;/groupId&#62;&#10;            &#60;artifactId&#62;slf4j-log4j12&#60;/artifactId&#62;&#10;            &#60;version&#62;1.7.2&#60;/version&#62;&#10;            &#60;exclusions&#62;&#10;                &#60;exclusion&#62;&#10;                    &#60;groupId&#62;log4j&#60;/groupId&#62;&#10;                    &#60;artifactId&#62;log4j&#60;/artifactId&#62;&#10;                &#60;/exclusion&#62;&#10;            &#60;/exclusions&#62;&#10;        &#60;/dependency&#62;&#10;&#10;    &#60;/dependencies&#62;</span><br></pre></td></tr></table></figure>
<p>2.编写主函数</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.yxl;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.yxl.parquet.WriteParquet;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.ToolRunner;</span><br><span class="line"><span class="keyword">import</span> org.slf4j.Logger;</span><br><span class="line"><span class="keyword">import</span> org.slf4j.LoggerFactory;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span><br><span class="line"> * 入口函数</span><br><span class="line"> *</span><br><span class="line"> * Created by xiaolong.yuanxl on 16-1-28.</span><br><span class="line"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Logger LOG = LoggerFactory.getLogger(Main.class);</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (args.length &lt; <span class="number">2</span>) &#123;</span><br><span class="line">            LOG.warn(<span class="string">"Usage: "</span> + <span class="string">" INPUTFILE OUTPUTFILE [compression gzip | snappy]"</span>);</span><br><span class="line">            System.out.println(<span class="string">"Usage: "</span> + <span class="string">" INPUTFILE OUTPUTFILE [compression gzip | snappy]"</span>);</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        String inputPath = args[<span class="number">0</span>];</span><br><span class="line">        String outputPath = args[<span class="number">1</span>];</span><br><span class="line">        String compression = (args.length &gt; <span class="number">2</span>) ? args[<span class="number">2</span>] : <span class="string">"none"</span>;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            ToolRunner.run(<span class="keyword">new</span> WriteParquet(), <span class="keyword">new</span> String[]&#123;inputPath, outputPath, compression&#125;);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            LOG.error(<span class="string">"run mr JOB convert parquet file happend error: "</span>, e);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>3.编写模块函数</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.yxl.parquet;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configured;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.*;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.TextInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.Tool;</span><br><span class="line"><span class="keyword">import</span> org.slf4j.Logger;</span><br><span class="line"><span class="keyword">import</span> org.slf4j.LoggerFactory;</span><br><span class="line"><span class="keyword">import</span> parquet.example.data.Group;</span><br><span class="line"><span class="keyword">import</span> parquet.hadoop.ParquetFileReader;</span><br><span class="line"><span class="keyword">import</span> parquet.hadoop.example.ExampleInputFormat;</span><br><span class="line"><span class="keyword">import</span> parquet.hadoop.example.ExampleOutputFormat;</span><br><span class="line"><span class="keyword">import</span> parquet.hadoop.example.GroupWriteSupport;</span><br><span class="line"><span class="keyword">import</span> parquet.hadoop.metadata.CompressionCodecName;</span><br><span class="line"><span class="keyword">import</span> parquet.hadoop.metadata.ParquetMetadata;</span><br><span class="line"><span class="keyword">import</span> parquet.schema.MessageType;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span><br><span class="line"> * 写文件为parquet格式</span><br><span class="line"> *</span><br><span class="line"> * Created by xiaolong.yuanxl on 16-1-28.</span><br><span class="line"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WriteParquet</span> <span class="keyword">extends</span> <span class="title">Configured</span> <span class="keyword">implements</span> <span class="title">Tool</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Logger LOG = LoggerFactory.getLogger(WriteParquet.class);</span><br><span class="line"></span><br><span class="line">    <span class="annotation">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">run</span><span class="params">(String[] strings)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        String input = strings[<span class="number">0</span>];</span><br><span class="line">        String output = strings[<span class="number">1</span>];</span><br><span class="line">        String compression = strings[<span class="number">2</span>];</span><br><span class="line"></span><br><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 删除已有结果集</span></span><br><span class="line">        FileSystem fs = FileSystem.get(conf);</span><br><span class="line">        Path out = <span class="keyword">new</span> Path(output);</span><br><span class="line">        <span class="keyword">if</span> (fs.exists(out)) &#123;</span><br><span class="line">            fs.delete(out, <span class="keyword">true</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        Job job = Job.getInstance();</span><br><span class="line"></span><br><span class="line">        job.setJobName(<span class="string">"Convert Text to Parquet"</span>);</span><br><span class="line">        job.setJarByClass(getClass());</span><br><span class="line"></span><br><span class="line">        job.setMapperClass(WriteParquetMapper.class);</span><br><span class="line">        job.setInputFormatClass(TextInputFormat.class);</span><br><span class="line">        job.setOutputFormatClass(ExampleOutputFormat.class);</span><br><span class="line">        ExampleOutputFormat.setSchema(job, WriteParquetMapper.SCHEMA);</span><br><span class="line">        job.setNumReduceTasks(<span class="number">0</span>);   <span class="comment">//不需要reduce</span></span><br><span class="line">        job.setOutputKeyClass(Void.class);</span><br><span class="line">        job.setOutputValueClass(Group.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//设置压缩</span></span><br><span class="line">        CompressionCodecName codec = CompressionCodecName.UNCOMPRESSED;</span><br><span class="line">        <span class="keyword">if</span> (compression.equalsIgnoreCase(<span class="string">"snappy"</span>)) &#123;</span><br><span class="line">            codec = CompressionCodecName.SNAPPY;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (compression.equalsIgnoreCase(<span class="string">"gzip"</span>)) &#123;</span><br><span class="line">            codec = CompressionCodecName.GZIP;</span><br><span class="line">        &#125;</span><br><span class="line">        LOG.info(<span class="string">"Output compression: "</span> + codec);</span><br><span class="line">        ExampleOutputFormat.setCompression(job, codec);</span><br><span class="line"></span><br><span class="line">        FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(input));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(output));</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> job.waitForCompletion(<span class="keyword">true</span>) ? <span class="number">0</span> : <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>4.编写mapper (Mapper根据自己情况优化代码，这里只实现功能)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">package com.yxl.parquet;&#10;&#10;import org.apache.commons.lang.StringUtils;&#10;import org.apache.hadoop.io.LongWritable;&#10;import org.apache.hadoop.io.Text;&#10;import org.apache.hadoop.mapreduce.Mapper;&#10;import parquet.example.data.Group;&#10;import parquet.example.data.GroupFactory;&#10;import parquet.example.data.simple.SimpleGroupFactory;&#10;import parquet.hadoop.ParquetWriter;&#10;import parquet.schema.MessageType;&#10;import parquet.schema.MessageTypeParser;&#10;&#10;import java.io.IOException;&#10;&#10;/**&#10; * &#20889;parquet mapper&#10; *&#10; * Created by xiaolong.yuanxl on 16-1-28.&#10; */&#10;public class WriteParquetMapper extends Mapper&#60;LongWritable, Text, Void, Group&#62; &#123;&#10;&#10;    public static final MessageType SCHEMA = MessageTypeParser.parseMessageType(&#10;            &#34;message hive_schema &#123;\n&#34; +&#10;                    &#34;  optional binary remote_addr (UTF8);\n&#34; +&#10;                    &#34;  optional binary upstream_addr (UTF8);\n&#34; +&#10;                    &#34;  optional binary http_x_forwarded_for (UTF8);\n&#34; +&#10;                    &#34;  optional binary visit_time (UTF8);\n&#34; +&#10;                    &#34;  optional binary request_uri (UTF8);\n&#34; +&#10;                    &#34;  optional binary request_method (UTF8);\n&#34; +&#10;                    &#34;  optional binary server_protocol (UTF8);\n&#34; +&#10;                    &#34;  optional binary status (UTF8);\n&#34; +&#10;                    &#34;  optional binary body_bytes_sent (UTF8);\n&#34; +&#10;                    &#34;  optional binary request_time (UTF8);\n&#34; +&#10;                    &#34;  optional binary uid (UTF8);\n&#34; +&#10;                    &#34;  optional binary uuid (UTF8);\n&#34; +&#10;                    &#34;  optional binary user_agent (UTF8);\n&#34; +&#10;                    &#34;  optional binary refer (UTF8);\n&#34; +&#10;                    &#34;  optional binary request_body (UTF8);\n&#34; +&#10;                    &#34;&#125;&#34;&#10;    );&#10;&#10;    private GroupFactory groupFactory = new SimpleGroupFactory(SCHEMA);&#10;&#10;    @Override&#10;    public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123;&#10;        String line = StringUtils.trim(value.toString());&#10;        String[] arr = StringUtils.splitByWholeSeparatorPreserveAllTokens(line, &#34;\t&#34;);&#10;        Group group = groupFactory.newGroup();&#10;        try&#123;&#10;            if (arr != null)&#123;&#10;                //&#30452;&#25509;&#33719;&#21462;&#19979;&#26631;&#10;                group&#10;                    .append(&#34;remote_addr&#34;, arr[0])&#10;                    .append(&#34;upstream_addr&#34;, arr[1])&#10;                    .append(&#34;http_x_forwarded_for&#34;, arr[2])&#10;                    .append(&#34;visit_time&#34;, arr[3])&#10;                    .append(&#34;request_uri&#34;,arr[4])&#10;                    .append(&#34;request_method&#34;,arr[5])&#10;                    .append(&#34;server_protocol&#34;, arr[6])&#10;                    .append(&#34;status&#34;,arr[7])&#10;                    .append(&#34;body_bytes_sent&#34;,arr[8])&#10;                    .append(&#34;request_time&#34;, arr[9])&#10;                    .append(&#34;uid&#34;, arr[10])&#10;                    .append(&#34;uuid&#34;, arr[11])&#10;                    .append(&#34;user_agent&#34;, arr[12])&#10;                    .append(&#34;refer&#34;, arr[13])&#10;                    .append(&#34;request_body&#34;, arr[14]);&#10;&#10;            &#125;&#10;        &#125;catch (Exception e)&#123;&#10;            System.out.println(&#34;[ERROR]: map happend error &#34; + e.getMessage());&#10;        &#125;&#10;        context.write(null, group);&#10;    &#125;&#10;&#10;&#125;</span><br></pre></td></tr></table></figure>
<p>5.然后运行即可</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop jar parquet-0.0.1-SNAPSHOT.jar &#60;input&#62; &#60;output&#62; &#60;&#21387;&#32553;&#26684;&#24335;snappy&#25110;gzip&#62;</span><br></pre></td></tr></table></figure>
<p>6.验证，可以用刚才我们编译的 parquet-cat 来看一下字段是否都ok了</p>
<h2 id="u5BFC_u5165hive_u8868_uFF08_u53EF_u9009_uFF0C_u6839_u636E_u81EA_u5DF1_u4E1A_u52A1_uFF09"><a href="#u5BFC_u5165hive_u8868_uFF08_u53EF_u9009_uFF0C_u6839_u636E_u81EA_u5DF1_u4E1A_u52A1_uFF09" class="headerlink" title="导入hive表（可选，根据自己业务）"></a>导入hive表（可选，根据自己业务）</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">alter table &#60;your-parquet-table&#62; add partition(dt=20160101,hour=00) location &#39;&#60;output&#62;&#39;;</span><br></pre></td></tr></table></figure>
<p>附上hive建表语句</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CREATE EXTERNAL TABLE `nginx_log`(&#10;  `remote_addr` string,&#10;  `upstream_addr` string,&#10;  `http_x_forwarded_for` string,&#10;  `visit_time` string,&#10;  `request_uri` string,&#10;  `request_method` string,&#10;  `server_protocol` string,&#10;  `status` string,&#10;  `body_bytes_sent` string,&#10;  `request_time` string,&#10;  `uid` string,&#10;  `uuid` string,&#10;  `user_agent` string,&#10;  `refer` string,&#10;  `request_body` string)&#10;PARTITIONED BY (&#10;  `dt` string,&#10;  `hour` string)&#10;ROW FORMAT DELIMITED&#10;  FIELDS TERMINATED BY &#39;\t&#39;&#10;STORED AS parquetfile</span><br></pre></td></tr></table></figure>
<p>可以利用下面脚本每日导入或初始化补数据导入</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">function</span> <span class="function"><span class="title">loadToHive</span></span>()&#123;</span><br><span class="line">    INPUT_BASE_DIR=/camus/topics/system_nginx</span><br><span class="line">    OUTPUT_BASE_DIR=/user/hive/warehouse/<span class="built_in">source</span>_log.db/nginx_<span class="built_in">log</span></span><br><span class="line"></span><br><span class="line">    INPUT_PARTITION=<span class="variable">$&#123;INPUT_BASE_DIR&#125;</span>/dt=<span class="variable">$1</span>/hour=<span class="variable">$2</span>/</span><br><span class="line">    OUTPUT_PARTITION=<span class="variable">$&#123;OUTPUT_BASE_DIR&#125;</span>/dt=<span class="variable">$1</span>/hour=<span class="variable">$2</span>/</span><br><span class="line"></span><br><span class="line">    COMPRESS=snappy</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 1. delete and mkdir output on hdfs</span></span><br><span class="line"></span><br><span class="line">    /usr/<span class="built_in">local</span>/datacenter/hadoop/bin/hadoop fs -rm -r -skipTrash <span class="variable">$&#123;OUTPUT_PARTITION&#125;</span></span><br><span class="line">    /usr/<span class="built_in">local</span>/datacenter/hadoop/bin/hadoop fs -mkdir -p <span class="variable">$&#123;OUTPUT_PARTITION&#125;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 2. convert textfile to parquetfile</span></span><br><span class="line"></span><br><span class="line">    /usr/<span class="built_in">local</span>/datacenter/hadoop/bin/hadoop jar /usr/<span class="built_in">local</span>/datacenter/camus/lib/parquet-<span class="number">0.0</span>.<span class="number">1</span>-SNAPSHOT.jar <span class="variable">$&#123;INPUT_PARTITION&#125;</span> <span class="variable">$&#123;OUTPUT_PARTITION&#125;</span> <span class="variable">$&#123;COMPRESS&#125;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 3. after parquet load data into hive external table</span></span><br><span class="line"></span><br><span class="line">    /usr/<span class="built_in">local</span>/datacenter/hive/bin/hive <span class="operator">-e</span> <span class="string">"alter table source_log.nginx_log add partition(dt=<span class="variable">$1</span>,hour=<span class="variable">$2</span>) location \"<span class="variable">$&#123;OUTPUT_PARTITION&#125;</span>\";"</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">startdate=<span class="number">20160128</span></span><br><span class="line">enddate=<span class="number">20160128</span></span><br><span class="line"></span><br><span class="line">curr=<span class="string">"<span class="variable">$startdate</span>"</span></span><br><span class="line"><span class="keyword">while</span> <span class="literal">true</span>; <span class="keyword">do</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">"<span class="variable">$curr</span>"</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#loadToHive $curr 00</span></span><br><span class="line">    <span class="comment">#loadToHive $curr 01</span></span><br><span class="line">    <span class="comment">#loadToHive $curr 02</span></span><br><span class="line">    <span class="comment">#loadToHive $curr 03</span></span><br><span class="line">    <span class="comment">#loadToHive $curr 04</span></span><br><span class="line">    <span class="comment">#loadToHive $curr 05</span></span><br><span class="line">    <span class="comment">#loadToHive $curr 06</span></span><br><span class="line">    <span class="comment">#loadToHive $curr 07</span></span><br><span class="line">    <span class="comment">#loadToHive $curr 08</span></span><br><span class="line">    <span class="comment">#loadToHive $curr 09</span></span><br><span class="line">    <span class="comment">#loadToHive $curr 10</span></span><br><span class="line">    loadToHive <span class="variable">$curr</span> <span class="number">11</span></span><br><span class="line">    loadToHive <span class="variable">$curr</span> <span class="number">12</span></span><br><span class="line">    loadToHive <span class="variable">$curr</span> <span class="number">13</span></span><br><span class="line">    loadToHive <span class="variable">$curr</span> <span class="number">14</span></span><br><span class="line">    loadToHive <span class="variable">$curr</span> <span class="number">15</span></span><br><span class="line">    loadToHive <span class="variable">$curr</span> <span class="number">16</span></span><br><span class="line">    loadToHive <span class="variable">$curr</span> <span class="number">17</span></span><br><span class="line">    loadToHive <span class="variable">$curr</span> <span class="number">18</span></span><br><span class="line">    loadToHive <span class="variable">$curr</span> <span class="number">19</span></span><br><span class="line">    loadToHive <span class="variable">$curr</span> <span class="number">20</span></span><br><span class="line">    loadToHive <span class="variable">$curr</span> <span class="number">21</span></span><br><span class="line">    loadToHive <span class="variable">$curr</span> <span class="number">22</span></span><br><span class="line">    loadToHive <span class="variable">$curr</span> <span class="number">23</span></span><br><span class="line"></span><br><span class="line">    [ <span class="string">"<span class="variable">$curr</span>"</span> \&lt; <span class="string">"<span class="variable">$enddate</span>"</span> ] || <span class="built_in">break</span></span><br><span class="line">    curr=$( date +%Y%m%d --date <span class="string">"<span class="variable">$curr</span> +1 day"</span> )</span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure>
<p>PS：你也可以clone 我在 github 上的 demo 工程 <a href="https://github.com/yuanxiaolong/ParquetDemo.git" target="_blank" rel="external">https://github.com/yuanxiaolong/ParquetDemo.git</a></p>
]]></content>
    <summary type="html">
    <![CDATA[利用parquet格式来存储数据]]>
    
    </summary>
    
      <category term="hadoop" scheme="http://yoursite.com/tags/hadoop/"/>
    
      <category term="hadoop" scheme="http://yoursite.com/categories/hadoop/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[AWS S3 Java API]]></title>
    <link href="http://yoursite.com/blog/2016/07/06/AWS-S3-Java-API/"/>
    <id>http://yoursite.com/blog/2016/07/06/AWS-S3-Java-API/</id>
    <published>2016-07-06T13:12:48.000Z</published>
    <updated>2016-07-06T13:33:46.000Z</updated>
    <content type="html"><![CDATA[<p>本文介绍一下 aws s3 （simple storage service）的 api 操作</p>
<a id="more"></a>
<p>利用 s3 我们可以将存储外挂在 aws 上，是一种扩展性的服务</p>
<h2 id="u73AF_u5883_u51C6_u5907"><a href="#u73AF_u5883_u51C6_u5907" class="headerlink" title="环境准备"></a>环境准备</h2><ul>
<li>一个aws账号，目前只能申请国际账号，即非中国账号。申请的时候，需要一张信用卡，及电话一部</li>
<li>java环境</li>
</ul>
<p>aws账号的注册，需要信用卡，然后冻结1$ 以免滥注册，以及需要接听一个美国呼叫过来的电话，是一个语音验证码</p>
<p><img src="/images/aws/20160706/1.png" alt=""></p>
<h3 id="u83B7_u53D6_accessKeyID__u53CA_secretKey"><a href="#u83B7_u53D6_accessKeyID__u53CA_secretKey" class="headerlink" title="获取 accessKeyID 及 secretKey"></a>获取 accessKeyID 及 secretKey</h3><p>首先登陆账户-&gt; 「安全证书」</p>
<p><img src="/images/aws/20160706/2.png" alt=""></p>
<p>创建「访问密钥」，这个密钥创建完后，只显示一次，请妥善保管</p>
<p><img src="/images/aws/20160706/3.png" alt=""></p>
<p>由于我已经创建过了密钥，所以这里会存在一条记录</p>
<p><img src="/images/aws/20160706/4.png" alt=""></p>
<h3 id="s3_u4E2D_u7684_u6982_u5FF5"><a href="#s3_u4E2D_u7684_u6982_u5FF5" class="headerlink" title="s3中的概念"></a>s3中的概念</h3><p>简单说明一下，更详情的可以百度。<br>buckets<br>桶 ，在s3中最大粒度的 存储单位 是 buckets，一般情况，需要你先通过 aws 官网登陆 s3 服务，在网页里人工创建buckets ，而不是通过 代码 api 来创建。buckets的命名 aws 建议 用类似这种命名法，来保证唯一 <code>yxl.aws.bucket</code> ，从代码的角度上看，一个bucket 就是一个命名空间</p>
<p>文件系统<br>在同一个bucket里，文件组织形式，虽然跟linux一样采用树形结构，但实际上没有完全一致。在AWS里采用『前缀』+『实际文件名』的形式来存储，只不过前缀的表现形式是树形结构</p>
<p>注：在s3中没有类似linux中 <code>/</code> 这样的根目录，在s3中指定了bucket，就充当了根目录</p>
<p>例如下图中有3个文件夹，那test文件夹的『前缀』是什么呢？</p>
<p><img src="/images/aws/20160706/5.png" alt=""><br><img src="/images/aws/20160706/6.png" alt=""></p>
<p>答案是 test 而不是 /test，因为s3已经忽略了根路径。如果你创建了一个前缀为 /test/abc.txt 的文件在bucket里，<br>那么s3变解析为 『//test/abc.txt』，及2个斜杠，在路径中表现为一个空文件夹下，才有test文件夹，及test文件夹下的abc.txt</p>
<h3 id="u7F16_u5199_u4EE3_u7801"><a href="#u7F16_u5199_u4EE3_u7801" class="headerlink" title="编写代码"></a>编写代码</h3><p>1.添加 pom.xml 依赖</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="title">dependency</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="title">groupId</span>&gt;</span>com.amazonaws<span class="tag">&lt;/<span class="title">groupId</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="title">artifactId</span>&gt;</span>aws-java-sdk<span class="tag">&lt;/<span class="title">artifactId</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="title">version</span>&gt;</span>1.10.48<span class="tag">&lt;/<span class="title">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>2.建立认证，其中 accessKeyID和secretKey 就是你刚才重点保存的。（中国调用美国接口，可能会超时）</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">AWSCredentials credentials;</span><br><span class="line">AmazonS3 s3Client;</span><br><span class="line">credentials = <span class="keyword">new</span> BasicAWSCredentials(accessKeyID, secretKey);</span><br><span class="line">s3Client = <span class="keyword">new</span> AmazonS3Client(credentials);</span><br></pre></td></tr></table></figure>
<p>列出buckets</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">listBuckets</span><span class="params">(AmazonS3 s3Client)</span></span>&#123;</span><br><span class="line">        List&lt;Bucket&gt; list = s3Client.listBuckets();</span><br><span class="line">        <span class="keyword">for</span> (Bucket bucket : list)&#123;</span><br><span class="line">            <span class="comment">// java.net.SocketException: Connection reset</span></span><br><span class="line">            <span class="comment">// java.net.SocketTimeoutException: connect timed out</span></span><br><span class="line">            <span class="comment">// java.io.EOFException: SSL peer shut down incorrectly</span></span><br><span class="line">            System.out.println(<span class="string">"---------&gt;"</span> + bucket.getName() + <span class="string">" ----&gt; "</span> + bucket.getOwner());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>遍历所有文件</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">listAll</span><span class="params">(AmazonS3 s3Client,String bucketName)</span></span>&#123;</span><br><span class="line">        ObjectListing objects = s3Client.listObjects(bucketName);</span><br><span class="line">        do &#123;</span><br><span class="line">            <span class="keyword">for</span> (S3ObjectSummary objectSummary : objects.getObjectSummaries()) &#123;</span><br><span class="line">                System.out.println(<span class="string">"Object: "</span> + objectSummary.getKey());</span><br><span class="line">            &#125;</span><br><span class="line">            objects = s3Client.listNextBatchOfObjects(objects);</span><br><span class="line">        &#125; <span class="keyword">while</span> (objects.isTruncated());</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>查询指定文件信息</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">    <span class="comment">//size : 86791631</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">info</span><span class="params">(AmazonS3 s3Client,String bucketName)</span></span>&#123;</span><br><span class="line">        ObjectMetadata metadata = s3Client.getObjectMetadata(bucketName,<span class="string">"test/apache-hive-1.1.0-bin.tar.gz"</span>);</span><br><span class="line">        System.out.println(<span class="string">"size : "</span> + metadata.getContentLength());</span><br><span class="line"><span class="comment">//        System.out.println("md5 : " + metadata.getContentMD5());</span></span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>删除指定文件</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">delete</span><span class="params">(AmazonS3 s3Client,String bucketName)</span></span>&#123;</span><br><span class="line">    <span class="comment">// 重复删除没有关系</span></span><br><span class="line">    DeleteObjectRequest request = <span class="keyword">new</span> DeleteObjectRequest(bucketName, <span class="string">"/test/apache-hive-1.1.0-bin.tar.gz"</span>);</span><br><span class="line">    s3Client.deleteObject(request);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上传单个文件（s3接口已封装成多块上传）<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 同名覆盖</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">upload</span><span class="params">(AmazonS3 s3Client,String bucketName,String from,String to)</span></span>&#123;</span><br><span class="line">    TransferManager tm = <span class="keyword">new</span> TransferManager(s3Client);</span><br><span class="line">    TransferManagerConfiguration conf = tm.getConfiguration();</span><br><span class="line"></span><br><span class="line">    Upload upload = tm.upload(bucketName,to,<span class="keyword">new</span> File(from));</span><br><span class="line">    TransferProgress p = upload.getProgress();</span><br><span class="line">    <span class="keyword">while</span> (upload.isDone() == <span class="keyword">false</span>)&#123;</span><br><span class="line">        <span class="keyword">int</span> percent =  (<span class="keyword">int</span>)(p.getPercentTransferred());</span><br><span class="line">        System.out.print(<span class="string">"\r"</span> + from + <span class="string">" - "</span> + <span class="string">"[ "</span> + percent + <span class="string">"% ] "</span></span><br><span class="line">                + p.getBytesTransferred() + <span class="string">" / "</span> + p.getTotalBytesToTransfer() );</span><br><span class="line">        <span class="comment">// Do work while we wait for our upload to complete...</span></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            Thread.sleep(<span class="number">500</span>);</span><br><span class="line">        &#125;<span class="keyword">catch</span> (Exception e)&#123;</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">try</span>&#123;</span><br><span class="line">        upload.waitForCompletion();</span><br><span class="line">        <span class="comment">// 默认添加public权限</span></span><br><span class="line">        s3Client.setObjectAcl(bucketName, to, CannedAccessControlList.PublicRead);</span><br><span class="line">    &#125;<span class="keyword">catch</span> (Exception e)&#123;</span><br><span class="line">        System.out.println(e.getMessage());</span><br><span class="line">    &#125;<span class="keyword">finally</span> &#123;</span><br><span class="line">        tm.shutdownNow();</span><br><span class="line">    &#125;</span><br><span class="line">    System.out.print(<span class="string">"\r"</span> + from + <span class="string">" - "</span> + <span class="string">"[ 100% ] "</span></span><br><span class="line">            + p.getBytesTransferred() + <span class="string">" / "</span> + p.getTotalBytesToTransfer() );</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>上传整个文件夹下的文件(不会创建本地文件前的路径)</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">uploadDirectory</span><span class="params">(AmazonS3 s3Client,String bucketName)</span></span>&#123;</span><br><span class="line">        TransferManager tm = <span class="keyword">new</span> TransferManager(s3Client);</span><br><span class="line">        <span class="keyword">final</span> String localpath = <span class="string">"/Users/yxl/Downloads/highlight"</span>;</span><br><span class="line">        MultipleFileUpload multipleFileUpload = tm.uploadDirectory(bucketName, <span class="string">"test/"</span>, <span class="keyword">new</span> File(localpath), <span class="keyword">false</span>);</span><br><span class="line">        <span class="keyword">final</span> TransferProgress p = multipleFileUpload.getProgress();</span><br><span class="line">        multipleFileUpload.addProgressListener(<span class="keyword">new</span> ProgressListener() &#123;</span><br><span class="line">            <span class="annotation">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">progressChanged</span><span class="params">(ProgressEvent progressEvent)</span> </span>&#123;</span><br><span class="line">                <span class="keyword">double</span> percent = p.getPercentTransferred();</span><br><span class="line">                System.out.print(<span class="string">"\n"</span> + localpath + <span class="string">" - "</span> + <span class="string">"[ "</span> + String.format(<span class="string">"%.2f"</span>,percent) + <span class="string">"% ] "</span></span><br><span class="line">                        + p.getBytesTransferred() + <span class="string">" / "</span> + p.getTotalBytesToTransfer() );</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> (progressEvent.getEventType() == ProgressEventType.TRANSFER_COMPLETED_EVENT) &#123;</span><br><span class="line">                    System.out.println(<span class="string">" Upload complete!!!"</span>);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">try</span>&#123;</span><br><span class="line">            multipleFileUpload.waitForCompletion();</span><br><span class="line">        &#125;<span class="keyword">catch</span> (Exception e)&#123;</span><br><span class="line">            System.out.println(e.getMessage());</span><br><span class="line">        &#125;<span class="keyword">finally</span> &#123;</span><br><span class="line">            tm.shutdownNow();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>上传多个文件（会创建本地文件前的路径到s3）</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//这个会创建相对路径的文件，例如本地路径 localfile1 和 localfile2，则上传到s3后，文件路径为test/Users/yxl/Documents/q.txt 和 test/Users/yxl/Downloads/ha.xml</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">uploadMultFiles</span><span class="params">(AmazonS3 s3Client,String bucketName)</span></span>&#123;</span><br><span class="line">        TransferManager tm = <span class="keyword">new</span> TransferManager(s3Client);</span><br><span class="line">        <span class="keyword">final</span> String localfile1 = <span class="string">"/Users/yxl/Documents/q.txt"</span>;</span><br><span class="line">        <span class="keyword">final</span> String localfile2 = <span class="string">"/Users/yxl/Downloads/ha.xml"</span>;</span><br><span class="line"></span><br><span class="line">        MultipleFileUpload multipleFileUpload = tm.uploadFileList(bucketName,<span class="string">"test/"</span>,<span class="keyword">new</span> File(<span class="string">"/"</span>), Arrays.asList(<span class="keyword">new</span> File(localfile1),<span class="keyword">new</span> File(localfile2)));</span><br><span class="line"><span class="comment">//        MultipleFileUpload multipleFileUpload = tm.uploadDirectory(bucketName, "test/", new File(localpath), false);</span></span><br><span class="line">        <span class="keyword">final</span> TransferProgress p = multipleFileUpload.getProgress();</span><br><span class="line">        multipleFileUpload.addProgressListener(<span class="keyword">new</span> ProgressListener() &#123;</span><br><span class="line">            <span class="annotation">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">progressChanged</span><span class="params">(ProgressEvent progressEvent)</span> </span>&#123;</span><br><span class="line">                <span class="keyword">double</span> percent = p.getPercentTransferred();</span><br><span class="line">                System.out.print(<span class="string">"\n"</span> + <span class="string">"file now "</span> + <span class="string">" - "</span> + <span class="string">"[ "</span> + String.format(<span class="string">"%.2f"</span>,percent) + <span class="string">"% ] "</span></span><br><span class="line">                        + p.getBytesTransferred() + <span class="string">" / "</span> + p.getTotalBytesToTransfer() );</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> (progressEvent.getEventType() == ProgressEventType.TRANSFER_COMPLETED_EVENT) &#123;</span><br><span class="line">                    System.out.println(<span class="string">" Upload complete!!!"</span>);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">try</span>&#123;</span><br><span class="line">            multipleFileUpload.waitForCompletion();</span><br><span class="line">        &#125;<span class="keyword">catch</span> (Exception e)&#123;</span><br><span class="line">            System.out.println(e.getMessage());</span><br><span class="line">        &#125;<span class="keyword">finally</span> &#123;</span><br><span class="line">            tm.shutdownNow();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>下载文件</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">download</span><span class="params">(AmazonS3 s3Client,String bucketName)</span></span>&#123;</span><br><span class="line">        GetObjectRequest request = <span class="keyword">new</span> GetObjectRequest(bucketName,<span class="string">"test/apache-hive-1.1.0-bin.tar.gz"</span>);</span><br><span class="line">        TransferManager tm = <span class="keyword">new</span> TransferManager(s3Client);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">final</span> String localpath = <span class="string">"/Users/yxl/Downloads/aws-hive-bin.tar.gz"</span>;</span><br><span class="line">        <span class="keyword">final</span> Download download = tm.download(request,<span class="keyword">new</span> File(localpath));</span><br><span class="line">        <span class="keyword">final</span> TransferProgress p = download.getProgress();</span><br><span class="line">        download.addProgressListener(<span class="keyword">new</span> ProgressListener() &#123;</span><br><span class="line">            <span class="annotation">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">progressChanged</span><span class="params">(ProgressEvent progressEvent)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">                <span class="keyword">double</span> percent = p.getPercentTransferred();</span><br><span class="line">                System.out.print(<span class="string">"\n"</span> + localpath + <span class="string">" - "</span> + <span class="string">"[ "</span> + String.format(<span class="string">"%.2f"</span>,percent) + <span class="string">"% ] "</span></span><br><span class="line">                    + p.getBytesTransferred() + <span class="string">" / "</span> + p.getTotalBytesToTransfer() );</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> (progressEvent.getEventType() == ProgressEventType.TRANSFER_COMPLETED_EVENT) &#123;</span><br><span class="line">                    System.out.println(<span class="string">" Download complete!!!"</span>);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">        <span class="keyword">try</span>&#123;</span><br><span class="line">            download.waitForCompletion();</span><br><span class="line">        &#125;<span class="keyword">catch</span> (Exception e)&#123;</span><br><span class="line">            System.out.println(e.getMessage());</span><br><span class="line">        &#125;<span class="keyword">finally</span> &#123;</span><br><span class="line">            tm.shutdownNow();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>下载整个文件夹</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//Unable to store object contents to disk: Read timed out</span></span><br><span class="line">    <span class="comment">//由于下载会超时导致不完整，需要校验元信息大小和实际下载大小</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">downloadDirectory</span><span class="params">(AmazonS3 s3Client,String bucketName)</span></span>&#123;</span><br><span class="line">        TransferManager tm = <span class="keyword">new</span> TransferManager(s3Client);</span><br><span class="line">        <span class="keyword">final</span> String localpath = <span class="string">"/Users/yxl/Downloads/aws_download"</span>;</span><br><span class="line">        <span class="keyword">final</span> MultipleFileDownload download = tm.downloadDirectory(bucketName,<span class="string">"test/"</span>,<span class="keyword">new</span> File(localpath));</span><br><span class="line">        <span class="keyword">final</span> TransferProgress p = download.getProgress();</span><br><span class="line">        download.addProgressListener(<span class="keyword">new</span> ProgressListener() &#123;</span><br><span class="line">            <span class="annotation">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">progressChanged</span><span class="params">(ProgressEvent progressEvent)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">                <span class="keyword">double</span> percent = p.getPercentTransferred();</span><br><span class="line">                System.out.print(<span class="string">"\n"</span> + localpath + <span class="string">" - "</span> + <span class="string">"[ "</span> + String.format(<span class="string">"%.2f"</span>,percent) + <span class="string">"% ] "</span></span><br><span class="line">                        + p.getBytesTransferred() + <span class="string">" / "</span> + p.getTotalBytesToTransfer() );</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> (progressEvent.getEventType() == ProgressEventType.TRANSFER_COMPLETED_EVENT) &#123;</span><br><span class="line">                    System.out.println(<span class="string">" Download complete!!!"</span>);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">        <span class="keyword">try</span>&#123;</span><br><span class="line">            download.waitForCompletion();</span><br><span class="line">        &#125;<span class="keyword">catch</span> (Exception e)&#123;</span><br><span class="line">            System.out.println(e.getMessage());</span><br><span class="line">        &#125;<span class="keyword">finally</span> &#123;</span><br><span class="line">            tm.shutdownNow();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<h2 id="u94FE_u63A5"><a href="#u94FE_u63A5" class="headerlink" title="链接"></a>链接</h2><p>aws s3 登陆地址 ：<a href="https://console.aws.amazon.com/s3/home" target="_blank" rel="external">https://console.aws.amazon.com/s3/home</a><br>aws s3 java api ：<a href="http://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/index.html" target="_blank" rel="external">http://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/index.html</a></p>
]]></content>
    <summary type="html">
    <![CDATA[用 aws 的 s3 来进行存储]]>
    
    </summary>
    
      <category term="AWS" scheme="http://yoursite.com/tags/AWS/"/>
    
      <category term="AWS" scheme="http://yoursite.com/categories/AWS/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[impala on yarn]]></title>
    <link href="http://yoursite.com/blog/2016/01/10/impala-on-yarn/"/>
    <id>http://yoursite.com/blog/2016/01/10/impala-on-yarn/</id>
    <published>2016-01-10T10:51:21.000Z</published>
    <updated>2016-01-16T01:41:47.000Z</updated>
    <content type="html"><![CDATA[<p>利用 llama 这个中间件 进行 impala on yarn</p>
<a id="more"></a>
<p>Llama 用于 Impala on yarn 作为一个代理，向 yarn 申请资源提交任务 <a href="http://archive-primary.cloudera.com/cdh5/cdh/5/" target="_blank" rel="external">下载地址</a></p>
<hr>
<h2 id="Llama__u5B89_u88C5_u542F_u52A8"><a href="#Llama__u5B89_u88C5_u542F_u52A8" class="headerlink" title="Llama 安装启动"></a>Llama 安装启动</h2><p>解压后在 <code>$LLAMA_HOME/llama-dist/target/llama-1.0.0-cdh5.4.0.tar.gz</code></p>
<p>修改 <code>yarn-site.xml</code></p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="title">name</span>&gt;</span>yarn.scheduler.minimum-allocation-mb<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="title">value</span>&gt;</span>0<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="title">name</span>&gt;</span>yarn.scheduler.minimum-allocation-vcores<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="title">value</span>&gt;</span>0<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="title">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="title">value</span>&gt;</span>mapreduce_shuffle,llama_nm_plugin<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="title">name</span>&gt;</span>yarn.nodemanager.aux-services.llama_nm_plugin.class<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="title">value</span>&gt;</span>com.cloudera.llama.nm.LlamaNMAuxiliaryService<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>修改 <code>core-site.xml</code></p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">name</span>&gt;</span>hadoop.proxyuser.llama.hosts<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">value</span>&gt;</span>*<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">name</span>&gt;</span>hadoop.proxyuser.llama.groups<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">value</span>&gt;</span>*<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>修改 <code>conf/llama-site.xml</code></p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="title">name</span>&gt;</span>llama.am.server.thrift.address<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="title">value</span>&gt;</span>192.168.7.11:15000<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="title">name</span>&gt;</span>llama.nm.server.thrift.address<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="title">value</span>&gt;</span>192.168.7.11:15100<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>配置 HA</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"> <span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">name</span>&gt;</span>llama.am.cluster.id<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">value</span>&gt;</span>llama<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">name</span>&gt;</span>llama.am.ha.enabled<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">value</span>&gt;</span>true<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">name</span>&gt;</span>llama.am.ha.zk-base<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">value</span>&gt;</span>/llama<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">name</span>&gt;</span>llama.am.ha.zk-quorum<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="title">value</span>&gt;</span>192.168.7.13:2181,192.168.7.15:2181,192.168.7.16:2181<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>修改 <code>llamaadmin.xml</code></p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">name</span>&gt;</span>llamaadmin.server.thrift.address<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">value</span>&gt;</span>192.168.7.11:15002<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>修改 <code>libexec/llama-env.sh</code>  ，建好 log文件夹，修改777权限</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> LLAMA_AM_SERVER_CONF=/usr/<span class="built_in">local</span>/datacenter/llama/conf</span><br><span class="line"><span class="built_in">export</span> LLAMA_AM_SERVER_LOG=/usr/<span class="built_in">local</span>/datacenter/llama/<span class="built_in">log</span></span><br></pre></td></tr></table></figure>
<p>把 $HADOOP_HOME/share/hadoop 下  hdfs 、common、 yarn 的jar ，软连接到 $LLAMA_HOME/lib 下，后同样，需要把 llama 的 jar 加入到 hadoop classpath 里，我用软连接的方式。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ln <span class="operator">-s</span> /usr/<span class="built_in">local</span>/datacenter/llama/lib/llama-<span class="number">1.0</span>.<span class="number">0</span>-cdh5.<span class="number">4.0</span>.jar /usr/<span class="built_in">local</span>/datacenter/hadoop/lib/</span><br><span class="line">ln <span class="operator">-s</span> /usr/<span class="built_in">local</span>/datacenter/llama/lib/metrics-core-<span class="number">3.0</span>.<span class="number">1</span>.jar /usr/<span class="built_in">local</span>/datacenter/hadoop/lib/</span><br><span class="line">ln <span class="operator">-s</span> /usr/<span class="built_in">local</span>/datacenter/llama/lib/libthrift-<span class="number">0.9</span>.<span class="number">0</span>.jar /usr/<span class="built_in">local</span>/datacenter/hadoop/lib/</span><br></pre></td></tr></table></figure>
<p>环境变量设置 <code>HADOOP_HOME</code> 在 2 台机器分别启动</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup ./llama &amp;</span><br></pre></td></tr></table></figure>
<p>访问 <a href="http://192.168.7.11:15001" target="_blank" rel="external">http://192.168.7.11:15001</a> 和 <a href="http://192.168.7.12:15001" target="_blank" rel="external">http://192.168.7.12:15001</a> （这里的 queueCrossref 和 nodesCrossref 为空是正常的，当有任务提交到 yarn 上时，资源分配后，这里才会显示很多其他信息）</p>
<p><img src="/images/impala/20160110/1.png" alt=""></p>
<p><img src="/images/impala/20160110/2.png" alt=""></p>
<hr>
<h2 id="u4FEE_u6539_Impala__u8BA9_u5176_u611F_u77E5_Llama"><a href="#u4FEE_u6539_Impala__u8BA9_u5176_u611F_u77E5_Llama" class="headerlink" title="修改 Impala 让其感知 Llama"></a>修改 Impala 让其感知 Llama</h2><p>1.准备工作已经完毕，下面设置 impala 的文件 <code>/etc/default/impala</code> 里添加 <code>enable_rm</code> <code>llama_addresses</code> <code>fair_scheduler_allocation_path</code> <code>cgroup_hierarchy_path</code></p>
<p>2.首先启动 cgroup 服务，如果没有安装先通过yum安装</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">service cgconfig start</span><br></pre></td></tr></table></figure>
<p>注意：设置<code>cgroup_hierarchy_path</code> 为 <code>/cgroup/cpu</code>  注意需要修改子目录权限，让其可写</p>
<p>如果是CentOS6（centos5 不支持cgroup好像，所以不能用 impala on yarn）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">IMPALA_SERVER_ARGS=&#34; \&#10;    -log_dir=$&#123;IMPALA_LOG_DIR&#125; \&#10;    -catalog_service_host=$&#123;IMPALA_CATALOG_SERVICE_HOST&#125; \&#10;    -state_store_port=$&#123;IMPALA_STATE_STORE_PORT&#125; \&#10;    -use_statestore \&#10;    -state_store_host=$&#123;IMPALA_STATE_STORE_HOST&#125; \&#10;    -enable_rm=true \&#10;    -rm_always_use_defaults=true \&#10;    -llama_addresses=192.168.7.11:15000,192.168.7.12:15000 \&#10;    -fair_scheduler_allocation_path=&#39;/usr/local/datacenter/hadoop/etc/hadoop/fair-scheduler.xml&#39; \&#10;    -cgroup_hierarchy_path=&#39;`/cgroup/cpu&#39; \&#10;    -be_port=$&#123;IMPALA_BACKEND_PORT&#125;&#34;</span><br></pre></td></tr></table></figure>
<p>3.然后，重启yarn 重启 impala-server，发现有4个客户端连接到 llama 上</p>
<p><img src="/images/impala/20160110/3.png" alt=""></p>
<p>4.然后提交一个任务，可以发现一个常驻的 application 在 yarn 上，无论开多少个 impala-shell 均只有一个。10分钟后此 application 状态变为 fail 且自动关闭</p>
<p><img src="/images/impala/20160110/4.png" alt=""></p>
<hr>
<h2 id="u78B0_u89C1_u7684_u95EE_u9898"><a href="#u78B0_u89C1_u7684_u95EE_u9898" class="headerlink" title="碰见的问题"></a>碰见的问题</h2><h3 id="UNKNOW_HOST"><a href="#UNKNOW_HOST" class="headerlink" title="UNKNOW HOST"></a><font color="#c89226">UNKNOW HOST</font></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">com.cloudera.llama.util.LlamaException: RESERVATION_ASKING_UNKNOWN_NODE&#10;- Reservation &#39;9348b4b355578e9f:16ce0ead700fff95&#39;,&#10;  expansion &#39;null&#39; asking for a resource on node &#39;node007013&#39; that does not exist.</span><br></pre></td></tr></table></figure>
<p>是因为 我的 yarn 虽然 ResourceManager 启动起来了，yarn web ui 正常，但各个节点 NodeManager 启动失败了，逐一启动，排除自身问题，直到正常为止  <code>yarn-deamon.sh start nodemanager</code></p>
<h3 id="CGroup"><a href="#CGroup" class="headerlink" title="CGroup"></a><font color="#c89226">CGroup</font></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ERROR: CGroup xxxxx does not have a /tasks file</span><br></pre></td></tr></table></figure>
<p>这个原因可能是   没开启CGroup服务 且 没有把 cgroup 路径设置 为  <code>/cgroup/cpu</code> 下</p>
<h3 id="llama_HA"><a href="#llama_HA" class="headerlink" title="llama HA"></a><font color="#c89226">llama HA</font></h3><p>llama HA 本身是没有问题的，验证方式可以 kill 掉 active llama pid ，然后看 standby 的是否会转变成 active，<br>然后此时 通过 web ui 是看不到有客户端 impala-server 连接的，可以启动一个 impala-shell ，就能看到此机器连接上了 llama active了</p>
<p>资料</p>
<ul>
<li><a href="http://cloudera.github.io/llama/RunningLlama.html" target="_blank" rel="external">http://cloudera.github.io/llama/RunningLlama.html</a></li>
<li><a href="http://www.cloudera.com/content/www/en-us/documentation/archive/impala/2-x/2-1-x/topics/impala_resource_management.html" target="_blank" rel="external">http://www.cloudera.com/content/www/en-us/documentation/archive/impala/2-x/2-1-x/topics/impala_resource_management.html</a></li>
<li><a href="http://www.cloudera.com/content/www/en-us/documentation/enterprise/latest/topics/cdh_hag_llama_ha.html" target="_blank" rel="external">http://www.cloudera.com/content/www/en-us/documentation/enterprise/latest/topics/cdh_hag_llama_ha.html</a></li>
<li><a href="http://ae.yyuap.com/pages/viewpage.action?pageId=918464" target="_blank" rel="external">http://ae.yyuap.com/pages/viewpage.action?pageId=918464</a></li>
</ul>
]]></content>
    <summary type="html">
    <![CDATA[利用llama进行 impala on yarn]]>
    
    </summary>
    
      <category term="impala" scheme="http://yoursite.com/tags/impala/"/>
    
      <category term="impala" scheme="http://yoursite.com/categories/impala/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[hbase data transfer]]></title>
    <link href="http://yoursite.com/blog/2016/01/10/hbase-data-transfer/"/>
    <id>http://yoursite.com/blog/2016/01/10/hbase-data-transfer/</id>
    <published>2016-01-10T10:32:27.000Z</published>
    <updated>2016-01-16T01:41:20.000Z</updated>
    <content type="html"><![CDATA[<p>Hbase 迁移数据</p>
<a id="more"></a>
<p>利用Hbase Export Import 导入导出工具，进行数据迁移</p>
<hr>
<p>Hbase 数据迁移的方法有大概4种，最近需要迁移数据，所以研究了一下，发现最灵活合适的 是利用 Export Import 进行导入导出。其他方式可以自己百度查阅。</p>
<h2 id="u73AF_u5883_u51C6_u5907"><a href="#u73AF_u5883_u51C6_u5907" class="headerlink" title="环境准备"></a>环境准备</h2><ul>
<li>hbase 2个不同集群 clusterA 和 clusterB （忽略网络不通）</li>
</ul>
<h2 id="u6B65_u9AA4"><a href="#u6B65_u9AA4" class="headerlink" title="步骤"></a>步骤</h2><p>前置：先在需要导入的新集群上，创建跟老集群一样的hbase表</p>
<p>1.先把 clusterA 的 Hbase 表导出到 cluster A hdfs 里</p>
<p>例如把 fact_mdd 表导出到 hdfs 路径</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./hbase org.apache.hadoop.hbase.mapreduce.Export fact_mdd /hbaseOldData/fact_mdd</span><br></pre></td></tr></table></figure>
<p>导出的时候，可以添加参数。例如指定 版本、时间戳、列族、压缩等</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./hbase org.apache.hadoop.hbase.mapreduce.Export \&#10;-D hbase.mapreduce.scan.column.family=1000121000 \&#10;-D mapreduce.output.fileoutputformat.compress=true \&#10;-D mapreduce.output.fileoutputformat.compress.codec=org.apache.hadoop.io.compress.SnappyCodec \&#10;-D mapreduce.output.fileoutputformat.compress.type=BLOCK \&#10;fact_mdd /hbaseOldData/fact_mdd_1000121000</span><br></pre></td></tr></table></figure>
<p>2.导入，指定的hdfs路径是老集群的，重复导入没有关系，只会刷新记录的 timestamp</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./hbase org.apache.hadoop.hbase.mapreduce.Import fact_mdd hdfs://<span class="number">192.168</span>.<span class="number">7.168</span>:<span class="number">9000</span>/hbaseOldData/fact_mdd_1000121000</span><br></pre></td></tr></table></figure>
<p>如果导入导出的集群，网段不通，可以先 get hdfs 到本地，然后再 put 到新集群hdfs上。</p>
<p>导入的时候还可以指定 hbase元数据版本（没有测试）</p>
<p>在导出的时候，我碰见一个表比较大，所以 Export 时候会失败，而且会挂掉 hbase 节点，所以可以按列族导出，然后再逐一导入。可以写个脚本，让hbase批量执行</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./hbase shell yourscript.hbaseshell</span><br></pre></td></tr></table></figure>
<p>建议，导入完成后，逐一修改列压缩为 SNAPPY 格式，注意列名，如果指定了一个错误的列名，会新增一个列族，也可以用脚本批量执行</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">disable</span> <span class="string">'test'</span></span><br><span class="line">alter <span class="string">'test'</span>, NAME =&gt; <span class="string">'f1'</span>, COMPRESSION =&gt; <span class="string">'snappy'</span></span><br><span class="line">alter <span class="string">'test'</span>, NAME =&gt; <span class="string">'f2'</span>, COMPRESSION =&gt; <span class="string">'snappy'</span></span><br><span class="line">alter <span class="string">'test'</span>, NAME =&gt; <span class="string">'f3'</span>, COMPRESSION =&gt; <span class="string">'snappy'</span></span><br><span class="line"><span class="built_in">enable</span> <span class="string">'test'</span></span><br><span class="line">major_compact <span class="string">'test'</span></span><br><span class="line"><span class="built_in">exit</span></span><br></pre></td></tr></table></figure>
<hr>
<h2 id="u5B98_u7F51_u6587_u6863"><a href="#u5B98_u7F51_u6587_u6863" class="headerlink" title="官网文档"></a>官网文档</h2><blockquote>
<p>Export</p>
<p><code>$ bin/hbase org.apache.hadoop.hbase.mapreduce.Export &lt;tablename&gt; &lt;outputdir&gt; [&lt;versions&gt; [&lt;starttime&gt; [&lt;endtime&gt;]]]</code></p>
<p>By default, the Export tool only exports the newest version of a given cell, regardless of the number of versions stored. To export more than one version, replace <versions> with the desired number of versions.</versions></p>
<p>Note: caching for the input Scan is configured via hbase.client.scanner.caching in the job configuration.</p>
<p>Import</p>
<p><code>$ bin/hbase org.apache.hadoop.hbase.mapreduce.Import &lt;tablename&gt; &lt;inputdir&gt;</code></p>
<p>To import 0.94 exported files in a 0.96 cluster or onwards, you need to set system property “hbase.import.version” when running the import command as below:</p>
<p><code>$ bin/hbase -Dhbase.import.version=0.94 org.apache.hadoop.hbase.mapreduce.Import &lt;tablename&gt; &lt;inputdir&gt;</code></p>
</blockquote>
]]></content>
    <summary type="html">
    <![CDATA[hbase 数据迁移]]>
    
    </summary>
    
      <category term="hbase" scheme="http://yoursite.com/tags/hbase/"/>
    
      <category term="hbase" scheme="http://yoursite.com/categories/hbase/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[neo4j cluster install]]></title>
    <link href="http://yoursite.com/blog/2015/12/30/neo4j-cluster-install/"/>
    <id>http://yoursite.com/blog/2015/12/30/neo4j-cluster-install/</id>
    <published>2015-12-30T13:26:59.000Z</published>
    <updated>2016-01-16T01:41:12.000Z</updated>
    <content type="html"><![CDATA[<p>介绍 neo4j 图数据库的集群部署</p>
<a id="more"></a>
<p>neo4j有企业版和社区版，本文介绍企业版的部署配置。启用HA功能</p>
<hr>
<h2 id="u73AF_u5883_u51C6_u5907"><a href="#u73AF_u5883_u51C6_u5907" class="headerlink" title="环境准备"></a>环境准备</h2><ul>
<li>centos 3台机器</li>
<li>neo4j-enterprise-2.3.1-unix.tar.gz <a href="http://pan.baidu.com/s/1c1kz1i4" target="_blank" rel="external"><font color="#517bd2">我的百度云共享</font></a></li>
</ul>
<hr>
<h2 id="u914D_u7F6E"><a href="#u914D_u7F6E" class="headerlink" title="配置"></a>配置</h2><p>1.将neo4j解压到3台机器，统一目录下，设置 NEO4J_HOME 环境变量（建议解压的linux 用户，就是运行neo4j时的用户）</p>
<p>2.修改配置 $NEO4J_HOME/conf 下 <code>neo4j.properties</code></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">remote_shell_enabled=<span class="keyword">true</span></span><br><span class="line">remote_shell_host=<span class="number">192.168</span>.7.11</span><br><span class="line">remote_shell_port=<span class="number">1337</span></span><br><span class="line">online_backup_enabled=<span class="keyword">true</span></span><br><span class="line">online_backup_server=<span class="number">192.168</span>.7.11:<span class="number">6362</span></span><br><span class="line">ha.server_id=<span class="number">1</span></span><br><span class="line">ha.initial_hosts=<span class="number">192.168</span>.7.11:<span class="number">5001</span>,<span class="number">192.168</span>.7.12:<span class="number">5001</span>,<span class="number">192.168</span>.7.13:<span class="number">5001</span></span><br><span class="line">ha.cluster_server=<span class="number">192.168</span>.7.11:<span class="number">5001</span></span><br><span class="line">ha.server=<span class="number">192.168</span>.7.11:<span class="number">6001</span></span><br></pre></td></tr></table></figure>
<p>其中 <code>ha.server_id</code> 代表集群中实例号，跟zookeeper类似</p>
<p>3.修改配置 $NEO4J_HOME/conf 下 <code>neo4j-server.properties</code> 我由于需要，关闭了权限校验。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">org.neo4j.server.database.mode=HA</span><br><span class="line">org.neo4j.server.webserver.address=<span class="number">192.168</span>.7.11</span><br><span class="line">dbms.security.auth_enabled=<span class="keyword">false</span></span><br><span class="line">dbms.browser.remote_content_hostname_whitelist=*</span><br><span class="line">dbms.security.allow_outgoing_browser_connections=<span class="keyword">true</span></span><br></pre></td></tr></table></figure>
<p>4.打开配置 $NEO4J_HOME/conf 下 <code>neo4j-wrapper.conf</code> 里</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">wrapper.java.additional=-Dcom.sun.management.jmxremote.port=<span class="number">3637</span></span><br><span class="line">wrapper.java.additional=-Dcom.sun.management.jmxremote.password.file=conf/jmx.password</span><br><span class="line">wrapper.java.additional=-Dcom.sun.management.jmxremote.access.file=conf/jmx.access</span><br></pre></td></tr></table></figure>
<p>其中需要注意这2个文件的权限</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">-rw------- <span class="number">1</span> hadoop hadoop  <span class="number">146</span> Nov <span class="number">10</span> <span class="number">20</span>:<span class="number">15</span> jmx.access</span><br><span class="line">-rw------- <span class="number">1</span> hadoop hadoop   <span class="number">95</span> Nov <span class="number">10</span> <span class="number">20</span>:<span class="number">15</span> jmx.password</span><br></pre></td></tr></table></figure>
<p>5.启动 neo4j ，执行 <code>$NEO4J_HOME/bin/neo4j start</code></p>
<p>访问任意一个节点的 7474端口，进入web UI 查看集群情况</p>
<p><img src="/images/neo4j/20151230/1.png" alt=""><br><img src="/images/neo4j/20151230/2.png" alt=""></p>
<p>可以通过左边的「收藏栏」执行 创建 节点，获取节点等信息。</p>
]]></content>
    <summary type="html">
    <![CDATA[neo4j cluster install]]>
    
    </summary>
    
      <category term="neo4j" scheme="http://yoursite.com/tags/neo4j/"/>
    
      <category term="neo4j" scheme="http://yoursite.com/categories/neo4j/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[impala udf]]></title>
    <link href="http://yoursite.com/blog/2015/12/30/impala-udf/"/>
    <id>http://yoursite.com/blog/2015/12/30/impala-udf/</id>
    <published>2015-12-30T13:25:20.000Z</published>
    <updated>2016-01-16T01:41:02.000Z</updated>
    <content type="html"><![CDATA[<p>Impala 从1.2开始就支持 udf 了，本文介绍 在 impala 里如何使用udf</p>
<a id="more"></a>
<p>impala 使用已有的 hive java udf 及 编写 impala c++ udf</p>
<hr>
<h2 id="hive_udf"><a href="#hive_udf" class="headerlink" title="hive udf"></a>hive udf</h2><p>我们先编写一个 hive 的 java udf ，然后让impala调用，来模拟这是一个 已有的 hive udf 是否能被 impala调用</p>
<p>1.新建maven工程，添加pom依赖</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="title">dependencies</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="title">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="title">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="title">artifactId</span>&gt;</span>hadoop-mapreduce-client-core<span class="tag">&lt;/<span class="title">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="title">version</span>&gt;</span>2.6.0<span class="tag">&lt;/<span class="title">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="title">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="title">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="title">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="title">artifactId</span>&gt;</span>hadoop-common<span class="tag">&lt;/<span class="title">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="title">version</span>&gt;</span>2.6.0<span class="tag">&lt;/<span class="title">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="title">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="title">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="title">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="title">artifactId</span>&gt;</span>hadoop-mapreduce-client-common<span class="tag">&lt;/<span class="title">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="title">version</span>&gt;</span>2.6.0<span class="tag">&lt;/<span class="title">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="title">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="title">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="title">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="title">artifactId</span>&gt;</span>hadoop-mapreduce-client-jobclient<span class="tag">&lt;/<span class="title">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="title">version</span>&gt;</span>2.6.0<span class="tag">&lt;/<span class="title">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="title">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="title">groupId</span>&gt;</span>org.apache.hive<span class="tag">&lt;/<span class="title">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="title">artifactId</span>&gt;</span>hive-exec<span class="tag">&lt;/<span class="title">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="title">version</span>&gt;</span>1.1.0<span class="tag">&lt;/<span class="title">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="title">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>2.编写udf代码</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.yxl;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.commons.lang.StringUtils;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.ql.exec.UDF;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> <span class="keyword">extends</span> <span class="title">UDF</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String PRE = <span class="string">"hello_"</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Text <span class="title">evaluate</span><span class="params">(String str)</span></span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (StringUtils.isBlank(str))&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">new</span> Text(<span class="string">"unknow"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> Text(PRE + str);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>3.导出jar包，可以用 IDE 导出不含依赖的 jar 。或者用 <code>mvn clean install</code> 直接安装到本地仓库，拿本地仓库的jar即可。</p>
<p>4.把此jar包放到 hdfs 上</p>
<p>5.进入impala-shell ，并进入对应的数据库上，执行类似如下命令（根据自己情况修改hdfs路径）</p>
<p>注：由于 Impala 和 hive 共享元数据，所以我们需要把新创建的 function 取跟hive function 里不一样的名字，尽管它们是同一个jar</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create <span class="keyword">function</span> my_foo(string) returns string location <span class="string">'/share/Udf4Hive.jar'</span> symbol=<span class="string">'com.yxl.Main'</span></span><br></pre></td></tr></table></figure>
<p>6.使用 hive udf</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">[node007012:<span class="number">21000</span>] &gt; desc pokes;</span><br><span class="line">Query: describe pokes</span><br><span class="line">+------+--------+---------+</span><br><span class="line">| name | <span class="built_in">type</span>   | comment |</span><br><span class="line">+------+--------+---------+</span><br><span class="line">| foo  | int    |         |</span><br><span class="line">| bar  | string |         |</span><br><span class="line">+------+--------+---------+</span><br><span class="line"></span><br><span class="line">[node007012:<span class="number">21000</span>] &gt; select foo,test.my_foo(bar) from pokes;</span><br><span class="line">Query: select foo,test.my_foo(bar) from pokes</span><br><span class="line">+-----+------------------+</span><br><span class="line">| foo | test.my_foo(bar) |</span><br><span class="line">+-----+------------------+</span><br><span class="line">| <span class="number">1</span>   | hello_hello      |</span><br><span class="line">| <span class="number">2</span>   | hello_pear       |</span><br><span class="line">| <span class="number">3</span>   | hello_world      |</span><br><span class="line">+-----+------------------+</span><br><span class="line">Fetched <span class="number">3</span> row(s) <span class="keyword">in</span> <span class="number">0.12</span>s</span><br></pre></td></tr></table></figure>
<hr>
<h2 id="C++_udf"><a href="#C++_udf" class="headerlink" title="C++ udf"></a>C++ udf</h2><p><a href="http://www.cloudera.com/content/cloudera/zh-CN/documentation/core/v5-3-x/topics/impala_udf.html" target="_blank" rel="external">cloudrea文档</a></p>
<p>概念</p>
<ul>
<li>UDF 一次处理一行的函数，0到多个入参，1个返回值</li>
<li>UDAF 一次处理多行的函数，类似聚集函数 sum 这样的，一个返回值 （未试验）</li>
</ul>
<p>官网示例  <a href="https://github.com/cloudera/impala-udf-samples" target="_blank" rel="external"><font color="#296ead">Github</font></a></p>
<h2 id="u73AF_u5883_u51C6_u5907"><a href="#u73AF_u5883_u51C6_u5907" class="headerlink" title="环境准备"></a>环境准备</h2><p>1.安装impala udf 安装包及编译器，可参见上文 Impala rpm 安装。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Use the appropriate package installation command for your Linux distribution.</span></span><br><span class="line">sudo yum install gcc-c++ cmake boost-devel</span><br><span class="line">sudo yum install impala-udf-devel</span><br></pre></td></tr></table></figure>
<p>2.编写 c++ 文件 ，包含头文件、逻辑文件、测试文件、和CMAKE文件</p>
<p>头文件 <code>udf-helloworld.h</code></p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="preprocessor">#<span class="keyword">ifndef</span> HELLOWORLD_UDF_H</span></span><br><span class="line"><span class="preprocessor">#<span class="keyword">define</span> HELLOWORLD_UDF_H</span></span><br><span class="line"></span><br><span class="line"><span class="preprocessor">#<span class="keyword">include</span> <span class="string">&lt;impala_udf/udf.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> impala_udf;</span><br><span class="line"></span><br><span class="line"><span class="function">StringVal <span class="title">Hello</span><span class="params">(FunctionContext* context, <span class="keyword">const</span> StringVal&amp; arg1)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="preprocessor">#<span class="keyword">endif</span></span></span><br></pre></td></tr></table></figure>
<p>逻辑文件 <code>udf-helloworld.cc</code> ，实现添加一个固定前缀</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">#include "udf-helloworld.h"</span><br><span class="line"></span><br><span class="line">#include &lt;cctype&gt;</span><br><span class="line">#include &lt;cmath&gt;</span><br><span class="line">#include &lt;string&gt;</span><br><span class="line">#include &lt;sstream&gt;</span><br><span class="line"></span><br><span class="line">StringVal Hello(FunctionContext* context, const StringVal&amp; arg1)&#123;</span><br><span class="line">    if (arg1.is_null) return StringVal::null();</span><br><span class="line"></span><br><span class="line">    int index;</span><br><span class="line">    std::string original((const char *)arg1.ptr,arg1.len);</span><br><span class="line">    std::string shorter("hello_");</span><br><span class="line"></span><br><span class="line">    int length;</span><br><span class="line">    length = original.length();</span><br><span class="line">    for (index = 0; index &lt; length; index++)&#123;</span><br><span class="line">        uint8_t c = original[index];</span><br><span class="line"></span><br><span class="line">        shorter.append(1, (char)c);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    StringVal result(context, shorter.size());</span><br><span class="line">    memcpy(result.ptr, shorter.c_str(), shorter.size());</span><br><span class="line"></span><br><span class="line">    return result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>测试文件 <code>udf-helloworld-test.cc</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">#include &#60;iostream&#62;&#10;&#10;#include &#60;impala_udf/udf-test-harness.h&#62;&#10;#include &#34;udf-helloworld.h&#34;&#10;&#10;using namespace impala;&#10;using namespace impala_udf;&#10;using namespace std;&#10;&#10;int main(int argc, char** argv) &#123;&#10;    bool passed = true;&#10;&#10;    passed &#38;= UdfTestHarness::ValidateUdf&#60;StringVal, StringVal&#62;(&#10;          Hello, StringVal(&#34;Tom&#34;), StringVal(&#34;hello_Tom&#34;));&#10;    passed &#38;= UdfTestHarness::ValidateUdf&#60;StringVal, StringVal&#62;(&#10;          Hello, StringVal::null(), StringVal::null());&#10;&#10;    cout &#60;&#60; &#34;Tests &#34; &#60;&#60; (passed ? &#34;Passed.&#34; : &#34;Failed.&#34;) &#60;&#60; endl;&#10;    return !passed;&#10;&#125;</span><br></pre></td></tr></table></figure>
<p>CMAKE文件 固定名字 <code>CMakeLists.txt</code></p>
<figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Copyright 2012 Cloudera Inc.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Licensed under the Apache License, Version 2.0 (the "License");</span></span><br><span class="line"><span class="comment"># you may not use this file except in compliance with the License.</span></span><br><span class="line"><span class="comment"># You may obtain a copy of the License at</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># http://www.apache.org/licenses/LICENSE-2.0</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Unless required by applicable law or agreed to in writing, software</span></span><br><span class="line"><span class="comment"># distributed under the License is distributed on an "AS IS" BASIS,</span></span><br><span class="line"><span class="comment"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span></span><br><span class="line"><span class="comment"># See the License for the specific language governing permissions and</span></span><br><span class="line"><span class="comment"># limitations under the License.</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">cmake_minimum_required</span>(VERSION <span class="number">2.6</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># where to put generated libraries</span></span><br><span class="line"><span class="keyword">set</span>(LIBRARY_OUTPUT_PATH <span class="string">"build"</span>)</span><br><span class="line"><span class="comment"># where to put generated binaries</span></span><br><span class="line"><span class="keyword">set</span>(EXECUTABLE_OUTPUT_PATH <span class="string">"build"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">find_program</span>(CLANG_EXECUTABLE clang++)</span><br><span class="line"></span><br><span class="line"><span class="keyword">SET</span>(CMAKE_CXX_FLAGS <span class="string">"$&#123;CMAKE_CXX_FLAGS&#125; -g -ggdb"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Function to generate rule to cross compile a source file to an IR module.</span></span><br><span class="line"><span class="comment"># This should be called with the .cc src file and it will generate a</span></span><br><span class="line"><span class="comment"># src-file-ir target that can be built.</span></span><br><span class="line"><span class="comment"># e.g. COMPILE_TO_IR(test.cc) generates the "test-ir" make target.</span></span><br><span class="line"><span class="comment"># Disable compiler optimizations because generated IR is optimized at runtime</span></span><br><span class="line"><span class="keyword">set</span>(IR_COMPILE_FLAGS <span class="string">"-emit-llvm"</span> <span class="string">"-c"</span>)</span><br><span class="line"><span class="keyword">function</span>(COMPILE_TO_IR SRC_FILE)</span><br><span class="line">  <span class="keyword">get_filename_component</span>(BASE_NAME <span class="envvar">$&#123;SRC_FILE&#125;</span> NAME_WE)</span><br><span class="line">  <span class="keyword">set</span>(OUTPUT_FILE <span class="string">"build/$&#123;BASE_NAME&#125;.ll"</span>)</span><br><span class="line">  <span class="keyword">add_custom_command</span>(</span><br><span class="line">    OUTPUT <span class="envvar">$&#123;OUTPUT_FILE&#125;</span></span><br><span class="line">    COMMAND <span class="envvar">$&#123;CLANG_EXECUTABLE&#125;</span> <span class="envvar">$&#123;IR_COMPILE_FLAGS&#125;</span> <span class="envvar">$&#123;SRC_FILE&#125;</span> -o <span class="envvar">$&#123;OUTPUT_FILE&#125;</span></span><br><span class="line">    DEPENDS <span class="envvar">$&#123;SRC_FILE&#125;</span>)</span><br><span class="line">  <span class="keyword">add_custom_target</span>(<span class="envvar">$&#123;BASE_NAME&#125;</span>-ir ALL DEPENDS <span class="envvar">$&#123;OUTPUT_FILE&#125;</span>)</span><br><span class="line"><span class="keyword">endfunction</span>(COMPILE_TO_IR)</span><br><span class="line"></span><br><span class="line"><span class="keyword">add_library</span>(udfhello SHARED udf-helloworld.cc)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (CLANG_EXECUTABLE)</span><br><span class="line">  COMPILE_TO_IR(udf-helloworld.cc )</span><br><span class="line"><span class="keyword">endif</span>(CLANG_EXECUTABLE)</span><br><span class="line"></span><br><span class="line"><span class="keyword">target_link_libraries</span>(udfhello ImpalaUdf)</span><br><span class="line"><span class="keyword">add_executable</span>(udf-helloworld-test udf-helloworld-test.cc)</span><br><span class="line"><span class="keyword">target_link_libraries</span>(udf-helloworld-test udfhello)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>C++ 定义的数据类型（位于 /usr/include/impala_udf/udf.h）是：</p>
<p>IntVal 代表 INT 列。</p>
<p>BigIntVal 代表 BIGINT 列。即使你不需要 BIGINT 值的全部范围，将函数参数设置为 BigIntVal 也会非常有用，以方便调用以不同种类的整数列和表达式作为参数的函数。Impala 会在适当时将较小的整数类型自动转换为较大的整数类型，但不会隐式将较大的整型转换为教小的。</p>
<p>SmallIntVal 代表SMALLINT 列。</p>
<p>TinyIntVal 代表 TINYINT 列。</p>
<p>StringVal 代表 STRING 列。其中 len 字段表示字符串长度，而 ptr 字段指向该字符串数据。基于 null 结尾的 C 风格字符串，或者一个指针加上长度，构造函数可以创建一个新的 StringVal 结构；这些新结构仍然参照原来的字符串数据，而不是为数据分配一个新的缓冲区。同时还包括一个构造函数，带有指向FunctionContext 结构和长度的指针，并且会为新复制的字符串数据分配空间，UDF 将使用该空间返回字符串值。</p>
<p>BooleanVal 代表 BOOLEAN 列。</p>
<p>FloatVal 代表 FLOAT 列。</p>
<p>DoubleVal 代表 DOUBLE 列。</p>
<p>TimestampVal 代表TIMESTAMP 列。具有一个 32 位整数的 date 字段，用以表示公历日期，历元后的天数。还具有一个 64 位整数的 time_of_day 字段，以纳秒表示当前时间。</p>
</blockquote>
<p>3.进入文件夹，执行 <code>cmake . &amp;&amp; make</code></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@node007012 udf]<span class="comment"># make</span></span><br><span class="line">[ <span class="number">50</span>%] Built target udfhello</span><br><span class="line">Scanning dependencies of target udf-helloworld-test</span><br><span class="line">[<span class="number">100</span>%] Building CXX object CMakeFiles/udf-helloworld-test.dir/udf-helloworld-test.cc.o</span><br><span class="line">Linking CXX executable build/udf-helloworld-test</span><br><span class="line">[<span class="number">100</span>%] Built target udf-helloworld-test</span><br></pre></td></tr></table></figure>
<p>4.进入build子文件夹，将里面的动态库文件，上传到hdfs上</p>
<p>5.进入impala-shell，到对应的数据库，并创建 function</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[node007012:<span class="number">21000</span>] &gt;  create <span class="keyword">function</span> hello (string) returns string location <span class="string">'/share/libudfhello.so'</span> symbol=<span class="string">'Hello'</span>;</span><br><span class="line">Query: create <span class="keyword">function</span> hello (string) returns string location <span class="string">'/share/libudfhello.so'</span> symbol=<span class="string">'Hello'</span></span><br><span class="line"></span><br><span class="line">Fetched <span class="number">0</span> row(s) <span class="keyword">in</span> <span class="number">0.15</span>s</span><br></pre></td></tr></table></figure>
<p>6.使用impala c++ function 查询，可见比java确实快很多，虽然只有3条数据，但能看见差异，反复测试现象一样</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[node007012:<span class="number">21000</span>] &gt; select foo,hello(bar) from pokes;</span><br><span class="line">Query: select foo,hello(bar) from pokes</span><br><span class="line">+-----+-----------------+</span><br><span class="line">| foo | test.hello(bar) |</span><br><span class="line">+-----+-----------------+</span><br><span class="line">| <span class="number">1</span>   | hello_hello     |</span><br><span class="line">| <span class="number">2</span>   | hello_pear      |</span><br><span class="line">| <span class="number">3</span>   | hello_world     |</span><br><span class="line">+-----+-----------------+</span><br><span class="line">Fetched <span class="number">3</span> row(s) <span class="keyword">in</span> <span class="number">0.01</span>s</span><br></pre></td></tr></table></figure>
<p>需要注意的是：</p>
<blockquote>
<p>目前，Metastore 数据库中不保留以 C++ 语言写入的 Impala UDF 和 UDA。这些函数的相关信息保留在 catalogd 守护程序的内存中。<br>每次重新启动 catalogd 守护程序时，必须再次运行 CREATE FUNCTION 语句才能加载它们。该限制不适用于以 Java 语言写入的 Impala UDF 和 UDA。</p>
</blockquote>
<hr>
<h2 id="u91CD_u65B0_u6CE8_u518C_u51FD_u6570"><a href="#u91CD_u65B0_u6CE8_u518C_u51FD_u6570" class="headerlink" title="重新注册函数"></a>重新注册函数</h2><p>1.拿刚才的为例，前缀改成 hello123_</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">std::string shorter(&#34;hello123_&#34;);</span><br></pre></td></tr></table></figure>
<p>2.重新cmake 当前目录，并且make</p>
<p>3.删除已有的 hdfs so 文件</p>
<p>4.进入impala-shell，查看已有函数</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[node007012:<span class="number">21000</span>] &gt; show <span class="built_in">functions</span>;</span><br><span class="line">Query: show <span class="built_in">functions</span></span><br><span class="line">+-------------+----------------+</span><br><span class="line">| <span class="built_in">return</span> <span class="built_in">type</span> | signature      |</span><br><span class="line">+-------------+----------------+</span><br><span class="line">| STRING      | hello(STRING)  |</span><br><span class="line">| STRING      | my_foo(STRING) |</span><br><span class="line">+-------------+----------------+</span><br><span class="line">Fetched <span class="number">2</span> row(s) <span class="keyword">in</span> <span class="number">0.02</span>s</span><br></pre></td></tr></table></figure>
<p>5.由于函数可能会重载，所以删除函数的时候，需要指定参数，才能对应上要删除的函数</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[node007012:<span class="number">21000</span>] &gt; drop <span class="keyword">function</span> hello(string);</span><br><span class="line">Query: drop <span class="keyword">function</span> hello(string)</span><br></pre></td></tr></table></figure>
<p>6.重新创建并查询</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[node007012:<span class="number">21000</span>] &gt; create <span class="keyword">function</span> hello (string) returns string location <span class="string">'/share/libudfhello.so'</span> symbol=<span class="string">'Hello'</span>;</span><br><span class="line">Query: create <span class="keyword">function</span> hello (string) returns string location <span class="string">'/share/libudfhello.so'</span> symbol=<span class="string">'Hello'</span></span><br><span class="line"></span><br><span class="line">Fetched <span class="number">0</span> row(s) <span class="keyword">in</span> <span class="number">0.04</span>s</span><br><span class="line"></span><br><span class="line">[node007012:<span class="number">21000</span>] &gt; select foo,hello(bar) from pokes;</span><br><span class="line">Query: select foo,hello(bar) from pokes</span><br><span class="line">+-----+-----------------+</span><br><span class="line">| foo | test.hello(bar) |</span><br><span class="line">+-----+-----------------+</span><br><span class="line">| <span class="number">1</span>   | hello123_hello  |</span><br><span class="line">| <span class="number">2</span>   | hello123_pear   |</span><br><span class="line">| <span class="number">3</span>   | hello123_world  |</span><br><span class="line">+-----+-----------------+</span><br><span class="line">Fetched <span class="number">3</span> row(s) <span class="keyword">in</span> <span class="number">0.02</span>s</span><br></pre></td></tr></table></figure>
]]></content>
    <summary type="html">
    <![CDATA[impala udf]]>
    
    </summary>
    
      <category term="impala" scheme="http://yoursite.com/tags/impala/"/>
    
      <category term="impala" scheme="http://yoursite.com/categories/impala/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[install impala]]></title>
    <link href="http://yoursite.com/blog/2015/12/27/install-impala/"/>
    <id>http://yoursite.com/blog/2015/12/27/install-impala/</id>
    <published>2015-12-27T10:13:36.000Z</published>
    <updated>2016-01-16T01:40:50.000Z</updated>
    <content type="html"><![CDATA[<p>介绍impala的 rpm包安装形式，利用impala 2.2.0</p>
<a id="more"></a>
<p>impala 是一个数据查询引擎，类似hive，但跟hive解决的问题也有不太一样的地方，优点速度快，缺点吃内存，不适合大数据批处理</p>
<hr>
<h2 id="u73AF_u5883_u51C6_u5907"><a href="#u73AF_u5883_u51C6_u5907" class="headerlink" title="环境准备"></a>环境准备</h2><p>注：本文的 hadoop hive hbase 都是通过tar 包独立安装的，并未通过rpm来安装，所以删除后面的软连接</p>
<p>1.下载rpm包 <a href="http://archive.cloudera.com/cdh5/redhat/6/x86_64/cdh/5.4.0/RPMS/x86_64/" target="_blank" rel="external"><font color="#2798a2">http://archive.cloudera.com/cdh5/redhat/6/x86_64/cdh/5.4.0/RPMS/x86_64/</font></a></p>
<p>共享我的<a href="http://pan.baidu.com/s/1i4nBSzF" target="_blank" rel="external"><font color="#2798a2">百度云</font></a></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">bigtop-utils-<span class="number">0.7</span>.<span class="number">0</span>+cdh5.<span class="number">4.0</span>+<span class="number">0</span>-<span class="number">1</span>.cdh5.<span class="number">4.0</span>.p0.<span class="number">47</span>.el6.noarch.rpm</span><br><span class="line">impala-<span class="number">2.2</span>.<span class="number">0</span>+cdh5.<span class="number">4.0</span>+<span class="number">0</span>-<span class="number">1</span>.cdh5.<span class="number">4.0</span>.p0.<span class="number">75</span>.el6.x86_64.rpm</span><br><span class="line">impala-state-store-<span class="number">2.2</span>.<span class="number">0</span>+cdh5.<span class="number">4.0</span>+<span class="number">0</span>-<span class="number">1</span>.cdh5.<span class="number">4.0</span>.p0.<span class="number">75</span>.el6.x86_64.rpm</span><br><span class="line">impala-shell-<span class="number">2.2</span>.<span class="number">0</span>+cdh5.<span class="number">4.0</span>+<span class="number">0</span>-<span class="number">1</span>.cdh5.<span class="number">4.0</span>.p0.<span class="number">75</span>.el6.x86_64.rpm</span><br><span class="line">impala-catalog-<span class="number">2.2</span>.<span class="number">0</span>+cdh5.<span class="number">4.0</span>+<span class="number">0</span>-<span class="number">1</span>.cdh5.<span class="number">4.0</span>.p0.<span class="number">75</span>.el6.x86_64.rpm</span><br><span class="line">impala-server-<span class="number">2.2</span>.<span class="number">0</span>+cdh5.<span class="number">4.0</span>+<span class="number">0</span>-<span class="number">1</span>.cdh5.<span class="number">4.0</span>.p0.<span class="number">75</span>.el6.x86_64.rpm</span><br><span class="line">impala-udf-devel-<span class="number">2.2</span>.<span class="number">0</span>+cdh5.<span class="number">4.0</span>+<span class="number">0</span>-<span class="number">1</span>.cdh5.<span class="number">4.0</span>.p0.<span class="number">75</span>.el6.x86_64.rpm</span><br><span class="line">impala-debuginfo-<span class="number">2.2</span>.<span class="number">0</span>+cdh5.<span class="number">4.0</span>+<span class="number">0</span>-<span class="number">1</span>.cdh5.<span class="number">4.0</span>.p0.<span class="number">75</span>.el6.x86_64.rpm (可不装)</span><br></pre></td></tr></table></figure>
<p>2.安装，建议上面的顺序</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo rpm -ivh impala-<span class="number">2.2</span>.<span class="number">0</span>+cdh5.<span class="number">4.0</span>+<span class="number">0</span>-<span class="number">1</span>.cdh5.<span class="number">4.0</span>.p0.<span class="number">75</span>.el6.x86_64.rpm  --nodeps --force</span><br></pre></td></tr></table></figure>
<p>由于我的hadoop是通过tar包安装的，所以需要强制安装此rpm，其余正常安装</p>
<p>3.查看一下哪些有impala目录</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[xiaolong@node007011 impala-rpm]$ sudo find / -name impala</span><br><span class="line">/var/<span class="built_in">log</span>/impala</span><br><span class="line">/var/run/impala</span><br><span class="line">/var/lib/impala</span><br><span class="line">/var/lib/alternatives/impala</span><br><span class="line">/usr/lib/impala</span><br><span class="line">/etc/default/impala</span><br><span class="line">/etc/impala</span><br><span class="line">/etc/alternatives/impala</span><br></pre></td></tr></table></figure>
<p>impala默认的安装目录为 <code>/usr/lib/impala</code>，jar包地址为<code>/usr/lib/impala/lib/</code></p>
<p>4.删除默认的不存在软连接，并添加新连接。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">sudo rm -rf /usr/lib/impala/lib/avro*.jar</span><br><span class="line">sudo rm -rf /usr/lib/impala/lib/hadoop-*.jar</span><br><span class="line">sudo rm -rf /usr/lib/impala/lib/hive-*.jar</span><br><span class="line">sudo rm -rf /usr/lib/impala/lib/hbase-*.jar</span><br><span class="line">sudo rm -rf /usr/lib/impala/lib/parquet-hadoop-bundle.jar</span><br><span class="line">sudo rm -rf /usr/lib/impala/lib/sentry-*.jar</span><br><span class="line">sudo rm -rf /usr/lib/impala/lib/zookeeper.jar</span><br><span class="line">sudo rm -rf /usr/lib/impala/lib/libhadoop.so</span><br><span class="line">sudo rm -rf /usr/lib/impala/lib/libhadoop.so.<span class="number">1.0</span>.<span class="number">0</span></span><br><span class="line">sudo rm -rf /usr/lib/impala/lib/libhdfs.so</span><br><span class="line">sudo rm -rf /usr/lib/impala/lib/libhdfs.so.<span class="number">0.0</span>.<span class="number">0</span></span><br></pre></td></tr></table></figure>
<p>5.建立新连接，建议利用脚本运行，自己设一下各个home即可，软连后的名字不能改。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line">sudo ln <span class="operator">-s</span> <span class="variable">$HBASE_HOME</span>/lib/avro-<span class="number">1.7</span>.<span class="number">6</span>-cdh5.<span class="number">4.0</span>.jar /usr/lib/impala/lib/avro.jar</span><br><span class="line">sudo ln <span class="operator">-s</span> <span class="variable">$HADOOP_HOME</span>/share/hadoop/common/lib/hadoop-annotations-<span class="number">2.6</span>.<span class="number">0</span>-cdh5.<span class="number">4.0</span>.jar /usr/lib/impala/lib/hadoop-annotations.jar</span><br><span class="line">sudo ln <span class="operator">-s</span> <span class="variable">$HADOOP_HOME</span>/share/hadoop/common/lib/hadoop-auth-<span class="number">2.6</span>.<span class="number">0</span>-cdh5.<span class="number">4.0</span>.jar /usr/lib/impala/lib/hadoop-auth.jar</span><br><span class="line">sudo ln <span class="operator">-s</span> <span class="variable">$HADOOP_HOME</span>/share/hadoop/tools/lib/hadoop-aws-<span class="number">2.6</span>.<span class="number">0</span>-cdh5.<span class="number">4.0</span>.jar /usr/lib/impala/lib/hadoop-aws.jar</span><br><span class="line">sudo ln <span class="operator">-s</span> <span class="variable">$HADOOP_HOME</span>/share/hadoop/common/hadoop-common-<span class="number">2.6</span>.<span class="number">0</span>-cdh5.<span class="number">4.0</span>.jar /usr/lib/impala/lib/hadoop-common.jar</span><br><span class="line">sudo ln <span class="operator">-s</span> <span class="variable">$HADOOP_HOME</span>/share/hadoop/hdfs/hadoop-hdfs-<span class="number">2.6</span>.<span class="number">0</span>-cdh5.<span class="number">4.0</span>.jar /usr/lib/impala/lib/hadoop-hdfs.jar</span><br><span class="line">sudo ln <span class="operator">-s</span> <span class="variable">$HADOOP_HOME</span>/share/hadoop/mapreduce/hadoop-mapreduce-client-common-<span class="number">2.6</span>.<span class="number">0</span>-cdh5.<span class="number">4.0</span>.jar /usr/lib/impala/lib/hadoop-mapreduce-client-common.jar</span><br><span class="line">sudo ln <span class="operator">-s</span> <span class="variable">$HADOOP_HOME</span>/share/hadoop/mapreduce/hadoop-mapreduce-client-core-<span class="number">2.6</span>.<span class="number">0</span>-cdh5.<span class="number">4.0</span>.jar /usr/lib/impala/lib/hadoop-mapreduce-client-core.jar</span><br><span class="line">sudo ln <span class="operator">-s</span> <span class="variable">$HADOOP_HOME</span>/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-<span class="number">2.6</span>.<span class="number">0</span>-cdh5.<span class="number">4.0</span>.jar /usr/lib/impala/lib/hadoop-mapreduce-client-jobclient.jar</span><br><span class="line">sudo ln <span class="operator">-s</span> <span class="variable">$HADOOP_HOME</span>/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-<span class="number">2.6</span>.<span class="number">0</span>-cdh5.<span class="number">4.0</span>.jar /usr/lib/impala/lib/hadoop-mapreduce-client-shuffle.jar</span><br><span class="line">sudo ln <span class="operator">-s</span> <span class="variable">$HADOOP_HOME</span>/share/hadoop/yarn/hadoop-yarn-api-<span class="number">2.6</span>.<span class="number">0</span>-cdh5.<span class="number">4.0</span>.jar /usr/lib/impala/lib/hadoop-yarn-api.jar</span><br><span class="line">sudo ln <span class="operator">-s</span> <span class="variable">$HADOOP_HOME</span>/share/hadoop/yarn/hadoop-yarn-client-<span class="number">2.6</span>.<span class="number">0</span>-cdh5.<span class="number">4.0</span>.jar /usr/lib/impala/lib/hadoop-yarn-client.jar</span><br><span class="line">sudo ln <span class="operator">-s</span> <span class="variable">$HADOOP_HOME</span>/share/hadoop/yarn/hadoop-yarn-common-<span class="number">2.6</span>.<span class="number">0</span>-cdh5.<span class="number">4.0</span>.jar /usr/lib/impala/lib/hadoop-yarn-common.jar</span><br><span class="line">sudo ln <span class="operator">-s</span> <span class="variable">$HADOOP_HOME</span>/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-<span class="number">2.6</span>.<span class="number">0</span>-cdh5.<span class="number">4.0</span>.jar /usr/lib/impala/lib/hadoop-yarn-server-applicationhistoryservice.jar</span><br><span class="line">sudo ln <span class="operator">-s</span> <span class="variable">$HADOOP_HOME</span>/share/hadoop/yarn/hadoop-yarn-server-common-<span class="number">2.6</span>.<span class="number">0</span>-cdh5.<span class="number">4.0</span>.jar /usr/lib/impala/lib/hadoop-yarn-server-common.jar</span><br><span class="line">sudo ln <span class="operator">-s</span> <span class="variable">$HADOOP_HOME</span>/share/hadoop/yarn/hadoop-yarn-server-nodemanager-<span class="number">2.6</span>.<span class="number">0</span>-cdh5.<span class="number">4.0</span>.jar /usr/lib/impala/lib/hadoop-yarn-server-nodemanager.jar</span><br><span class="line">sudo ln <span class="operator">-s</span> <span class="variable">$HADOOP_HOME</span>/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-<span class="number">2.6</span>.<span class="number">0</span>-cdh5.<span class="number">4.0</span>.jar /usr/lib/impala/lib/hadoop-yarn-server-resourcemanager.jar</span><br><span class="line">sudo ln <span class="operator">-s</span> <span class="variable">$HADOOP_HOME</span>/share/hadoop/yarn/hadoop-yarn-server-web-proxy-<span class="number">2.6</span>.<span class="number">0</span>-cdh5.<span class="number">4.0</span>.jar /usr/lib/impala/lib/hadoop-yarn-server-web-proxy.jar</span><br><span class="line"></span><br><span class="line">sudo ln <span class="operator">-s</span> <span class="variable">$HBASE_HOME</span>/lib/hbase-annotations-<span class="number">1.0</span>.<span class="number">0</span>-cdh5.<span class="number">4.0</span>.jar /usr/lib/impala/lib/hbase-annotations.jar</span><br><span class="line">sudo ln <span class="operator">-s</span> <span class="variable">$HBASE_HOME</span>/lib/hbase-client-<span class="number">1.0</span>.<span class="number">0</span>-cdh5.<span class="number">4.0</span>.jar /usr/lib/impala/lib/hbase-client.jar</span><br><span class="line">sudo ln <span class="operator">-s</span> <span class="variable">$HBASE_HOME</span>/lib/hbase-common-<span class="number">1.0</span>.<span class="number">0</span>-cdh5.<span class="number">4.0</span>.jar /usr/lib/impala/lib/hbase-common.jar</span><br><span class="line">sudo ln <span class="operator">-s</span> <span class="variable">$HBASE_HOME</span>/lib/hbase-protocol-<span class="number">1.0</span>.<span class="number">0</span>-cdh5.<span class="number">4.0</span>.jar /usr/lib/impala/lib/hbase-protocol.jar</span><br><span class="line"></span><br><span class="line">sudo ln <span class="operator">-s</span> <span class="variable">$HIVE_HOME</span>/lib/hive-ant-<span class="number">1.1</span>.<span class="number">0</span>-cdh5.<span class="number">4.0</span>.jar /usr/lib/impala/lib/hive-ant.jar</span><br><span class="line">sudo ln <span class="operator">-s</span> <span class="variable">$HIVE_HOME</span>/lib/hive-beeline-<span class="number">1.1</span>.<span class="number">0</span>-cdh5.<span class="number">4.0</span>.jar /usr/lib/impala/lib/hive-beeline.jar</span><br><span class="line">sudo ln <span class="operator">-s</span> <span class="variable">$HIVE_HOME</span>/lib/hive-common-<span class="number">1.1</span>.<span class="number">0</span>-cdh5.<span class="number">4.0</span>.jar /usr/lib/impala/lib/hive-common.jar</span><br><span class="line">sudo ln <span class="operator">-s</span> <span class="variable">$HIVE_HOME</span>/lib/hive-exec-<span class="number">1.1</span>.<span class="number">0</span>-cdh5.<span class="number">4.0</span>.jar /usr/lib/impala/lib/hive-exec.jar</span><br><span class="line">sudo ln <span class="operator">-s</span> <span class="variable">$HIVE_HOME</span>/lib/hive-hbase-handler-<span class="number">1.1</span>.<span class="number">0</span>-cdh5.<span class="number">4.0</span>.jar /usr/lib/impala/lib/hive-hbase-handler.jar</span><br><span class="line">sudo ln <span class="operator">-s</span> <span class="variable">$HIVE_HOME</span>/lib/hive-metastore-<span class="number">1.1</span>.<span class="number">0</span>-cdh5.<span class="number">4.0</span>.jar /usr/lib/impala/lib/hive-metastore.jar</span><br><span class="line">sudo ln <span class="operator">-s</span> <span class="variable">$HIVE_HOME</span>/lib/hive-serde-<span class="number">1.1</span>.<span class="number">0</span>-cdh5.<span class="number">4.0</span>.jar /usr/lib/impala/lib/hive-serde.jar</span><br><span class="line">sudo ln <span class="operator">-s</span> <span class="variable">$HIVE_HOME</span>/lib/hive-service-<span class="number">1.1</span>.<span class="number">0</span>-cdh5.<span class="number">4.0</span>.jar /usr/lib/impala/lib/hive-service.jar</span><br><span class="line">sudo ln <span class="operator">-s</span> <span class="variable">$HIVE_HOME</span>/lib/hive-shims-common-<span class="number">1.1</span>.<span class="number">0</span>-cdh5.<span class="number">4.0</span>.jar /usr/lib/impala/lib/hive-shims-common.jar</span><br><span class="line">sudo ln <span class="operator">-s</span> <span class="variable">$HIVE_HOME</span>/lib/hive-shims-<span class="number">1.1</span>.<span class="number">0</span>-cdh5.<span class="number">4.0</span>.jar /usr/lib/impala/lib/hive-shims.jar</span><br><span class="line">sudo ln <span class="operator">-s</span> <span class="variable">$HIVE_HOME</span>/lib/hive-shims-scheduler-<span class="number">1.1</span>.<span class="number">0</span>-cdh5.<span class="number">4.0</span>.jar /usr/lib/impala/lib/hive-shims-scheduler.jar</span><br><span class="line"></span><br><span class="line">sudo ln <span class="operator">-s</span> <span class="variable">$HADOOP_HOME</span>/lib/native/libhadoop.so /usr/lib/impala/lib/libhadoop.so</span><br><span class="line">sudo ln <span class="operator">-s</span> <span class="variable">$HADOOP_HOME</span>/lib/native/libhadoop.so.<span class="number">1.0</span>.<span class="number">0</span> /usr/lib/impala/lib/libhadoop.so.<span class="number">1.0</span>.<span class="number">0</span></span><br><span class="line">sudo ln <span class="operator">-s</span> <span class="variable">$HADOOP_HOME</span>/lib/native/libhdfs.so /usr/lib/impala/lib/libhdfs.so</span><br><span class="line">sudo ln <span class="operator">-s</span> <span class="variable">$HADOOP_HOME</span>/lib/native/libhdfs.so.<span class="number">0.0</span>.<span class="number">0</span> /usr/lib/impala/lib/libhdfs.so.<span class="number">0.0</span>.<span class="number">0</span></span><br><span class="line"></span><br><span class="line">sudo ln <span class="operator">-s</span> <span class="variable">$HIVE_HOME</span>/lib/parquet-hadoop-bundle-<span class="number">1.5</span>.<span class="number">0</span>-cdh5.<span class="number">4.0</span>.jar /usr/lib/impala/lib/parquet-hadoop-bundle.jar</span><br><span class="line"></span><br><span class="line">sudo ln <span class="operator">-s</span> <span class="variable">$SENTRY_HOME</span>/lib/sentry-binding-hive-<span class="number">1.4</span>.<span class="number">0</span>-cdh5.<span class="number">4.0</span>.jar /usr/lib/impala/lib/sentry-binding-hive.jar</span><br><span class="line">sudo ln <span class="operator">-s</span> <span class="variable">$SENTRY_HOME</span>/lib/sentry-core-common-<span class="number">1.4</span>.<span class="number">0</span>-cdh5.<span class="number">4.0</span>.jar /usr/lib/impala/lib/sentry-core-common.jar</span><br><span class="line">sudo ln <span class="operator">-s</span> <span class="variable">$SENTRY_HOME</span>/lib/sentry-core-model-db-<span class="number">1.4</span>.<span class="number">0</span>-cdh5.<span class="number">4.0</span>.jar /usr/lib/impala/lib/sentry-core-model-db.jar</span><br><span class="line">sudo ln <span class="operator">-s</span> <span class="variable">$SENTRY_HOME</span>/lib/sentry-policy-common-<span class="number">1.4</span>.<span class="number">0</span>-cdh5.<span class="number">4.0</span>.jar /usr/lib/impala/lib/sentry-policy-common.jar</span><br><span class="line">sudo ln <span class="operator">-s</span> <span class="variable">$SENTRY_HOME</span>/lib/sentry-policy-db-<span class="number">1.4</span>.<span class="number">0</span>-cdh5.<span class="number">4.0</span>.jar /usr/lib/impala/lib/sentry-policy-db.jar</span><br><span class="line">sudo ln <span class="operator">-s</span> <span class="variable">$SENTRY_HOME</span>/lib/sentry-provider-cache-<span class="number">1.4</span>.<span class="number">0</span>-cdh5.<span class="number">4.0</span>.jar /usr/lib/impala/lib/sentry-provider-cache.jar</span><br><span class="line">sudo ln <span class="operator">-s</span> <span class="variable">$SENTRY_HOME</span>/lib/sentry-provider-common-<span class="number">1.4</span>.<span class="number">0</span>-cdh5.<span class="number">4.0</span>.jar /usr/lib/impala/lib/sentry-provider-common.jar</span><br><span class="line">sudo ln <span class="operator">-s</span> <span class="variable">$SENTRY_HOME</span>/lib/sentry-provider-db-<span class="number">1.4</span>.<span class="number">0</span>-cdh5.<span class="number">4.0</span>.jar /usr/lib/impala/lib/sentry-provider-db.jar</span><br><span class="line">sudo ln <span class="operator">-s</span> <span class="variable">$SENTRY_HOME</span>/lib/sentry-provider-file-<span class="number">1.4</span>.<span class="number">0</span>-cdh5.<span class="number">4.0</span>.jar /usr/lib/impala/lib/sentry-provider-file.jar</span><br><span class="line"></span><br><span class="line">sudo ln <span class="operator">-s</span> <span class="variable">$ZOOKEEPER_HOME</span>/zookeeper-<span class="number">3.4</span>.<span class="number">5</span>-cdh5.<span class="number">3.3</span>.jar /usr/lib/impala/lib/zookeeper.jar</span><br></pre></td></tr></table></figure>
<hr>
<h2 id="u914D_u7F6E"><a href="#u914D_u7F6E" class="headerlink" title="配置"></a>配置</h2><p>1.修改<code>/etc/default/bigtop-utils</code></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> JAVA_HOME=/usr/<span class="built_in">local</span>/datacenter/jdk1.<span class="number">7.0</span>_79</span><br></pre></td></tr></table></figure>
<p>2.修改 <code>/etc/default/impala</code> 默认文件，除了修改环境，还需要修改启动参数，显示传递给 statestored 和 catalogd 它们各自需要的ip和port ，不然都默认 localhost ，分开部署会有问题的。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">IMPALA_CATALOG_SERVICE_HOST=<span class="number">192.168</span>.<span class="number">7.11</span></span><br><span class="line">IMPALA_STATE_STORE_HOST=<span class="number">192.168</span>.<span class="number">7.11</span></span><br><span class="line"></span><br><span class="line">IMPALA_CATALOG_ARGS=<span class="string">" -log_dir=<span class="variable">$&#123;IMPALA_LOG_DIR&#125;</span> -catalog_service_host=<span class="variable">$&#123;IMPALA_CATALOG_SERVICE_HOST&#125;</span> -state_store_host=<span class="variable">$&#123;IMPALA_STATE_STORE_HOST&#125;</span>"</span></span><br><span class="line">IMPALA_STATE_STORE_ARGS=<span class="string">" -log_dir=<span class="variable">$&#123;IMPALA_LOG_DIR&#125;</span> -state_store_host=<span class="variable">$&#123;IMPALA_STATE_STORE_HOST&#125;</span> -state_store_port=<span class="variable">$&#123;IMPALA_STATE_STORE_PORT&#125;</span>  -catalog_service_host=<span class="variable">$&#123;IMPALA_CATALOG_SERVICE_HOST&#125;</span> -catalog_service_port=<span class="variable">$&#123;CATALOG_SERVICE_PORT&#125;</span> "</span></span><br><span class="line"></span><br><span class="line">MYSQL_CONNECTOR_JAR=/usr/<span class="built_in">local</span>/datacenter/hive/lib/mysql-connector-java-<span class="number">5.1</span>.<span class="number">34</span>.jar</span><br></pre></td></tr></table></figure>
<p>3.根据实际环境修改impala相关脚本文件 <code>/etc/init.d/impala-state-store</code>、<code>/etc/init.d/impala-server</code>、<code>/etc/init.d/impala-catalog</code>，修改其中两处跟用户相关的地方</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">###编者注：这里默认是impala</span></span><br><span class="line">SVC_USER=<span class="string">"hadoop"</span></span><br><span class="line"><span class="comment">###编者注：这里默认是impala</span></span><br><span class="line">install <span class="operator">-d</span> -m <span class="number">0755</span> -o hadoop -g hadoop /var/run/impala <span class="number">1</span>&gt;/dev/null <span class="number">2</span>&gt;&amp;<span class="number">1</span> || :</span><br></pre></td></tr></table></figure>
<p>为保险起见 手工建立日志目录并修改权限 <code>mkdir /var/log/impala &amp;&amp; chmod -R 777 /var/log/impala</code></p>
<p>4.修改 hdfs-site.xml 这一处就够了</p>
<figure class="highlight xml"><figcaption><span>hdfs-site.xml</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">name</span>&gt;</span>dfs.client.read.shortcircuit<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">value</span>&gt;</span>true<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">name</span>&gt;</span>dfs.domain.socket.path<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">value</span>&gt;</span>/var/run/hadoop-hdfs/dn<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="title">name</span>&gt;</span>dfs.datanode.hdfs-blocks-metadata.enabled<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="title">value</span>&gt;</span>true<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="title">name</span>&gt;</span>dfs.client.use.legacy.blockreader.local<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="title">value</span>&gt;</span>false<span class="tag">&lt;/<span class="title">value</span>&gt;</span> ###编者注：这里需要为false ，官网文档错了</span><br><span class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="title">name</span>&gt;</span>dfs.datanode.data.dir.perm<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="title">value</span>&gt;</span>750<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="title">name</span>&gt;</span>dfs.block.local-path-access.user<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="title">value</span>&gt;</span>hadoop<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="title">name</span>&gt;</span>dfs.client.file-block-storage-locations.timeout<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="title">value</span>&gt;</span>3000<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>官网的文档有一处错误，<code>dfs.client.use.legacy.blockreader.local</code> 属性应该为 false，文档中是 true</p>
<p><a href="http://www.cloudera.com/content/www/en-us/documentation/archive/impala/2-x/2-1-x/topics/impala_config_performance.html?scroll=config_performance" target="_blank" rel="external">http://www.cloudera.com/content/www/en-us/documentation/archive/impala/2-x/2-1-x/topics/impala_config_performance.html?scroll=config_performance </a></p>
<p>注意每个节点创建 <code>hdfs-site.xml</code> 里指定的 <code>/var/run/hdfs-sockets/dn</code> 权限</p>
<p>5.impalad的配置文件路径由环境变量IMPALA_CONF_DIR指定，默认为 <code>/etc/impala/conf</code>，拷贝配置或建立软连接 的<code>hive-site.xml</code>、<code>core-site.xml</code>、<code>hdfs-site.xml</code>、<code>hbase-site.xml</code>文件至<code>/etc/impala/conf</code>目录下。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo ln <span class="operator">-s</span> /usr/<span class="built_in">local</span>/datacenter/hadoop/etc/hadoop/core-site.xml /etc/impala/conf/</span><br><span class="line">sudo ln <span class="operator">-s</span> /usr/<span class="built_in">local</span>/datacenter/hadoop/etc/hadoop/hdfs-site.xml /etc/impala/conf/</span><br><span class="line">sudo ln <span class="operator">-s</span> /usr/<span class="built_in">local</span>/datacenter/hive/conf/hive-site.xml /etc/impala/conf/</span><br><span class="line">sudo ln <span class="operator">-s</span> /usr/<span class="built_in">local</span>/datacenter/hbase/conf/hbase-site.xml /etc/impala/conf/</span><br></pre></td></tr></table></figure>
<hr>
<h2 id="u542F_u52A8"><a href="#u542F_u52A8" class="headerlink" title="启动"></a>启动</h2><p>启动是最诡异的地方，由于官网建议statestored和catalogd在同一台机器上，所以ip一样，但端口不一样，然后利用service启动就只能启动一个，可以通过 pid file 看到是同一个进程.</p>
<p>1.在 <code>statestored</code> 和 <code>catalogd</code> 机器上启动「impala主节点」， 建议启动时，一个利用service 启动一个利用手工启动</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /usr/lib/impala</span><br><span class="line">nohup /usr/lib/impala/sbin/statestored -log_dir=/var/<span class="built_in">log</span>/impala -state_store_host=<span class="number">192.168</span>.<span class="number">7.11</span> -state_store_port=<span class="number">24000</span> -catalog_service_host=<span class="number">192.168</span>.<span class="number">7.11</span>:<span class="number">26000</span> -catalog_service_port=<span class="number">26000</span> &amp;</span><br><span class="line"></span><br><span class="line">sudo service impala-catalog start</span><br></pre></td></tr></table></figure>
<p>碰见问题 ,缺少jdk so文件，缺少classpath impala ，所以修改环境变量</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> LD_LIBRARY_PATH=<span class="variable">$LD_LIBRARY_PATH</span>:/usr/<span class="built_in">local</span>/lib:/usr/<span class="built_in">local</span>/datacenter/jdk1.<span class="number">7.0</span>_79/jre/lib/amd64:/usr/<span class="built_in">local</span>/datacenter/jdk1.<span class="number">7.0</span>_79/jre/lib/amd64/server:/usr/lib/impala/lib/</span><br><span class="line"></span><br><span class="line"><span class="built_in">export</span> CLASSPATH=<span class="variable">$CLASSPATH</span>:<span class="variable">$JAVA_HOME</span>/lib:<span class="variable">$JAVA_HOME</span>/jre/lib:/usr/lib/impala/lib</span><br></pre></td></tr></table></figure>
<p>2.在「impala从节点」上，启动 <code>serverd</code></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo service  impala-server start</span><br></pre></td></tr></table></figure>
<p>3.访问主节点 25010 端口，得到statestore web ui ，其中比较有参考意义的，logs 和 varz 里面有环境变量的信息<br><img src="/images/impala/20151227/1.png" alt=""></p>
<p>4.访问从节点的 25000 ，得到 serverd 的 web ui<br><img src="/images/impala/20151227/2.png" alt=""></p>
<hr>
<h2 id="u6267_u884C"><a href="#u6267_u884C" class="headerlink" title="执行"></a>执行</h2><p>1.进入<code>impala-shell</code>，即连接的本机的 impala-server</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@node007012 ~]<span class="comment"># impala-shell</span></span><br><span class="line">Starting Impala Shell without Kerberos authentication</span><br><span class="line">Connected to node007012:<span class="number">21000</span></span><br><span class="line">Server version: impalad version <span class="number">2.2</span>.<span class="number">0</span>-cdh5 RELEASE (build <span class="number">2</span>ffd73a4255cefd521362ffe1cfb37463f67f75c)</span><br></pre></td></tr></table></figure>
<p>2.同步hive元信息，以便感知已存在的hive库表</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[node007012:<span class="number">21000</span>] &gt; invalidate metadata;</span><br><span class="line">Query: invalidate metadata</span><br><span class="line"></span><br><span class="line">Fetched <span class="number">0</span> row(s) <span class="keyword">in</span> <span class="number">3.98</span>s</span><br></pre></td></tr></table></figure>
<p>3.执行一下ddl语句，看是否正常</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">[node007012:<span class="number">21000</span>] &gt; SHOW DATABASES;</span><br><span class="line">Query: show DATABASES</span><br><span class="line">+------------------+</span><br><span class="line">| name             |</span><br><span class="line">+------------------+</span><br><span class="line">| _impala_builtins |</span><br><span class="line">| default          |</span><br><span class="line">| <span class="built_in">test</span>             |</span><br><span class="line">+------------------+</span><br><span class="line">Fetched <span class="number">3</span> row(s) <span class="keyword">in</span> <span class="number">0.01</span>s</span><br><span class="line">[node007012:<span class="number">21000</span>] &gt; USE <span class="built_in">test</span>;</span><br><span class="line">Query: use <span class="built_in">test</span></span><br><span class="line">[node007012:<span class="number">21000</span>] &gt; show tables;</span><br><span class="line">Query: show tables</span><br><span class="line">+--------------+</span><br><span class="line">| name         |</span><br><span class="line">+--------------+</span><br><span class="line">| g_info       |</span><br><span class="line">| g_info_pa    |</span><br><span class="line">| g_info_sqoop |</span><br><span class="line">| pokes        |</span><br><span class="line">| pokes_pa     |</span><br><span class="line">| sales_order  |</span><br><span class="line">| t1           |</span><br><span class="line">| t2           |</span><br><span class="line">| user_sqoop   |</span><br><span class="line">+--------------+</span><br><span class="line">Fetched <span class="number">9</span> row(s) <span class="keyword">in</span> <span class="number">0.01</span>s</span><br></pre></td></tr></table></figure>
<p>4.验证查询语句，其中g_info表是textfile，g_info_pa 是parquetfile 的，可见效率差异性</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">[node007012:<span class="number">21000</span>] &gt; select count(*) from g_info;</span><br><span class="line">Query: select count(*) from g_info</span><br><span class="line">+----------+</span><br><span class="line">| count(*) |</span><br><span class="line">+----------+</span><br><span class="line">| <span class="number">1222023</span>  |</span><br><span class="line">+----------+</span><br><span class="line">WARNINGS: Unknown disk id.  This will negatively affect performance. Check your hdfs settings to <span class="built_in">enable</span> block location metadata. (<span class="number">1</span> of <span class="number">8</span> similar)</span><br><span class="line"></span><br><span class="line">Fetched <span class="number">1</span> row(s) <span class="keyword">in</span> <span class="number">44.13</span>s</span><br><span class="line">[node007012:<span class="number">21000</span>] &gt; select count(*) from g_info_pa;</span><br><span class="line">Query: select count(*) from g_info_pa</span><br><span class="line">+----------+</span><br><span class="line">| count(*) |</span><br><span class="line">+----------+</span><br><span class="line">| <span class="number">1192760</span>  |</span><br><span class="line">+----------+</span><br><span class="line">WARNINGS: Unknown disk id.  This will negatively affect performance. Check your hdfs settings to <span class="built_in">enable</span> block location metadata. (<span class="number">1</span> of <span class="number">4</span> similar)</span><br><span class="line"></span><br><span class="line">Fetched <span class="number">1</span> row(s) <span class="keyword">in</span> <span class="number">4.73</span>s</span><br></pre></td></tr></table></figure>
<p>这个警告日志里报错的是，待查</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">W1226 <span class="number">19</span>:<span class="number">14</span>:<span class="number">35.052875</span> <span class="number">30015</span> DomainSocketFactory.java:<span class="number">167</span>] error creating DomainSocket</span><br><span class="line">Java exception follows:</span><br><span class="line">java.net.ConnectException: connect(<span class="number">2</span>) error: Connection refused when trying to connect to <span class="string">'/var/run/hdfs-sockets/dn'</span></span><br></pre></td></tr></table></figure>
<p>参考：</p>
<ul>
<li><p><a href="http://blog.csdn.net/zhong_han_jun/article/details/45563505" target="_blank" rel="external">http://blog.csdn.net/zhong_han_jun/article/details/45563505</a></p>
</li>
<li><p><a href="http://www.cnblogs.com/chenz/articles/3629698.html" target="_blank" rel="external">http://www.cnblogs.com/chenz/articles/3629698.html</a></p>
</li>
<li><p><a href="http://www.cloudera.com/content/cloudera/zh-CN/documentation/core/v5-3-x/topics/impala_config_options.html" target="_blank" rel="external">http://www.cloudera.com/content/cloudera/zh-CN/documentation/core/v5-3-x/topics/impala_config_options.html</a></p>
</li>
<li><p><a href="http://stackoverflow.com/questions/23469758/aws-emr-impala-daemon-issue" target="_blank" rel="external">http://stackoverflow.com/questions/23469758/aws-emr-impala-daemon-issue</a></p>
</li>
</ul>
]]></content>
    <summary type="html">
    <![CDATA[impala安装配置]]>
    
    </summary>
    
      <category term="impala" scheme="http://yoursite.com/tags/impala/"/>
    
      <category term="impala" scheme="http://yoursite.com/categories/impala/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[tez on yarn]]></title>
    <link href="http://yoursite.com/blog/2015/12/27/tez-on-yarn/"/>
    <id>http://yoursite.com/blog/2015/12/27/tez-on-yarn/</id>
    <published>2015-12-27T09:47:23.000Z</published>
    <updated>2016-01-16T12:08:52.000Z</updated>
    <content type="html"><![CDATA[<p>介绍 tez 框架运行在 yarn上</p>
<a id="more"></a>
<p>tez是一个底层框架，跟mr不同之处，是在于 tez 将mr处理进行了优化，然后再跑优化后的 mr DAG，因此效率会快一些</p>
<hr>
<h2 id="u73AF_u5883_u51C6_u5907"><a href="#u73AF_u5883_u51C6_u5907" class="headerlink" title="环境准备"></a>环境准备</h2><p>1.下载源码，建议0.6.0 以上，因为Tez以0.6.0版本为分隔点，很多功能例如web ui 都仅支持 0.6.0 以上的版本</p>
<p><a href="http://tez.apache.org/releases/" target="_blank" rel="external"><font color="#2798a2">官网网址 </font></a>   （tez 没有打包好的二进制可运行包，需要自己通过源码编译）</p>
<p><a href="http://pan.baidu.com/s/1nukfT49" target="_blank" rel="external"><font color="#2798a2">我共享的百度云地址 </font></a></p>
<p>2.jdk1.7 和 maven3 , 也共享一下jdk1.7 的<a href="http://pan.baidu.com/s/1jGPTsu6" target="_blank" rel="external"><font color="#2798a2">地址</font></a></p>
<p>3.protobuf 2.5.0 编译安装（如果已有则跳过）, protobuf 的代码在 googlecode 上我共享一下 <a href="http://pan.baidu.com/s/1eRisu9G" target="_blank" rel="external"><font color="#2798a2">我的百度云地址</font></a></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo ./configure &amp;&amp; sudo make &amp;&amp; sudo make install</span><br></pre></td></tr></table></figure>
<p>4.安装 <code>npm  install phantomjs -g</code> 以便构建 tez-ui.war (npm是 nodejs的包管理器，如果没有npm命令，安装一个较新版本的nodejs就可以了)</p>
<hr>
<h2 id="u7F16_u8BD1"><a href="#u7F16_u8BD1" class="headerlink" title="编译"></a>编译</h2><p>1.编译源码，进入 tez-src 目录，根据自己的hadoop版本，修改顶层pom.xml 里 <code>hadoop.version</code> 然后打包</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mvn clean package -DskipTests=<span class="literal">true</span> -Dmaven.javadoc.skip=<span class="literal">true</span></span><br></pre></td></tr></table></figure>
<p>2.打包好后，把tez-dist/target 下的 tez-0.7.0.tar.gz 上传到 hdfs 上的一个自定义目录里</p>
<p>3.下载tomcat 7.0.67 或以上，并解压，把 <code>tez-dist/target/tez-0.7.0/tez-ui-0.7.0.war</code> 解压到 <code>$TOMCAT_HOME/webapp</code> 下，形成 <code>$TOMCAT_HOME/webapp/tez-ui</code> 这样的结构（可以删除tomcat自带的webapp下的项目，也可以通过 <code>$TOMCAT_HOME/conf/server.xml</code> 指定CONTEXT 配到指定 tez-ui-0.7.0.war解压后的目录）</p>
<p>4.编辑<code>$TOMCAT_HOME/conf/web.xml</code>，并添加以下内容，不然访问context会报404</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="title">filter</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="title">filter-name</span>&gt;</span>CorsFilter<span class="tag">&lt;/<span class="title">filter-name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="title">filter-class</span>&gt;</span>org.apache.catalina.filters.CorsFilter<span class="tag">&lt;/<span class="title">filter-class</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">filter</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">filter-mapping</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="title">filter-name</span>&gt;</span>CorsFilter<span class="tag">&lt;/<span class="title">filter-name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="title">url-pattern</span>&gt;</span>/*<span class="tag">&lt;/<span class="title">url-pattern</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">filter-mapping</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>5.把编译好的 tez-0.7.0.tar.gz 分发到所有节点上，并解压到路径一致的目录里，以便设置 TEZ_HOME 环境变量</p>
<hr>
<h2 id="u914D_u7F6E"><a href="#u914D_u7F6E" class="headerlink" title="配置"></a>配置</h2><p>1.在 <code>$HADOOP_HOME/etc/hadoop/</code> 目录下创建 <code>tez-site.xml</code> 文件并编辑 <code>tez-site.xml</code> 文件内容如下</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="pi">&lt;?xml version="1.0" encoding="UTF-8"?&gt;</span></span><br><span class="line"><span class="pi">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="title">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="title">name</span>&gt;</span>tez.lib.uris<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="title">value</span>&gt;</span>$&#123;fs.defaultFS&#125;/share/tez-0.7.0.tar.gz<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="title">name</span>&gt;</span>tez.history.logging.service.class<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="title">value</span>&gt;</span>org.apache.tez.dag.history.logging.ats.ATSHistoryLoggingService<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="title">description</span>&gt;</span>URL for where the Tez UI is hosted<span class="tag">&lt;/<span class="title">description</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="title">name</span>&gt;</span>tez.tez-ui.history-url.base<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="title">value</span>&gt;</span>http://127.0.0.1:8080/tez-ui/<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>注：这里的127.0.0.1 是tomcat的ip地址，同时8080是tomcat的端口（也可以通过tomcat指定为其他端口），tez-ui是tomcat的 context 。而 <code>tez.lib.uris</code> 是刚才上传到hdfs的文件</p>
<p>2.编辑 hadoop-env.sh</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> TEZ_HOME=/usr/<span class="built_in">local</span>/datacenter/tez-<span class="number">0.7</span>.<span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> jar <span class="keyword">in</span> `ls <span class="variable">$TEZ_HOME</span> |grep jar`; <span class="keyword">do</span></span><br><span class="line">    <span class="built_in">export</span> HADOOP_CLASSPATH=<span class="variable">$HADOOP_CLASSPATH</span>:<span class="variable">$TEZ_HOME</span>/<span class="variable">$jar</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"><span class="keyword">for</span> jar <span class="keyword">in</span> `ls <span class="variable">$TEZ_HOME</span>/lib`; <span class="keyword">do</span></span><br><span class="line">    <span class="built_in">export</span> HADOOP_CLASSPATH=<span class="variable">$HADOOP_CLASSPATH</span>:<span class="variable">$TEZ_HOME</span>/lib/<span class="variable">$jar</span></span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure>
<p>3.编辑 <code>mapred-site.xml</code> ，修改为 tez 类型</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">value</span>&gt;</span>yarn-tez<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>4.修改 <code>yarn-site.xml</code> 添加如下</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="title">description</span>&gt;</span>Indicate to clients whether Timeline service is enabled or not. If enabled, the TimelineClient library used by end-users will post entities and events to the Timeline server.<span class="tag">&lt;/<span class="title">description</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="title">name</span>&gt;</span>yarn.timeline-service.enabled<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="title">value</span>&gt;</span>true<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="title">description</span>&gt;</span>The hostname of the Timeline service web application.<span class="tag">&lt;/<span class="title">description</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="title">name</span>&gt;</span>yarn.timeline-service.hostname<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="title">value</span>&gt;</span>127.0.0.1<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="title">description</span>&gt;</span>Enables cross-origin support (CORS) for web services where cross-origin web response headers are needed. For example, javascript making a web services request to the timeline server.<span class="tag">&lt;/<span class="title">description</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="title">name</span>&gt;</span>yarn.timeline-service.http-cross-origin.enabled<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="title">value</span>&gt;</span>true<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="title">description</span>&gt;</span>Publish YARN information to Timeline Server<span class="tag">&lt;/<span class="title">description</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="title">name</span>&gt;</span>yarn.resourcemanager.system-metrics-publisher.enabled<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="title">value</span>&gt;</span>true<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">description</span>&gt;</span>Address for the Timeline server to start the RPC server.<span class="tag">&lt;/<span class="title">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">name</span>&gt;</span>yarn.timeline-service.address<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">value</span>&gt;</span>$&#123;yarn.timeline-service.hostname&#125;:10200<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">description</span>&gt;</span>The http address of the Timeline service web application.<span class="tag">&lt;/<span class="title">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">name</span>&gt;</span>yarn.timeline-service.webapp.address<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">value</span>&gt;</span>$&#123;yarn.timeline-service.hostname&#125;:8188<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">description</span>&gt;</span>The https address of the Timeline service web application.<span class="tag">&lt;/<span class="title">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">name</span>&gt;</span>yarn.timeline-service.webapp.https.address<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">value</span>&gt;</span>$&#123;yarn.timeline-service.hostname&#125;:8190<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">description</span>&gt;</span>Handler thread count to serve the client RPC requests.<span class="tag">&lt;/<span class="title">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">name</span>&gt;</span>yarn.timeline-service.handler-thread-count<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">value</span>&gt;</span>10<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">description</span>&gt;</span>Enables cross-origin support (CORS) for web services where</span><br><span class="line">    cross-origin web response headers are needed. For example, javascript making</span><br><span class="line">    a web services request to the timeline server.<span class="tag">&lt;/<span class="title">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">name</span>&gt;</span>yarn.timeline-service.http-cross-origin.enabled<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">value</span>&gt;</span>true<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">description</span>&gt;</span>Comma separated list of origins that are allowed for web</span><br><span class="line">    services needing cross-origin (CORS) support. Wildcards (*) and patterns</span><br><span class="line">    allowed<span class="tag">&lt;/<span class="title">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">name</span>&gt;</span>yarn.timeline-service.http-cross-origin.allowed-origins<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">value</span>&gt;</span>*<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">description</span>&gt;</span>Comma separated list of methods that are allowed for web</span><br><span class="line">    services needing cross-origin (CORS) support.<span class="tag">&lt;/<span class="title">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">name</span>&gt;</span>yarn.timeline-service.http-cross-origin.allowed-methods<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">value</span>&gt;</span>GET,POST,HEAD,OPTIONS<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">description</span>&gt;</span>Comma separated list of headers that are allowed for web</span><br><span class="line">    services needing cross-origin (CORS) support.<span class="tag">&lt;/<span class="title">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">name</span>&gt;</span>yarn.timeline-service.http-cross-origin.allowed-headers<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">value</span>&gt;</span>X-Requested-With,Content-Type,Accept,Origin,Access-Control-Allow-Origin<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">description</span>&gt;</span>The number of seconds a pre-flighted request can be cached</span><br><span class="line">    for web services needing cross-origin (CORS) support.<span class="tag">&lt;/<span class="title">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">name</span>&gt;</span>yarn.timeline-service.http-cross-origin.max-age<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">value</span>&gt;</span>1800<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>注：跟官网给出的文档有3处不同</p>
<ul>
<li><code>yarn.timeline-service.http-cross-origin.enabled</code> 默认为false，我修改为true让其支持跨域访问</li>
<li><code>yarn.timeline-service.http-cross-origin.allowed-headers</code> 里新增了Access-Control-Allow-Origin 这种 header</li>
<li><code>yarn.timeline-service.http-cross-origin.allowed-methods</code> 里新增了 OPTIONS 方法</li>
</ul>
<p>以下内容我没有添加，没有试验，不过在 yarn-default.xml 里可以看到，即使没有显示写 ttl 配置，也有默认值</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="title">description</span>&gt;</span>Indicate to ResourceManager as well as clients whether</span><br><span class="line">  history-service is enabled or not. If enabled, ResourceManager starts</span><br><span class="line">  recording historical data that Timelien service can consume. Similarly,</span><br><span class="line">  clients can redirect to the history service when applications</span><br><span class="line">  finish if this is enabled.<span class="tag">&lt;/<span class="title">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="title">name</span>&gt;</span>yarn.timeline-service.generic-application-history.enabled<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="title">value</span>&gt;</span>true<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="title">description</span>&gt;</span>Store class name for history store, defaulting to file system</span><br><span class="line">  store<span class="tag">&lt;/<span class="title">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="title">name</span>&gt;</span>yarn.timeline-service.generic-application-history.store-class<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="title">value</span>&gt;</span>org.apache.hadoop.yarn.server.applicationhistoryservice.FileSystemApplicationHistoryStore<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="title">description</span>&gt;</span>Store class name for timeline store.<span class="tag">&lt;/<span class="title">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="title">name</span>&gt;</span>yarn.timeline-service.store-class<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="title">value</span>&gt;</span>org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="title">description</span>&gt;</span>Enable age off of timeline store data.<span class="tag">&lt;/<span class="title">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="title">name</span>&gt;</span>yarn.timeline-service.ttl-enable<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="title">value</span>&gt;</span>true<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="title">description</span>&gt;</span>Time to live for timeline store data in milliseconds.<span class="tag">&lt;/<span class="title">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="title">name</span>&gt;</span>yarn.timeline-service.ttl-ms<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="title">value</span>&gt;</span>6048000000<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p><img src="/images/tez/20151227/1.png" alt=""></p>
<hr>
<h2 id="u8FD0_u884C"><a href="#u8FD0_u884C" class="headerlink" title="运行"></a>运行</h2><p>1.重启 yarn 及 jobhistory</p>
<ul>
<li><p>stop-yarn.sh 和 start-yarn.sh</p>
</li>
<li><p>yarn-daemon.sh start timelineserver 和 yarn-daemon.sh stop timelineserver</p>
</li>
</ul>
<p>2.验证 tez 计算框架</p>
<p>先上传一个文本文件</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hadoop dfs -mkdir -p /<span class="built_in">test</span>/out</span><br><span class="line">hadoop dfs -put LICENSE.txt /<span class="built_in">test</span>/LICENSE.txt</span><br></pre></td></tr></table></figure>
<p>进入  <code>tez-dist/target/tez-0.7.0/</code> ，执行 <code>hadoop jar ./tez-examples-0.7.0.jar orderedwordcount /test/LICENSE.txt /test/out</code></p>
<p><img src="/images/tez/20151227/2.png" alt=""></p>
<p>我碰见第一次未执行成功，是因为TEZ去找 <code>/bin/java</code> 了，可以把当前jdk做个软连接直接连接到 <code>/bin/java</code> 就可以了</p>
<p>启动tomcat 访问 <a href="http://127.0.0.1:8080/tez-ui" target="_blank" rel="external">http://127.0.0.1:8080/tez-ui</a></p>
<p><img src="/images/tez/20151227/3.png" alt=""><br><img src="/images/tez/20151227/4.png" alt=""><br><img src="/images/tez/20151227/5.png" alt=""><br><img src="/images/tez/20151227/6.png" alt=""></p>
<hr>
<h2 id="tez_on_hive"><a href="#tez_on_hive" class="headerlink" title="tez on hive"></a>tez on hive</h2><p>在hive里使用tez十分方便，有2种方式</p>
<p>1.进入hive，然后执行 <code>set hive.execution.engine=tez;</code> ，然后就可以正常使用</p>
<p>2.编写<code>hive-site.xml</code>，修改下面属性，这样就全局影响了</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">name</span>&gt;</span>hive.execution.engine<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">value</span>&gt;</span>tez<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">description</span>&gt;</span></span><br><span class="line">      Expects one of [mr, tez, spark].</span><br><span class="line">      Chooses execution engine. Options are: mr (Map reduce, default), tez (hadoop 2 only), spark</span><br><span class="line">    <span class="tag">&lt;/<span class="title">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
<hr>
<h2 id="u5DF2_u78B0_u89C1_u95EE_u9898"><a href="#u5DF2_u78B0_u89C1_u95EE_u9898" class="headerlink" title="已碰见问题"></a>已碰见问题</h2><p>1.Failed to execute goal com.github.eirslett:frontend-maven-plugin ~ A required class was missing</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[ERROR] Failed to execute goal com.github.eirslett:frontend-maven-plugin:<span class="number">0.0</span>.<span class="number">16</span>:install-node-and-npm</span><br><span class="line">(install node and npm) on project tez-ui: Execution install node and npm of goal</span><br><span class="line">com.github.eirslett:frontend-maven-plugin:<span class="number">0.0</span>.<span class="number">16</span>:install-node-and-npm failed:</span><br><span class="line">A required class was missing <span class="keyword">while</span> executing</span><br><span class="line">com.github.eirslett:frontend-maven-plugin:<span class="number">0.0</span>.<span class="number">16</span>:install-node-and-npm:org/slf4j/helpers/MarkerIgnoringBase</span><br></pre></td></tr></table></figure>
<blockquote>
<p>Reason: Eirslett frontend-maven-plugin version(0.0.22 in above case) is not compatible with current maven version<br>Solution: Force the expected plugging version while building tez-ui: mvn clean package -Dfrontend-maven-plugin.version=0.0.XX<br>                       For maven version &lt; 3.1 the frontend-maven-plugin version has to be &lt;= 0.0.22<br>                       For maven version &gt;=3.1 the frontend-maven-plugin version has to be &gt;= 0.0.23</p>
</blockquote>
<p>maven 版本 和 插件不兼容, 修改 顶层 pom.xml</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="title">plugin</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="title">groupId</span>&gt;</span>com.github.eirslett<span class="tag">&lt;/<span class="title">groupId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="title">artifactId</span>&gt;</span>frontend-maven-plugin<span class="tag">&lt;/<span class="title">artifactId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="title">version</span>&gt;</span>0.0.23<span class="tag">&lt;/<span class="title">version</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="title">plugin</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p><a href="https://cwiki.apache.org/confluence/display/TEZ/Build+errors+and+solutions" target="_blank" rel="external">https://cwiki.apache.org/confluence/display/TEZ/Build+errors+and+solutions</a></p>
<p>2.网络不好，下载不下来包，重新编译几次就可以了</p>
<p>3.Classnotfound MRVersion 的问题，解决不掉，重新换一个hive就行了，估计是环境问题</p>
<h3 id="u540E_u8BB0_MRVersion_u7684_u95EE_u9898_u89E3_u51B3_u6389_u4E86"><a href="#u540E_u8BB0_MRVersion_u7684_u95EE_u9898_u89E3_u51B3_u6389_u4E86" class="headerlink" title="后记 MRVersion的问题解决掉了"></a>后记 MRVersion的问题解决掉了</h3><ul>
<li><a href="https://issues.apache.org/jira/browse/TEZ-3030" target="_blank" rel="external">https://issues.apache.org/jira/browse/TEZ-3030</a></li>
<li><a href="https://issues.apache.org/jira/browse/TEZ-3031" target="_blank" rel="external">https://issues.apache.org/jira/browse/TEZ-3031</a></li>
</ul>
<p>在 <code>hive-env.sh</code> 里添加</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> HIVE_AUX_JARS_PATH=hadoop-core-<span class="number">2.6</span>.<span class="number">0</span>-mr1-cdh5.<span class="number">4.0</span>.jar</span><br><span class="line"><span class="built_in">export</span> HIVE_AUX_JARS_PATH=/usr/<span class="built_in">local</span>/datacenter/phoenix/lib/hadoop-core-<span class="number">2.6</span>.<span class="number">0</span>-mr1-cdh5.<span class="number">4.0</span>.jar:/usr/<span class="built_in">local</span>/datacenter/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-<span class="number">2.6</span>.<span class="number">0</span>-cdh5.<span class="number">4.0</span>.jar</span><br></pre></td></tr></table></figure>
<p>如果通过 hue -&gt; beeswax 查询，需要把这2个jar 放到 <code>HIVE_CLASSPATH</code> 环境变量里，并重启hiveserver2，即可</p>
<p>tez指定队列，需要在<code>tez-site.xml</code>里</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="title">name</span>&gt;</span>tez.queue.name<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="title">value</span>&gt;</span>tez<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
]]></content>
    <summary type="html">
    <![CDATA[tez on yarn]]>
    
    </summary>
    
      <category term="tez" scheme="http://yoursite.com/tags/tez/"/>
    
      <category term="tez" scheme="http://yoursite.com/categories/tez/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[hive hbase handler]]></title>
    <link href="http://yoursite.com/blog/2015/12/27/hive-hbase-handler/"/>
    <id>http://yoursite.com/blog/2015/12/27/hive-hbase-handler/</id>
    <published>2015-12-27T09:43:20.000Z</published>
    <updated>2016-01-16T03:04:40.000Z</updated>
    <content type="html"><![CDATA[<p>介绍 hive 和 hbase 互通</p>
<a id="more"></a>
<p>利用hive 查询 hbase 已有数据，或向hbase导入数据</p>
<hr>
<h2 id="u4ECB_u7ECD"><a href="#u4ECB_u7ECD" class="headerlink" title="介绍"></a>介绍</h2><p>主要是利用 hive 的 「hive-hbase-hander」模块进行处理。我用的是 hive-1.1.0，已经可以直接使用这个模块了，<br>听说低版本的hive，还需要将这个模块，单独打成jar后，利用jar包来处理（最后我将简略写一下构建此jar的过程）。</p>
<hr>
<h2 id="hive__u4E2D__u521B_u5EFA_hbase__u8868"><a href="#hive__u4E2D__u521B_u5EFA_hbase__u8868" class="headerlink" title="hive 中 创建 hbase 表"></a>hive 中 创建 hbase 表</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> hbase_table_1(<span class="keyword">key</span> <span class="built_in">int</span>, <span class="keyword">value</span> <span class="keyword">string</span>) \</span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">BY</span> <span class="string">'org.apache.hadoop.hive.hbase.HBaseStorageHandler'</span> \</span><br><span class="line"><span class="keyword">WITH</span> SERDEPROPERTIES (<span class="string">"hbase.columns.mapping"</span> =<span class="string">":key,cf1:"</span>) \</span><br><span class="line">TBLPROPERTIES(<span class="string">"hbase.table.name"</span> = <span class="string">"aaa"</span>);</span></span><br></pre></td></tr></table></figure>
<p>这句话的意思，新建一个 hive表 叫 <code>hbase_table_1</code> ，新建一个 hbase 表叫 <code>aaa</code> ，然后它们的映射关系是，<br>hive中key -&gt; base中 key ，hive中value -&gt; base中cf1</p>
<p>然后我们可以从一个已存在的hive表中，给我们新建的 hbase_table_1 导入数据，这里虚构一个表叫 <code>pokes</code></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> hbase_table_1 <span class="keyword">select</span> * <span class="keyword">from</span> pokes;</span></span><br></pre></td></tr></table></figure>
<p>导入成功后，我们可以比较一下数据，例如</p>
<p>进入hive</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">select</span> * <span class="keyword">from</span> hbase_table1;</span></span><br></pre></td></tr></table></figure>
<p>进入hbase</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scan "aaa"</span><br></pre></td></tr></table></figure>
<hr>
<h2 id="hive_u8BFB_u53D6_u5DF2_u5B58_u5728_u7684hbase_u8868"><a href="#hive_u8BFB_u53D6_u5DF2_u5B58_u5728_u7684hbase_u8868" class="headerlink" title="hive读取已存在的hbase表"></a>hive读取已存在的hbase表</h2><p>这个场景特别常见，hive和hbase分开存储，但查询统一走hive</p>
<p>1.先造一个hbase表并放入数据，创建一个scores表，有2个列族，grade列族只有1列，course列族有2列</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">create</span> <span class="string">'scores'</span>,<span class="string">'grade'</span>, <span class="string">'course'</span></span><br><span class="line"></span><br><span class="line">put <span class="string">'scores'</span>,<span class="string">'Tom'</span>,<span class="string">'grade:'</span>,<span class="string">'5'</span></span><br><span class="line">put <span class="string">'scores'</span>,<span class="string">'Tom'</span>,<span class="string">'course:math'</span>,<span class="string">'97'</span></span><br><span class="line">put <span class="string">'scores'</span>,<span class="string">'Tom'</span>,<span class="string">'course:art'</span>,<span class="string">'87'</span></span><br><span class="line">put <span class="string">'scores'</span>,<span class="string">'Jim'</span>,<span class="string">'grade'</span>,<span class="string">'4'</span></span><br><span class="line">put <span class="string">'scores'</span>,<span class="string">'Jim'</span>,<span class="string">'course:math'</span>,<span class="string">'89'</span></span><br><span class="line">put <span class="string">'scores'</span>,<span class="string">'Jim'</span>,<span class="string">'course:art'</span>,<span class="string">'80'</span></span></span><br></pre></td></tr></table></figure>
<p>2.建立hive外部表，映射到hbase上</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">CREATE</span> <span class="keyword">EXTERNAL</span> <span class="keyword">TABLE</span> \</span><br><span class="line">hive_scores (rowkey <span class="keyword">string</span>, grade <span class="keyword">map</span>&lt;<span class="keyword">string</span>,<span class="keyword">string</span>&gt;, course <span class="keyword">map</span>&lt;<span class="keyword">string</span>,<span class="keyword">string</span>&gt;) \</span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">BY</span> <span class="string">'org.apache.hadoop.hive.hbase.HBaseStorageHandler'</span> \</span><br><span class="line"><span class="keyword">WITH</span> SERDEPROPERTIES (<span class="string">"hbase.columns.mapping"</span> = <span class="string">":key,grade:,course:"</span>) \</span><br><span class="line">TBLPROPERTIES(<span class="string">"hbase.table.name"</span> = <span class="string">"scores"</span>);</span></span><br></pre></td></tr></table></figure>
<p>此时便可以查询hbase表了</p>
<hr>
<h2 id="u7F16_u8BD1_hive-hbase-hander__u63D2_u4EF6"><a href="#u7F16_u8BD1_hive-hbase-hander__u63D2_u4EF6" class="headerlink" title="编译 hive-hbase-hander 插件"></a>编译 hive-hbase-hander 插件</h2><p>注：如果你能成功执行以上语句，就没必要编译这个插件了，编译好后，需要放到 <code>$HIVE_HOME/lib</code> 下</p>
<p>1.首先导入hive此模块源码，到IDE里，去除原有的工程依赖 jar包，然后在 classpath 下添加如下jar包</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">/YOUR_WORK_DIR/hive-<span class="number">1.1</span>.<span class="number">0</span>/lib/commons-io-<span class="number">2.4</span>.jar</span><br><span class="line">/YOUR_WORK_DIR/hive-<span class="number">1.1</span>.<span class="number">0</span>/lib/commons-lang-<span class="number">2.6</span>.jar</span><br><span class="line">/YOUR_WORK_DIR/hive-<span class="number">1.1</span>.<span class="number">0</span>/lib/commons-logging-<span class="number">1.1</span>.<span class="number">3</span>.jar</span><br><span class="line">/YOUR_WORK_DIR/hadoop-<span class="number">2.6</span>.<span class="number">0</span>/share/hadoop/common/hadoop-common-<span class="number">2.6</span>.<span class="number">0</span>.jar</span><br><span class="line">/YOUR_WORK_DIR/hadoop-<span class="number">2.6</span>.<span class="number">0</span>/share/hadoop/hdfs/hadoop-hdfs-<span class="number">2.6</span>.<span class="number">0</span>.jar</span><br><span class="line">/YOUR_WORK_DIR/hadoop-<span class="number">2.6</span>.<span class="number">0</span>/share/hadoop/hdfs/hadoop-hdfs-nfs-<span class="number">2.6</span>.<span class="number">0</span>.jar</span><br><span class="line">/YOUR_WORK_DIR/hadoop-<span class="number">2.6</span>.<span class="number">0</span>/share/hadoop/mapreduce/hadoop-mapreduce-client-app-<span class="number">2.6</span>.<span class="number">0</span>.jar</span><br><span class="line">/YOUR_WORK_DIR/hadoop-<span class="number">2.6</span>.<span class="number">0</span>/share/hadoop/mapreduce/hadoop-mapreduce-client-common-<span class="number">2.6</span>.<span class="number">0</span>.jar</span><br><span class="line">/YOUR_WORK_DIR/hadoop-<span class="number">2.6</span>.<span class="number">0</span>/share/hadoop/mapreduce/hadoop-mapreduce-client-core-<span class="number">2.6</span>.<span class="number">0</span>.jar</span><br><span class="line">/YOUR_WORK_DIR/hadoop-<span class="number">2.6</span>.<span class="number">0</span>/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-<span class="number">2.6</span>.<span class="number">0</span>.jar</span><br><span class="line">/YOUR_WORK_DIR/hadoop-<span class="number">2.6</span>.<span class="number">0</span>/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-<span class="number">2.6</span>.<span class="number">0</span>.jar</span><br><span class="line">/YOUR_WORK_DIR/hadoop-<span class="number">2.6</span>.<span class="number">0</span>/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-<span class="number">2.6</span>.<span class="number">0</span>.jar</span><br><span class="line">/YOUR_WORK_DIR/hadoop-<span class="number">2.6</span>.<span class="number">0</span>/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-<span class="number">2.6</span>.<span class="number">0</span>.jar</span><br><span class="line">/YOUR_WORK_DIR/hadoop-<span class="number">2.6</span>.<span class="number">0</span>/share/hadoop/common/hadoop-nfs-<span class="number">2.6</span>.<span class="number">0</span>.jar</span><br><span class="line">/YOUR_WORK_DIR/hadoop-<span class="number">2.6</span>.<span class="number">0</span>/share/hadoop/yarn/hadoop-yarn-api-<span class="number">2.6</span>.<span class="number">0</span>.jar</span><br><span class="line">/YOUR_WORK_DIR/hadoop-<span class="number">2.6</span>.<span class="number">0</span>/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-<span class="number">2.6</span>.<span class="number">0</span>.jar</span><br><span class="line">/YOUR_WORK_DIR/hadoop-<span class="number">2.6</span>.<span class="number">0</span>/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-<span class="number">2.6</span>.<span class="number">0</span>.jar</span><br><span class="line">/YOUR_WORK_DIR/hadoop-<span class="number">2.6</span>.<span class="number">0</span>/share/hadoop/yarn/hadoop-yarn-client-<span class="number">2.6</span>.<span class="number">0</span>.jar</span><br><span class="line">/YOUR_WORK_DIR/hadoop-<span class="number">2.6</span>.<span class="number">0</span>/share/hadoop/yarn/hadoop-yarn-common-<span class="number">2.6</span>.<span class="number">0</span>.jar</span><br><span class="line">/YOUR_WORK_DIR/hadoop-<span class="number">2.6</span>.<span class="number">0</span>/share/hadoop/yarn/hadoop-yarn-registry-<span class="number">2.6</span>.<span class="number">0</span>.jar</span><br><span class="line">/YOUR_WORK_DIR/hadoop-<span class="number">2.6</span>.<span class="number">0</span>/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-<span class="number">2.6</span>.<span class="number">0</span>.jar</span><br><span class="line">/YOUR_WORK_DIR/hadoop-<span class="number">2.6</span>.<span class="number">0</span>/share/hadoop/yarn/hadoop-yarn-server-common-<span class="number">2.6</span>.<span class="number">0</span>.jar</span><br><span class="line">/YOUR_WORK_DIR/hadoop-<span class="number">2.6</span>.<span class="number">0</span>/share/hadoop/yarn/hadoop-yarn-server-nodemanager-<span class="number">2.6</span>.<span class="number">0</span>.jar</span><br><span class="line">/YOUR_WORK_DIR/hadoop-<span class="number">2.6</span>.<span class="number">0</span>/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-<span class="number">2.6</span>.<span class="number">0</span>.jar</span><br><span class="line">/YOUR_WORK_DIR/hadoop-<span class="number">2.6</span>.<span class="number">0</span>/share/hadoop/yarn/hadoop-yarn-server-web-proxy-<span class="number">2.6</span>.<span class="number">0</span>.jar</span><br><span class="line">/YOUR_WORK_DIR/hbase-<span class="number">1.0</span>.<span class="number">0</span>-cdh5.<span class="number">4.0</span>/lib/hbase-annotations-<span class="number">1.0</span>.<span class="number">0</span>-cdh5.<span class="number">4.0</span>.jar</span><br><span class="line">/YOUR_WORK_DIR/hbase-<span class="number">1.0</span>.<span class="number">0</span>-cdh5.<span class="number">4.0</span>/lib/hbase-client-<span class="number">1.0</span>.<span class="number">0</span>-cdh5.<span class="number">4.0</span>.jar</span><br><span class="line">/YOUR_WORK_DIR/hbase-<span class="number">1.0</span>.<span class="number">0</span>-cdh5.<span class="number">4.0</span>/lib/hbase-common-<span class="number">1.0</span>.<span class="number">0</span>-cdh5.<span class="number">4.0</span>.jar</span><br><span class="line">/YOUR_WORK_DIR/hbase-<span class="number">1.0</span>.<span class="number">0</span>-cdh5.<span class="number">4.0</span>/lib/hbase-hadoop2-compat-<span class="number">1.0</span>.<span class="number">0</span>-cdh5.<span class="number">4.0</span>.jar</span><br><span class="line">/YOUR_WORK_DIR/hbase-<span class="number">1.0</span>.<span class="number">0</span>-cdh5.<span class="number">4.0</span>/lib/hbase-prefix-tree-<span class="number">1.0</span>.<span class="number">0</span>-cdh5.<span class="number">4.0</span>.jar</span><br><span class="line">/YOUR_WORK_DIR/hbase-<span class="number">1.0</span>.<span class="number">0</span>-cdh5.<span class="number">4.0</span>/lib/hbase-protocol-<span class="number">1.0</span>.<span class="number">0</span>-cdh5.<span class="number">4.0</span>.jar</span><br><span class="line">/YOUR_WORK_DIR/hbase-<span class="number">1.0</span>.<span class="number">0</span>-cdh5.<span class="number">4.0</span>/lib/hbase-rest-<span class="number">1.0</span>.<span class="number">0</span>-cdh5.<span class="number">4.0</span>.jar</span><br><span class="line">/YOUR_WORK_DIR/hbase-<span class="number">1.0</span>.<span class="number">0</span>-cdh5.<span class="number">4.0</span>/lib/hbase-server-<span class="number">1.0</span>.<span class="number">0</span>-cdh5.<span class="number">4.0</span>.jar</span><br><span class="line">/YOUR_WORK_DIR/hbase-<span class="number">1.0</span>.<span class="number">0</span>-cdh5.<span class="number">4.0</span>/lib/hbase-thrift-<span class="number">1.0</span>.<span class="number">0</span>-cdh5.<span class="number">4.0</span>.jar</span><br><span class="line">/YOUR_WORK_DIR/hive-<span class="number">1.1</span>.<span class="number">0</span>/lib/hive-accumulo-handler-<span class="number">1.1</span>.<span class="number">0</span>.jar</span><br><span class="line">/YOUR_WORK_DIR/hive-<span class="number">1.1</span>.<span class="number">0</span>/lib/hive-ant-<span class="number">1.1</span>.<span class="number">0</span>.jar</span><br><span class="line">/YOUR_WORK_DIR/hive-<span class="number">1.1</span>.<span class="number">0</span>/lib/hive-beeline-<span class="number">1.1</span>.<span class="number">0</span>.jar</span><br><span class="line">/YOUR_WORK_DIR/hive-<span class="number">1.1</span>.<span class="number">0</span>/lib/hive-cli-<span class="number">1.1</span>.<span class="number">0</span>.jar</span><br><span class="line">/YOUR_WORK_DIR/hive-<span class="number">1.1</span>.<span class="number">0</span>/lib/hive-common-<span class="number">1.1</span>.<span class="number">0</span>.jar</span><br><span class="line">/YOUR_WORK_DIR/hive-<span class="number">1.1</span>.<span class="number">0</span>/lib/hive-contrib-<span class="number">1.1</span>.<span class="number">0</span>.jar</span><br><span class="line">/YOUR_WORK_DIR/hive-<span class="number">1.1</span>.<span class="number">0</span>/lib/hive-exec-<span class="number">1.1</span>.<span class="number">0</span>.jar</span><br><span class="line">/YOUR_WORK_DIR/hive-<span class="number">1.1</span>.<span class="number">0</span>/lib/hive-hwi-<span class="number">1.1</span>.<span class="number">0</span>.jar</span><br><span class="line">/YOUR_WORK_DIR/hive-<span class="number">1.1</span>.<span class="number">0</span>/lib/hive-jdbc-<span class="number">1.1</span>.<span class="number">0</span>.jar</span><br><span class="line">/YOUR_WORK_DIR/hive-<span class="number">1.1</span>.<span class="number">0</span>/lib/hive-metastore-<span class="number">1.1</span>.<span class="number">0</span>.jar</span><br><span class="line">/YOUR_WORK_DIR/hive-<span class="number">1.1</span>.<span class="number">0</span>/lib/hive-serde-<span class="number">1.1</span>.<span class="number">0</span>.jar</span><br><span class="line">/YOUR_WORK_DIR/hive-<span class="number">1.1</span>.<span class="number">0</span>/lib/hive-service-<span class="number">1.1</span>.<span class="number">0</span>.jar</span><br><span class="line">/YOUR_WORK_DIR/hive-<span class="number">1.1</span>.<span class="number">0</span>/lib/hive-shims-<span class="number">0.20</span>S-<span class="number">1.1</span>.<span class="number">0</span>.jar</span><br><span class="line">/YOUR_WORK_DIR/hive-<span class="number">1.1</span>.<span class="number">0</span>/lib/hive-shims-<span class="number">0.23</span>-<span class="number">1.1</span>.<span class="number">0</span>.jar</span><br><span class="line">/YOUR_WORK_DIR/hive-<span class="number">1.1</span>.<span class="number">0</span>/lib/hive-shims-common-<span class="number">1.1</span>.<span class="number">0</span>.jar</span><br><span class="line">/YOUR_WORK_DIR/hive-<span class="number">1.1</span>.<span class="number">0</span>/lib/hive-shims-scheduler-<span class="number">1.1</span>.<span class="number">0</span>.jar</span><br><span class="line">/YOUR_WORK_DIR/hive-<span class="number">1.1</span>.<span class="number">0</span>/lib/hive-testutils-<span class="number">1.1</span>.<span class="number">0</span>.jar</span><br><span class="line">/YOUR_WORK_DIR/jsr305-<span class="number">3.0</span>.<span class="number">0</span>.jar</span><br><span class="line">/YOUR_WORK_DIR/zookeeper-<span class="number">3.4</span>.<span class="number">7</span>/zookeeper-<span class="number">3.4</span>.<span class="number">7</span>.jar</span><br></pre></td></tr></table></figure>
<p>2.然后再根据自己IDE的不同，百度搜一下如何打成jar</p>
<p>后记：通过hive查询hbase，利用rowkey查询特别快，但通过其他条件查，不一定，甚至很慢，效果不是很理想。</p>
<p>参考：</p>
<p><a href="http://blog.csdn.net/wulantian/article/details/38111683" target="_blank" rel="external">http://blog.csdn.net/wulantian/article/details/38111683</a></p>
<p><a href="http://www.aboutyun.com/thread-7817-1-1.html" target="_blank" rel="external">http://www.aboutyun.com/thread-7817-1-1.html</a></p>
]]></content>
    <summary type="html">
    <![CDATA[hive 和 Hbase 整合]]>
    
    </summary>
    
      <category term="hive" scheme="http://yoursite.com/tags/hive/"/>
    
      <category term="hive" scheme="http://yoursite.com/categories/hive/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[data exchange]]></title>
    <link href="http://yoursite.com/blog/2015/12/15/data-exchange/"/>
    <id>http://yoursite.com/blog/2015/12/15/data-exchange/</id>
    <published>2015-12-15T15:54:41.000Z</published>
    <updated>2016-01-16T03:16:11.000Z</updated>
    <content type="html"><![CDATA[<p>本文将介绍一下常见的几种数据交换的场景，及解决方法</p>
<a id="more"></a>
<p>1.不同集群之间 hdfs 文件 或 hive表 迁移</p>
<p>2.利用sqoop实现按日增量导入，及状态表的全量覆盖</p>
<hr>
<h2 id="u73AF_u5883_u51C6_u5907"><a href="#u73AF_u5883_u51C6_u5907" class="headerlink" title="环境准备"></a>环境准备</h2><ul>
<li>hadoop2</li>
<li>sqoop-1.4.6.bin__hadoop-2.0.4-alpha</li>
<li>hive</li>
</ul>
<hr>
<h2 id="hive_u8868_u96C6_u7FA4_u95F4_u8FC1_u79FB"><a href="#hive_u8868_u96C6_u7FA4_u95F4_u8FC1_u79FB" class="headerlink" title="hive表集群间迁移"></a>hive表集群间迁移</h2><p>利用hadoop distcp命令，例如把hive 的 test库下的g_info表导入到数据目录下，如果仅仅是文件拷贝而不涉及hive表，则这一条命令就可以了</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop distcp hdfs://<span class="number">192.168</span>.<span class="number">1.169</span>:<span class="number">9000</span>/user/hive/warehouse/test.db/t1 hdfs://<span class="number">192.168</span>.<span class="number">7.11</span>:<span class="number">9000</span>/user/hive/warehouse/test.db/t1_cp</span><br></pre></td></tr></table></figure>
<h3 id="hive_u8868_u662F_u975E_u5206_u533A_u8868"><a href="#hive_u8868_u662F_u975E_u5206_u533A_u8868" class="headerlink" title="hive表是非分区表"></a><font color="#d67b0f"> hive表是非分区表 </font></h3><p>1.先创建「外部表」</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">CREATE</span> <span class="keyword">external</span> <span class="keyword">TABLE</span> t1_cp(</span><br><span class="line">  a <span class="built_in">bigint</span>,</span><br><span class="line">  b <span class="built_in">int</span>,</span><br><span class="line">  <span class="keyword">c</span> <span class="built_in">int</span>)</span><br><span class="line"><span class="keyword">COMMENT</span> <span class="string">'Created by xiaolong.yuanxl 20151211'</span></span><br><span class="line"><span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span></span><br><span class="line">  <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">'\t'</span></span><br><span class="line">  <span class="keyword">LINES</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">'\n'</span></span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> TEXTFILE location <span class="string">'hdfs://192.168.7.11:9000/user/hive/warehouse/test.db/t1_cp'</span>;</span></span><br></pre></td></tr></table></figure>
<p>2.通过load data的方式加载数据</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">LOAD</span> <span class="keyword">DATA</span> INPATH <span class="string">'hdfs://192.168.7.11:9000/user/hive/warehouse/test.db/t1_cp/'</span> <span class="keyword">INTO</span> <span class="keyword">TABLE</span> t1_cp;</span></span><br></pre></td></tr></table></figure>
<h3 id="hive_u662F_u5206_u533A_u8868"><a href="#hive_u662F_u5206_u533A_u8868" class="headerlink" title="hive是分区表"></a><font color="#d67b0f"> hive是分区表 </font></h3><p>1.如果hive是分区表，则迁移复制过来的目录结构应该带「分区文件夹」的，所以不能直接一次性导入，因此需要先创建「外部表」且添加「分区信息」，去掉「数据目录」</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">CREATE</span> <span class="keyword">external</span> <span class="keyword">TABLE</span> t1_cp(</span><br><span class="line">  a <span class="built_in">bigint</span>,</span><br><span class="line">  b <span class="built_in">int</span>,</span><br><span class="line">  <span class="keyword">c</span> <span class="built_in">int</span>)</span><br><span class="line"><span class="keyword">COMMENT</span> <span class="string">'Created by xiaolong.yuanxl 20151211'</span></span><br><span class="line">PARTITIONED <span class="keyword">BY</span> (</span><br><span class="line">  <span class="string">`pdt`</span> <span class="keyword">string</span>)</span><br><span class="line"><span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span></span><br><span class="line">  <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">'\t'</span></span><br><span class="line">  <span class="keyword">LINES</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">'\n'</span></span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> TEXTFILE;</span></span><br></pre></td></tr></table></figure>
<p>2.利用 <code>hadoop distcp</code> 命令把数据复制到 hive <code>show create table t1_cp</code> 下的 location 数据目录下</p>
<p>3.由于一个表的分区信息可能很多，甚至几千个，所以需要用脚本自动化来实现「添加分区」及「导入数据」的过程 （注意：根据自己的实际情况，修改路径及 awk截取字符串的位置）</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="shebang">#!/bin/sh</span></span><br><span class="line">ret=`hadoop fs -ls hdfs://<span class="number">192.168</span>.<span class="number">7.168</span>:<span class="number">9000</span>/user/hive/warehouse/user.db/channel_user_reg_t | awk <span class="string">'&#123;print substr($8,78)&#125;'</span>`</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> pt <span class="keyword">in</span> <span class="variable">$&#123;ret[@]&#125;</span></span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">"hive -e \"use user;alter table channel_user_reg_t add partition (pdt="</span>\<span class="string">"<span class="variable">$&#123;pt&#125;</span>\""</span>) location \<span class="string">"/user/hive/warehouse/user.db/channel_user_reg_t/pdt=<span class="variable">$&#123;pt&#125;</span>\";\""</span></span><br><span class="line">    hive <span class="operator">-e</span> <span class="string">"use user;alter table channel_user_reg_t add partition (pdt="</span>\<span class="string">"<span class="variable">$&#123;pt&#125;</span>\""</span>) location \<span class="string">"/user/hive/warehouse/user.db/channel_user_reg_t/pdt=<span class="variable">$&#123;pt&#125;</span>\";"</span></span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure>
<p>4.执行脚本 <code>nohup sh your.sh &amp;</code></p>
<hr>
<h2 id="sqoop__u5BFC_u5165_u6570_u636E"><a href="#sqoop__u5BFC_u5165_u6570_u636E" class="headerlink" title="sqoop 导入数据"></a>sqoop 导入数据</h2><p>一般我们的导入，有「全量覆盖」或「按日新增」 两种方式，而这2种方式，都要先做两件事情</p>
<p>1.创建内部表，如果是「按日增量」则需要添加 partition 语句</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> t1_cp(</span><br><span class="line">  a <span class="built_in">bigint</span>,</span><br><span class="line">  b <span class="built_in">int</span>,</span><br><span class="line">  <span class="keyword">c</span> <span class="built_in">int</span>)</span><br><span class="line"><span class="keyword">COMMENT</span> <span class="string">'Created by xiaolong.yuanxl 20151211'</span></span><br><span class="line">PARTITIONED <span class="keyword">BY</span> (</span><br><span class="line">  <span class="string">`pdt`</span> <span class="keyword">string</span>)</span><br><span class="line"><span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span></span><br><span class="line">  <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">'\t'</span></span><br><span class="line">  <span class="keyword">LINES</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">'\n'</span></span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> TEXTFILE;</span></span><br></pre></td></tr></table></figure>
<p>2.建立分区</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">alter</span> <span class="keyword">table</span> t1_cp <span class="keyword">ADD</span> <span class="keyword">PARTITION</span> (pdt=<span class="string">'2015-12-12'</span>);</span></span><br></pre></td></tr></table></figure>
<p>3.执行sqoop导入命令</p>
<h3 id="u65E0_u5206_u533A"><a href="#u65E0_u5206_u533A" class="headerlink" title="无分区"></a><font color="#d67b0f"> 无分区 </font></h3><font color="#b65b25" size="3">注:下面是一行命令</font>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sqoop import --connect <span class="string">"jdbc:mysql://127.0.0.1:3306/test"</span> --username sqoop --password sqoop \</span><br><span class="line">--query <span class="string">"select id,name,age,created from test.school where created &gt;= '2015-12-10 00:00:00' and created &lt; '2015-12-11 00:00:00' and \$CONDITIONS "</span> \</span><br><span class="line">--target-dir <span class="string">"/user/hive/warehouse/test.db/t2"</span> \</span><br><span class="line">--delete-target-dir \</span><br><span class="line">--fields-terminated-by <span class="string">'\t'</span> \</span><br><span class="line">--hive-database <span class="string">"test"</span> --hive-table <span class="string">"t2"</span> \</span><br><span class="line">--verbose -m <span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>重要参数说明</p>
<ul>
<li><code>--query</code> 代表需要执行的sql语句，其中需要硬性添加一个 <code>$CONDITIONS</code> 变量，注意shell环境$符号需要转义</li>
<li><code>--target-dir</code> 代表sqoop执行mr完后的输出目录，跟query成对出现</li>
<li><code>--delete-target-dir</code> 代表每次删除目录，然后重新覆盖导入</li>
<li><code>--fields-terminated-by</code> 字段间分隔符</li>
<li><code>--m</code> 代表mapreduce中 map 个数</li>
</ul>
<h3 id="u5206_u533A_u8868"><a href="#u5206_u533A_u8868" class="headerlink" title="分区表"></a><font color="#d67b0f"> 分区表 </font></h3><font color="#b65b25" size="3">注:下面是一行命令</font>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">sqoop import --connect <span class="string">"jdbc:mysql://127.0.0.1:3306/test"</span> --username sqoop --password sqoop \</span><br><span class="line">--query <span class="string">"select id,name,age,created from test.school where created &gt;= '2015-12-10 00:00:00' and created &lt; '2015-12-11 00:00:00' and \$CONDITIONS "</span> \</span><br><span class="line">--target-dir <span class="string">"/user/hive/warehouse/test.db/t2” \</span><br><span class="line">--append --fields-terminated-by '\t' \</span><br><span class="line">--hive-database "</span><span class="built_in">test</span><span class="string">" --hive-table "</span>t2<span class="string">" \</span><br><span class="line">--verbose -m 1</span></span><br></pre></td></tr></table></figure>
<p>重要参数说明</p>
<ul>
<li><code>--append</code> 追加模式，不会增量，仅仅增加数据文件。因此需要用sql控制增量</li>
<li>不要添加 <code>--delete-target-dir</code></li>
</ul>
<hr>
<h2 id="u5176_u4ED6"><a href="#u5176_u4ED6" class="headerlink" title="其他"></a>其他</h2><p>1.有些数据库，会有中文的问题，需要在sql中添加转义，例如</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">convert(binary convert(title using latin1) using gbk) as title</span><br></pre></td></tr></table></figure>
<p>2.sqoop会用 <code>hadoop-env.sh</code> 里配置的 <code>$JAVA_HOME</code> 指向的 jdk，而不会关心系统环境变量</p>
<p>3.如果需要自动化，可以利用shell获取时间，然后每日crontab导入</p>
<h3 id="u540E_u8BB0_u8865_u5145"><a href="#u540E_u8BB0_u8865_u5145" class="headerlink" title="后记补充"></a>后记补充</h3><h4 id="u95EE_u9898_u4E00_3A__u6700_u8FD1__u4F7F_u7528hive_u8868_u7684_u5B57_u6BB5_u65F6_uFF0C_u53D1_u73B0_u9519_u4F4D_u4E86"><a href="#u95EE_u9898_u4E00_3A__u6700_u8FD1__u4F7F_u7528hive_u8868_u7684_u5B57_u6BB5_u65F6_uFF0C_u53D1_u73B0_u9519_u4F4D_u4E86" class="headerlink" title="问题一: 最近 使用hive表的字段时，发现错位了"></a>问题一: 最近 使用hive表的字段时，发现错位了</h4><p>原因是字段内容本身有 \t 字符，然后删除表，设定sqoop脚本为 \001 字符，然后创建hive表分割字符为 \001  换行符也是一样（hive只支持\n）</p>
<p>可以添加 <code>--hive-drop-import-delims</code> （Drops \n, \r, and \01 from string fields when importing to Hive.）参数</p>
<h4 id="u95EE_u9898_u4E8C_3A__u5BFC_u51FA_u6570_u636E_u4F1A_u81EA_u52A8_u628A__u6570_u5B570__u548C_u975E_u96F6_mysql__u6570_u636E_uFF0C_u8F6C_u6362_u5230hive__u4E3A_true_false"><a href="#u95EE_u9898_u4E8C_3A__u5BFC_u51FA_u6570_u636E_u4F1A_u81EA_u52A8_u628A__u6570_u5B570__u548C_u975E_u96F6_mysql__u6570_u636E_uFF0C_u8F6C_u6362_u5230hive__u4E3A_true_false" class="headerlink" title="问题二: 导出数据会自动把 数字0 和非零 mysql 数据，转换到hive 为 true false"></a>问题二: 导出数据会自动把 数字0 和非零 mysql 数据，转换到hive 为 true false</h4><p>引用自：<a href="http://www.cnblogs.com/cenyuhai/archive/2013/09/06/3306073.html" target="_blank" rel="external">http://www.cnblogs.com/cenyuhai/archive/2013/09/06/3306073.html</a></p>
<p>jdbc会把 tinyint（1）认为是 java.sql.Types.BIT ,然后sqoop就会转为Boolean了</p>
<ul>
<li><p>解决方法1：</p>
<ul>
<li>在连接上加上一句话 tinyInt1isBit=false 例如  jdbc:mysql://localhost/test?tinyInt1isBit=false</li>
</ul>
</li>
<li><p>解决方法2：</p>
<ul>
<li>hive使用 –map-column-hive foo=tinyint</li>
<li>非hive使用–map-column-java foo=integer</li>
</ul>
</li>
</ul>
]]></content>
    <summary type="html">
    <![CDATA[数据交换 常见场景及方法]]>
    
    </summary>
    
      <category term="sqoop" scheme="http://yoursite.com/tags/sqoop/"/>
    
      <category term="sqoop" scheme="http://yoursite.com/categories/sqoop/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[write cabot alert by wechat plugin]]></title>
    <link href="http://yoursite.com/blog/2015/12/15/write-cabot-alert-by-wechat-plugin/"/>
    <id>http://yoursite.com/blog/2015/12/15/write-cabot-alert-by-wechat-plugin/</id>
    <published>2015-12-15T15:40:35.000Z</published>
    <updated>2016-01-16T03:19:38.000Z</updated>
    <content type="html"><![CDATA[<p>介绍如何利用 微信 和 Cabot 结合进行报警</p>
<a id="more"></a>
<p>有了上一篇文章的 自定义Cabot扩展点插件 的基础后，我们就可以实现 自己的「微信告警插件了」</p>
<hr>
<h2 id="u7B80_u4ECB"><a href="#u7B80_u4ECB" class="headerlink" title="简介"></a>简介</h2><p>利用「微信企业号」进行报警 ，微信企业号的注册过程，请参考这篇文章 <a href="http://wuhf2015.blog.51cto.com/8213008/1688614" target="_blank" rel="external"><font color="#2798a2">zabbix如何实现微信报警</font></a></p>
<p>微信发送原理：</p>
<ol>
<li>根据微信企业号属性 CorpId 和 Secret 向服务接口，获取本次请求 Token</li>
<li>携带刚才返回的 Token ，向消息接口发送 post 请求</li>
</ol>
<p>效果：</p>
<p><img src="/images/cabot/20151215/1.png" alt=""></p>
<hr>
<h2 id="u63D2_u4EF6_u5B89_u88C5"><a href="#u63D2_u4EF6_u5B89_u88C5" class="headerlink" title="插件安装"></a>插件安装</h2><p>前置：</p>
<ul>
<li>已进入cabot的安装目录，例如 <code>/usr/local/datacenter/cabot</code></li>
<li>已停止 cabot 相关进程，有2组进程。</li>
<li>消息队列处理进程 <code>ps -ef | grep python | grep celery</code> 10个进程</li>
<li>UI 进程 <code>ps -ef | grep python | grep manage.py</code> 1个进程（或许你有其他 django 应用在运行，请自己通过端口区分）</li>
</ul>
<p>如果你实在记不清，也可以通过启动日志查看，类似这样的日志，其中 celery 有10个进程，注意需要逐一关闭</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">14</span>:<span class="number">04</span>:<span class="number">00</span> web.<span class="number">1</span> | started with pid <span class="number">36652</span></span><br><span class="line"><span class="number">14</span>:<span class="number">04</span>:<span class="number">00</span> celery.<span class="number">1</span> | started with pid <span class="number">36653</span></span><br></pre></td></tr></table></figure>
<ul>
<li>已注册了一个 「微信企业号」，并有 「CorpID」和「Secret」及「应用ID」和「接收组ID」。如果记不住 「CorpID」和「Secret」可以在 微信公共号后台<br><code>「设置」-&gt; 「权限管理」-&gt;「你自己建的组名」</code> 最下面有</li>
<li>已通过 <a href="http://qydev.weixin.qq.com/debug" target="_blank" rel="external"><font color="#2798a2">微信企业号接口调试工具</font></a> 调试OK了微信账号，附上 <a href="http://qydev.weixin.qq.com/wiki/index.php?title=消息类型及数据格式" target="_blank" rel="external"><font color="#2798a2">发送接口说明</font></a></li>
</ul>
<p>1.编写配置文件,添加 插件名称（注意是下划线，不是工程名，而是里面的 模块名，由setup.py里指定的）, 并修改 <code>setup.py</code> 内的相关信息</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">vi conf/development.env</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plugins to be loaded at launch</span></span><br><span class="line">CABOT_PLUGINS_ENABLED=cabot_alert_hipchat==<span class="number">1.7</span>.<span class="number">0</span>,cabot_alert_twilio==<span class="number">1.6</span>.<span class="number">1</span>,cabot_alert_email==<span class="number">1.3</span>.<span class="number">1</span>,cabot_alert_wechat==<span class="number">0.0</span>.<span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>2.通过 pip 安装自定义插件</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install git+git://github.com/yuanxiaolong/cabot-alert-wechat.git</span><br></pre></td></tr></table></figure>
<p>3.初始化数据库</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sh setup_dev.sh</span><br></pre></td></tr></table></figure>
<p>4.无误后，启动 cabot</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup foreman start &amp;</span><br></pre></td></tr></table></figure>
<hr>
<h2 id="u63D2_u4EF6_u7F16_u5199"><a href="#u63D2_u4EF6_u7F16_u5199" class="headerlink" title="插件编写"></a>插件编写</h2><p>官方更详细的 文档 <a href="http://cabotapp.com/dev/writing-alert-plugins.html" target="_blank" rel="external"><font color="#2798a2"> http://cabotapp.com/dev/writing-alert-plugins.html</font></a></p>
]]></content>
    <summary type="html">
    <![CDATA[cabot 微信报警插件]]>
    
    </summary>
    
      <category term="cabot" scheme="http://yoursite.com/tags/cabot/"/>
    
      <category term="cabot" scheme="http://yoursite.com/categories/cabot/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[write cabot alert plungin]]></title>
    <link href="http://yoursite.com/blog/2015/12/15/write-cabot-alert-plungin/"/>
    <id>http://yoursite.com/blog/2015/12/15/write-cabot-alert-plungin/</id>
    <published>2015-12-15T15:24:18.000Z</published>
    <updated>2016-01-16T03:20:30.000Z</updated>
    <content type="html"><![CDATA[<p>Cabot 可以有扩展点，让你自定义扩展一种报警方式。</p>
<a id="more"></a>
<p>我们可以本地部署一个 Cabot 然后用 touch file 的方式来验证。</p>
<p>官网文档：<a href="http://cabotapp.com/dev/writing-alert-plugins.html" target="_blank" rel="external"><font color="#2798a2">http://cabotapp.com/dev/writing-alert-plugins.html</font></a></p>
<hr>
<h2 id="u7B80_u4ECB"><a href="#u7B80_u4ECB" class="headerlink" title="简介"></a>简介</h2><p>根据 <a href="https://github.com/bonniejools/cabot-alert-skeleton" target="_blank" rel="external"><font color="#2798a2">https://github.com/bonniejools/cabot-alert-skeleton</font></a> 来做一个简单测试</p>
<p>效果：在 <code>/tmp/</code> 目录下 新建一个空文件 代表执行了报警逻辑，用于本地测试。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">root@MacBook-Pro ~$ ll /tmp/ | grep cabot</span><br><span class="line">-rw-r--r-- <span class="number">1</span> root wheel <span class="number">0</span> <span class="number">12</span> <span class="number">10</span> <span class="number">14</span>:<span class="number">03</span> cabotTestLocalAlert_20151210_140306</span><br><span class="line">-rw-r--r-- <span class="number">1</span> root wheel <span class="number">0</span> <span class="number">12</span> <span class="number">10</span> <span class="number">14</span>:<span class="number">15</span> cabotTestLocalAlert_20151210_141543</span><br></pre></td></tr></table></figure>
<hr>
<h2 id="u63D2_u4EF6_u5B89_u88C5"><a href="#u63D2_u4EF6_u5B89_u88C5" class="headerlink" title="插件安装"></a>插件安装</h2><p>前置：</p>
<ul>
<li>已进入cabot的安装目录，例如 <code>/usr/local/datacenter/cabot</code></li>
<li>已停止 cabot 相关进程，有2组进程。<ul>
<li>消息队列处理进程 <code>ps -ef | grep python | grep celery</code> 10个进程</li>
<li>UI 进程 <code>ps -ef | grep python | grep manage.py</code> 1个进程（或许你有其他 django 应用在运行，请自己通过端口区分）</li>
</ul>
</li>
</ul>
<p>如果你实在记不清，也可以通过启动日志查看，类似这样的日志</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">14</span>:<span class="number">04</span>:<span class="number">00</span> web.<span class="number">1</span> | started with pid <span class="number">36652</span></span><br><span class="line"><span class="number">14</span>:<span class="number">04</span>:<span class="number">00</span> celery.<span class="number">1</span> | started with pid <span class="number">36653</span></span><br></pre></td></tr></table></figure>
<p>1.编写配置文件,添加 插件名称（注意是下划线，不是工程名，而是里面的 模块名，由setup.py里指定的）, 并修改 <code>setup.py</code> 内的相关信息</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">vi conf/development.env</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plugins to be loaded at launch</span></span><br><span class="line">CABOT_PLUGINS_ENABLED=cabot_alert_hipchat==<span class="number">1.7</span>.<span class="number">0</span>,cabot_alert_twilio==<span class="number">1.6</span>.<span class="number">1</span>,cabot_alert_email==<span class="number">1.3</span>.<span class="number">1</span>,cabot_alert_localtest==<span class="number">0.0</span>.<span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>2.通过 pip 安装自定义插件 (自己编写的插件需要先上传到自己的git仓库上)</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install git+git://github.com/yuanxiaolong/cabot-alert-localtest.git</span><br></pre></td></tr></table></figure>
<p>3.初始化数据库</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sh setup_dev.sh</span><br></pre></td></tr></table></figure>
<p>4.无误后，启动 cabot</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup foreman start &amp;</span><br></pre></td></tr></table></figure>
<hr>
<h2 id="u63D2_u4EF6_u7F16_u5199"><a href="#u63D2_u4EF6_u7F16_u5199" class="headerlink" title="插件编写"></a>插件编写</h2><p>不建议直接 clone 或 fork 此工程，因为此工程是测试工程。</p>
<p>可以直接 fork 官方的「骨架工程」<a href="https://github.com/bonniejools/cabot-alert-skeleton" target="_blank" rel="external"><font color="#2798a2">https://github.com/bonniejools/cabot-alert-skeleton</font></a><br>然后自己新建个仓库，再cp必要文件过去，进行修改调试</p>
]]></content>
    <summary type="html">
    <![CDATA[cabot 写自定义插件]]>
    
    </summary>
    
      <category term="cabot" scheme="http://yoursite.com/tags/cabot/"/>
    
      <category term="cabot" scheme="http://yoursite.com/categories/cabot/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[graphite and cabot]]></title>
    <link href="http://yoursite.com/blog/2015/12/08/graphite-and-cabot/"/>
    <id>http://yoursite.com/blog/2015/12/08/graphite-and-cabot/</id>
    <published>2015-12-08T09:38:57.000Z</published>
    <updated>2016-01-16T03:26:15.000Z</updated>
    <content type="html"><![CDATA[<p>介绍如何 部署及配置 启动 graphite 和 cabot</p>
<a id="more"></a>
<p>graphite 是一个 指标数据收集系统，一般作为监控系统中 「监控数据收集存储」使用，它有3个组件 Carbon Whisper 和 Graphite-web 这3个组件都是 python pip 安装的，它们三个结合起来就是 graphite 了</p>
<p>cabot 是一个根据 graphite Whisper 里的 指标数据，进行规制监控，并报警的一个软件</p>
<p>与 zabbix 、ganglia 有何不同呢? 一般系统级 用 它们就足够了，如果是业务数据，可能就需要 graphite 和 cabot 了</p>
<hr>
<h2 id="u73AF_u5883_u51C6_u5907"><a href="#u73AF_u5883_u51C6_u5907" class="headerlink" title="环境准备"></a>环境准备</h2><p>python 2.7 及以上（因为cabot需要）<br>centos 或其他系统主机 或许基础软件不一样</p>
<p>一般来说，或许有人喜欢用 virtualenv 但是，我不太熟悉 python 因此，直接用了 python 2.7</p>
<hr>
<h2 id="u642D_u5EFA_graphite"><a href="#u642D_u5EFA_graphite" class="headerlink" title="搭建 graphite"></a>搭建 graphite</h2><p>安装基础</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">pip install whisper</span><br><span class="line">pip install carbon</span><br><span class="line">pip install graphite-web</span><br><span class="line">pip install django==<span class="number">1.6</span>.<span class="number">8</span></span><br><span class="line">pip install django-tagging==<span class="number">0.3</span>.<span class="number">6</span></span><br><span class="line">pip install uWSGI==<span class="number">2.0</span>.<span class="number">11.1</span></span><br><span class="line">pip install MySQL-python==<span class="number">1.2</span>.<span class="number">5</span></span><br><span class="line">pip install daemonize</span><br></pre></td></tr></table></figure>
<p>最后安装下来 python2.7.3 其中组件版本 whisper(0.9.15) daemonize(2.4.1)</p>
<p>引用</p>
<ul>
<li>graphite使用cairo进行绘图，由于系统自带的cairo版本较低（需要cairo1.10以上），使用pip安装cairo会出错，所以采用编译安装。</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">wget http://cairographics.org/releases/pycairo-<span class="number">1.8</span>.<span class="number">8</span>.tar.gz</span><br><span class="line">tar zxvf pycairo-<span class="number">1.8</span>.<span class="number">8</span>.tar.gz</span><br><span class="line">python -c <span class="string">"import sys; print sys.prefix"</span></span><br><span class="line"><span class="built_in">cd</span> pycairo-<span class="number">1.8</span>.<span class="number">8</span></span><br><span class="line">./configure --prefix=/data/server/python-envs/graphite</span><br><span class="line">make</span><br><span class="line">make install</span><br></pre></td></tr></table></figure>
<p>目录说明</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">bin -- 数据收集相关工具</span><br><span class="line">conf -- 数据存储相关配置文件</span><br><span class="line">    carbon.conf -- 数据收集carbon进程涉及的配置</span><br><span class="line">    dashboard.conf -- Dashboard UI相关配置</span><br><span class="line">    graphite.wsgi -- wsgi相关配置</span><br><span class="line">    storage-schemas.conf -- Schema definitions <span class="keyword">for</span> Whisper files</span><br><span class="line">    whitelist.conf -- 定义允许存储的metrics白名单</span><br><span class="line">    graphTemplates.conf -- 图形化展示数据时使用的模板</span><br><span class="line">examples -- 示例脚本</span><br><span class="line">lib -- carbon和twisted库</span><br><span class="line">storage -- 数据文件存储目录</span><br><span class="line">webapp -- 数据前端展示涉及程序</span><br></pre></td></tr></table></figure>
<p>配置 graphite-web</p>
<p>初始化配置文件</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /opt/graphite/webapp/graphite</span><br><span class="line">cp <span class="built_in">local</span>_settings.py.example <span class="built_in">local</span>_settings.py</span><br><span class="line">cp /opt/graphite/conf/graphite.wsgi.example /opt/graphite/conf/graphite.wsgi</span><br><span class="line">cp /opt/graphite/conf/graphTemplates.conf.example /opt/graphite/conf/graphTemplates.conf</span><br><span class="line">cp /opt/graphite/conf/dashboard.conf.example /opt/graphite/conf/dashboard.conf</span><br></pre></td></tr></table></figure>
<p>修改 <code>local_settings.py</code> ，其中修改自己的环境</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">TIME_ZONE = 'Asia/Shanghai'</span><br><span class="line">DATABASES = &#123;</span><br><span class="line">    'default': &#123;</span><br><span class="line">        'NAME': 'graphitedb', #自己定义名字</span><br><span class="line">        'ENGINE': 'django.db.backends.mysql',</span><br><span class="line">        'USER': 'graphite',</span><br><span class="line">        'PASSWORD': 'graphite',</span><br><span class="line">        'HOST': '192.168.7.11',</span><br><span class="line">        'PORT': '3306'</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>初始化数据库，期间会让你指定 用户名 和 密码</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python manage.py syncdb</span><br></pre></td></tr></table></figure>
<p>启动 graphite-web</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup uwsgi --http <span class="number">192.168</span>.<span class="number">7.12</span>:<span class="number">8050</span> --master --processes <span class="number">1</span> --pythonpath /opt/graphite/webapp/graphite --wsgi-file=/opt/graphite/conf/graphite.wsgi --enable-threads --thunder-lock &amp;</span><br></pre></td></tr></table></figure>
<p>访问 <code>http://192.168.7.12:8050</code> 看是否有界面，如果没有界面 则首先看 nohup.out 日志是否报错，如果未报错，则估计上面的 <code>cairo</code> 绘图版本有问题，安装一下就好了</p>
<p>配置收集服务</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cp /opt/graphite/conf/carbon.conf.example /opt/graphite/conf/carbon.conf</span><br><span class="line">cp /opt/graphite/conf/storage-schemas.conf.example /opt/graphite/conf/storage-schemas.conf</span><br><span class="line">cp /opt/graphite/conf/whitelist.conf.example /opt/graphite/conf/whitelist.conf</span><br></pre></td></tr></table></figure>
<p>其中 <code>/opt/graphite/conf/whitelist.conf</code> 是白名单，可以添加类似这样的正则，就让 graphite 只收集 test 和 server 的数据了</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">^<span class="built_in">test</span>\..*</span><br><span class="line">^server\..*</span><br></pre></td></tr></table></figure>
<p>其中 <code>/opt/graphite/conf/storage-schemas.conf</code> 是配置存储策略的文件，可以添加类似如下的规则</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[server]</span><br><span class="line">pattern = ^server\..*</span><br><span class="line">retentions = <span class="number">60</span>s:<span class="number">1</span>d,<span class="number">5</span>m:<span class="number">7</span>d,<span class="number">15</span>m:<span class="number">3</span>y</span><br><span class="line"></span><br><span class="line">[default]</span><br><span class="line">pattern = ^<span class="built_in">test</span>\..*</span><br><span class="line">retentions = <span class="number">60</span>s:<span class="number">1</span>d,<span class="number">5</span>m:<span class="number">7</span>d</span><br></pre></td></tr></table></figure>
<p>上面的配置，会对于test.开头的metrics，以60秒为精度存储一天，以5分钟为精度存储7天。即查询一天内的数据时，可以精确到1分钟，查询7天内的数据时，只能精确到5分钟。</p>
<p>启动 <code>nohup python /opt/graphite/bin/carbon-cache.py --config=/opt/graphite/conf/carbon.conf --debug start &amp;</code></p>
<p>引用</p>
<ul>
<li>造收集数据，利用 crontab 添加定时任务，往 2003 端口发送 指标数据</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="shebang">#!/bin/sh</span><br><span class="line"></span></span><br><span class="line">HOST=$(hostname | awk -F<span class="string">'.'</span> <span class="string">'&#123;print $1&#125;'</span>)</span><br><span class="line">IDC=<span class="string">"local"</span></span><br><span class="line"></span><br><span class="line">SYSTEM_LOAD=$(awk <span class="string">'&#123;print $1&#125;'</span> /proc/loadavg)</span><br><span class="line">SYSTEM_MEMORY_FREE=$(free -m | grep <span class="string">'buffers/cache'</span> | awk <span class="string">'&#123;print $NF&#125;'</span>)</span><br><span class="line">SYSTEM_SWAP_USE=$(free -m | grep <span class="string">'Swap'</span> | awk <span class="string">'&#123;print $(NF-1)&#125;'</span>)</span><br><span class="line">SYSTEM_DISK_USED=$(df -h | grep <span class="string">'/'</span> | awk <span class="string">'BEGIN&#123;_max=0&#125;&#123;len=length($5);i=substr($5,0,len-1);if(_max&lt;i)&#123;_max=i&#125;&#125;END&#123;print _max&#125;'</span>)</span><br><span class="line"></span><br><span class="line">TIMESTAMP=$(date +%s)</span><br><span class="line"></span><br><span class="line"><span class="comment">### send to garphite through udp port 2003 ########</span></span><br><span class="line"><span class="built_in">echo</span> -n <span class="string">"server.<span class="variable">$IDC</span>.<span class="variable">$HOST</span>.system.load <span class="variable">$SYSTEM_LOAD</span> <span class="variable">$TIMESTAMP</span>"</span> &gt; /dev/udp/<span class="number">127.0</span>.<span class="number">0.1</span>/<span class="number">2003</span></span><br><span class="line"><span class="built_in">echo</span> -n <span class="string">"server.<span class="variable">$IDC</span>.<span class="variable">$HOST</span>.system.memory_free <span class="variable">$SYSTEM_MEMORY_FREE</span> <span class="variable">$TIMESTAMP</span>"</span> &gt; /dev/udp/<span class="number">127.0</span>.<span class="number">0.1</span>/<span class="number">2003</span></span><br><span class="line"><span class="built_in">echo</span> -n <span class="string">"server.<span class="variable">$IDC</span>.<span class="variable">$HOST</span>.system.swap_used <span class="variable">$SYSTEM_SWAP_USED</span> <span class="variable">$TIMESTAMP</span>"</span> &gt; /dev/udp/<span class="number">127.0</span>.<span class="number">0.1</span>/<span class="number">2003</span></span><br><span class="line"><span class="built_in">echo</span> -n <span class="string">"server.<span class="variable">$IDC</span>.<span class="variable">$HOST</span>.system.disk_used <span class="variable">$SYSTEM_DISK_USED</span> <span class="variable">$TIMESTAMP</span>"</span> &gt; /dev/udp/<span class="number">127.0</span>.<span class="number">0.1</span>/<span class="number">2003</span></span><br></pre></td></tr></table></figure>
<hr>
<h2 id="u642D_u5EFACabot"><a href="#u642D_u5EFACabot" class="headerlink" title="搭建Cabot"></a>搭建Cabot</h2><ul>
<li>ruby 1.9.3 以上</li>
<li>安装 nodejs（内嵌了npm）</li>
<li>同时通过 npm 安装 coffee-script <code>npm install -g coffee-script less@1.3 --registry http://registry.npmjs.org/</code> 以便 UI 渲染</li>
<li>redis</li>
</ul>
<p>安装即 git clone 下 Cabot 源码，并修改配置文件</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/arachnys/cabot.git</span><br><span class="line"><span class="built_in">cd</span> cabot</span><br><span class="line">cp conf/development.env.example conf/development.env</span><br></pre></td></tr></table></figure>
<p>根据个人配置修改环境变量，及是否转成私网ip</p>
<figure class="highlight bash"><figcaption><span>development.env</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">DATABASE_URL=mysql://cabot:cabot@<span class="number">192.168</span>.<span class="number">7.11</span>:<span class="number">3306</span>/cabot?characterEncoding=utf8</span><br><span class="line">TIME_ZONE=Asia/Shanghai</span><br><span class="line">ADMIN_EMAIL=your@qq.com</span><br><span class="line">CABOT_FROM_EMAIL=your@<span class="number">126</span>.com</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这里指向了一个 reids实例</span></span><br><span class="line">CELERY_BROKER_URL=redis://<span class="number">192.168</span>.<span class="number">7.12</span>:<span class="number">6379</span>/<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># graphite</span></span><br><span class="line">GRAPHITE_API=http://<span class="number">192.168</span>.<span class="number">7.12</span>:<span class="number">8050</span>/</span><br><span class="line">GRAPHITE_USER=admin</span><br><span class="line">GRAPHITE_PASS=admin</span><br><span class="line"></span><br><span class="line"><span class="comment"># smtp</span></span><br><span class="line">SES_HOST=smtp.<span class="number">126</span>.com</span><br><span class="line">SES_USER=yourname</span><br><span class="line">SES_PASS=yourpassword</span><br><span class="line">SES_PORT=<span class="number">465</span>          <span class="comment"># 用SSL 端口，因为python的组件需要</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 回调查看报警url</span></span><br><span class="line">WWW_HTTP_HOST=test.cabot.yourdomain.com</span><br></pre></td></tr></table></figure>
<p>端口绑定私网地址</p>
<figure class="highlight bash"><figcaption><span>Procfile.dev</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">web:       python manage.py runserver <span class="number">192.168</span>.<span class="number">7.12</span>:<span class="number">8051</span></span><br><span class="line">celery:    celery -A cabot worker --loglevel=DEBUG -B -c <span class="number">8</span> -Ofair</span><br></pre></td></tr></table></figure>
<p>修改 <code>setup.py</code> 添加依赖</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'MySQL-python==1.2.5'</span>,</span><br></pre></td></tr></table></figure>
<p>后安装依赖，并初始化数据库，在初始化时，会让你指定一个用户名和密码</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">python setup.py install</span><br><span class="line">/bin/sh ./setup_dev.sh</span><br></pre></td></tr></table></figure>
<p>一系列完成后，则你可以在数据库看到很多张表，如果中间报错，多半是因为环境问题，缺少python 包，可以通过 pip 安装<br>在官网 <a href="https://pypi.python.org/pypi" target="_blank" rel="external"><font color="#2798a2">https://pypi.python.org/pypi</font></a>查找，并通过命令行 安装（pip安装需要前置安装setuptools，请自行百度）。</p>
<p>启动 <code>nohup foreman start &amp;</code> ，如果启动不成功，需要仔细阅读 nohup.out 日志内容定位</p>
<p>参考：<br><a href="http://blog.gaoyuan.xyz/2014/10/01/use-graphite-and-alter-build-monitor-system/?utm_source=tuicool&amp;utm_medium=referral" target="_blank" rel="external"><font color="#2798a2">http://blog.gaoyuan.xyz/2014/10/01/use-graphite-and-alter-build-monitor-system/?utm_source=tuicool&amp;utm_medium=referral</font></a></p>
]]></content>
    <summary type="html">
    <![CDATA[graphite 及 cabot 部署 及 配置]]>
    
    </summary>
    
      <category term="cabot" scheme="http://yoursite.com/tags/cabot/"/>
    
      <category term="graphite" scheme="http://yoursite.com/tags/graphite/"/>
    
      <category term="graphite" scheme="http://yoursite.com/categories/graphite/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[spark on yarn]]></title>
    <link href="http://yoursite.com/blog/2015/09/09/spark-on-yarn/"/>
    <id>http://yoursite.com/blog/2015/09/09/spark-on-yarn/</id>
    <published>2015-09-09T08:08:17.000Z</published>
    <updated>2016-01-16T03:30:37.000Z</updated>
    <content type="html"><![CDATA[<p>介绍如何让 spark 跑在 yarn 上</p>
<a id="more"></a>
<p>spark 可以单独部署，用自带的资源管理器，也支持在 yarn 上运行. 本文介绍 spark streaming on yarn client mode</p>
<hr>
<h2 id="u73AF_u5883_u51C6_u5907"><a href="#u73AF_u5883_u51C6_u5907" class="headerlink" title="环境准备"></a>环境准备</h2><ol>
<li>hadoop集群</li>
<li>在每个节点上下载spark 1.4.0 的tar包并解压</li>
<li>在 /etc/profile 下设置 YARN_CONF_DIR 以便能够让 spark 提交任务的时候读取到 yarn 的 ResourceManager 地址后申请资源</li>
<li>kafka 0.8.2.1</li>
<li>scala 2.10.5</li>
</ol>
<hr>
<h2 id="u5DE5_u7A0B_u642D_u5EFA_u6982_u8FF0"><a href="#u5DE5_u7A0B_u642D_u5EFA_u6982_u8FF0" class="headerlink" title="工程搭建概述"></a>工程搭建概述</h2><p>sbt 和 maven scala插件都可以创建 spark job 任务，本质上还是打成了一个 jar 包。本文介绍的是 spark streaming on yarn ，因为spark更适合处理实时流的任务</p>
<p>如果是离线计算就交给hadoop吧</p>
<p>这里有篇文章 官网介绍(虽然是 spark 1.1.0 但实际上代码是兼容的，目前我跑在 1.4.0 上没有问题) <a href="https://spark.apache.org/docs/1.1.0/streaming-programming-guide.html" target="_blank" rel="external"><font color="#528ad5">https://spark.apache.org/docs/1.1.0/streaming-programming-guide.html</font></a></p>
<p>这里介绍sbt工程需要注意的地方：</p>
<ul>
<li>build.sbt： 构建依赖管理文件，其中的scalaVersion需与安装的scala版本一致</li>
<li>assembly.sbt： 打包成jar包时，执行策略</li>
<li>plugins.sbt：用于添加仓库地址或插件</li>
</ul>
<h3 id="assembly-sbt"><a href="#assembly-sbt" class="headerlink" title="assembly.sbt"></a><font color="#cf753c">assembly.sbt</font></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">//一般来说还会有<span class="built_in">log</span>4j冲突的问题，添加下面解决</span><br><span class="line"><span class="keyword">case</span> <span class="string">"log4j.properties"</span> =&gt; MergeStrategy.first</span><br><span class="line"><span class="keyword">case</span> x <span class="keyword">if</span> x.contains(<span class="string">"org/apache/commons/logging"</span>) =&gt; MergeStrategy.last</span><br><span class="line"></span><br><span class="line">//这里指定了入口函数</span><br><span class="line">mainClass <span class="keyword">in</span> assembly := Some(<span class="string">"com.myproject.main.Main"</span>)</span><br></pre></td></tr></table></figure>
<h3 id="build-sbt"><a href="#build-sbt" class="headerlink" title="build.sbt"></a><font color="#cf753c">build.sbt</font></h3><p>像一些jar包可能 maven 仓库没有 就需要从本地添加</p>
<ul>
<li>spark-streaming_2.10-1.1.0.jar</li>
<li>spark-sql_2.10-1.1.0.jar</li>
<li>spark-core_2.10-1.1.0.jar</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sbt.<span class="type">Keys</span>._</span><br><span class="line"></span><br><span class="line">name := <span class="string">"Jinx"</span></span><br><span class="line">version := <span class="string">"1.0"</span></span><br><span class="line">scalaVersion := <span class="string">"2.10.5"</span></span><br><span class="line"></span><br><span class="line">libraryDependencies += <span class="string">"org.apache.spark"</span> %% <span class="string">"spark-streaming_2.10"</span> % <span class="string">"1.1.0"</span> % <span class="string">"provided"</span> from  <span class="string">"file://"</span> + baseDirectory.value + <span class="string">"/src/main/spark-lib/spark-streaming_2.10-1.1.0.jar"</span></span><br><span class="line">libraryDependencies +=  <span class="string">"org.apache.spark"</span> %% <span class="string">"spark-core"</span> % <span class="string">"1.1.0"</span> % <span class="string">"provided"</span> from <span class="string">"file://"</span> + baseDirectory.value + <span class="string">"/src/main/spark-lib/spark-core_2.10-1.1.0.jar"</span></span><br><span class="line">libraryDependencies +=  <span class="string">"org.apache.spark"</span> %% <span class="string">"spark-sql"</span> % <span class="string">"1.1.0"</span> % <span class="string">"provided"</span> from <span class="string">"file://"</span> + baseDirectory.value + <span class="string">"src/main/spark-lib/spark-sql_2.10-1.1.0.jar"</span></span><br><span class="line"></span><br><span class="line">libraryDependencies ++= <span class="type">Seq</span>(</span><br><span class="line">                            (<span class="string">"org.apache.hadoop"</span> % <span class="string">"hadoop-common"</span> % <span class="string">"2.3.0"</span>).exclude(<span class="string">"commons-httpclient"</span>, <span class="string">"commons-httpclient"</span>),</span><br><span class="line">                            (<span class="string">"org.apache.hadoop"</span> % <span class="string">"hadoop-hdfs"</span> % <span class="string">"2.3.0"</span>).exclude(<span class="string">"commons-httpclient"</span>, <span class="string">"commons-httpclient"</span>),</span><br><span class="line">                            (<span class="string">"org.apache.hadoop"</span> % <span class="string">"hadoop-yarn"</span> % <span class="string">"2.3.0"</span>).exclude(<span class="string">"commons-httpclient"</span>, <span class="string">"commons-httpclient"</span>),</span><br><span class="line">                            (<span class="string">"org.apache.hadoop"</span> % <span class="string">"hadoop-yarn-client"</span> % <span class="string">"2.3.0"</span>).exclude(<span class="string">"commons-httpclient"</span>, <span class="string">"commons-httpclient"</span>),</span><br><span class="line">                            (<span class="string">"org.apache.hadoop"</span> % <span class="string">"hadoop-client"</span> % <span class="string">"2.3.0"</span>).exclude(<span class="string">"commons-httpclient"</span>, <span class="string">"commons-httpclient"</span>)</span><br><span class="line">                            )</span><br><span class="line"></span><br><span class="line">libraryDependencies += <span class="string">"org.slf4j"</span> % <span class="string">"slf4j-api"</span> % <span class="string">"1.7.2"</span></span><br><span class="line">libraryDependencies +=</span><br><span class="line">  <span class="string">"org.slf4j"</span> % <span class="string">"slf4j-log4j12"</span> % <span class="string">"1.7.2"</span> excludeAll(</span><br><span class="line">    <span class="type">ExclusionRule</span>(organization = <span class="string">"log4j"</span>)</span><br><span class="line">    )</span><br><span class="line">libraryDependencies += <span class="string">"com.typesafe.akka"</span> % <span class="string">"akka-actor_2.10"</span> % <span class="string">"2.3.9"</span></span><br><span class="line">libraryDependencies += (<span class="string">"org.apache.httpcomponents"</span> % <span class="string">"httpclient"</span> % <span class="string">"4.3.2"</span>)</span><br><span class="line">libraryDependencies += (<span class="string">"net.sf.json-lib"</span> % <span class="string">"json-lib"</span> % <span class="string">"2.3"</span> classifier <span class="string">"jdk15"</span>)</span><br><span class="line">  .exclude(<span class="string">"commons-collections"</span>, <span class="string">"commons-collections"</span>)</span><br><span class="line"><span class="comment">//  .exclude("commons-beanutils", "commons-beanutils")</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//http://twitter.github.io/util/docs/#package</span></span><br><span class="line">libraryDependencies ++= <span class="type">Seq</span>(<span class="string">"com.twitter"</span> %% <span class="string">"util-collection"</span> % <span class="string">"6.23.0"</span>,</span><br><span class="line">                            <span class="string">"com.twitter"</span> % <span class="string">"util-eval_2.10"</span> % <span class="string">"6.25.0"</span>)</span><br><span class="line">libraryDependencies ++= <span class="type">Seq</span>(<span class="string">"org.mortbay.jetty"</span> % <span class="string">"jetty"</span> % <span class="string">"6.1.26"</span>,</span><br><span class="line">                            <span class="string">"org.mortbay.jetty"</span> % <span class="string">"servlet-api"</span> % <span class="string">"2.5-20081211"</span>,</span><br><span class="line">                            <span class="string">"org.mortbay.jetty"</span> % <span class="string">"jetty-util"</span> % <span class="string">"6.1.25"</span>)</span><br><span class="line">libraryDependencies +=</span><br><span class="line">  <span class="string">"log4j"</span> % <span class="string">"log4j"</span> % <span class="string">"1.2.16"</span> excludeAll(</span><br><span class="line">    <span class="type">ExclusionRule</span>(organization = <span class="string">"com.sun.jdmk"</span>),</span><br><span class="line">    <span class="type">ExclusionRule</span>(organization = <span class="string">"com.sun.jmx"</span>),</span><br><span class="line">    <span class="type">ExclusionRule</span>(organization = <span class="string">"javax.jms"</span>)</span><br><span class="line">    )</span><br><span class="line">libraryDependencies ++= <span class="type">Seq</span>(</span><br><span class="line">  <span class="string">"net.debasishg"</span> %% <span class="string">"redisclient"</span> % <span class="string">"3.0"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">name :=<span class="string">"myproject-Jinx-Stream"</span></span><br><span class="line">organization :=<span class="string">"com.myorg"</span></span><br><span class="line">version :=<span class="string">"0.0.1-SNAPSHOT"</span></span><br></pre></td></tr></table></figure>
<h3 id="u5176_u4E2D_u8FD0_u884C_u7684_u8F83_u6838_u5FC3_u7684_u4EE3_u7801"><a href="#u5176_u4E2D_u8FD0_u884C_u7684_u8F83_u6838_u5FC3_u7684_u4EE3_u7801" class="headerlink" title="其中运行的较核心的代码"></a><font color="#cf753c">其中运行的较核心的代码</font></h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.commons.lang.<span class="type">StringUtils</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka.<span class="type">KafkaUtils</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"><span class="keyword">import</span> scala.collection.mutable</span><br><span class="line"><span class="keyword">import</span> scala.collection.mutable.<span class="type">Map</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//初始化spark配置 appName是自己的名字 设置属性</span></span><br><span class="line"><span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"myTest"</span>)</span><br><span class="line">.set(<span class="string">"spark.yarn.jar"</span>,<span class="string">"hdfs://hadoop3:8020/share/spark-assembly-1.4.0-hadoop2.3.0.jar"</span>)</span><br><span class="line">        .set(<span class="string">"spark.yarn.historyServer.address"</span>, <span class="string">"http://hadoop3:18080"</span>)</span><br><span class="line">        .set(<span class="string">"spark.yarn.access.namenodes"</span>, <span class="string">"hdfs://hadoop3:8032"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//60秒切割一次RDD集</span></span><br><span class="line">ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">60</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">//读取topic 一般来说是单个topic</span></span><br><span class="line"><span class="keyword">val</span> topicMap = topics.split(<span class="string">","</span>).map((_, <span class="number">5</span>)).toMap</span><br><span class="line"></span><br><span class="line"><span class="comment">//zkQuorum集群 hadoop4:2181,hadoop5:2181,hadoop6:2181 ，group是kafka的组</span></span><br><span class="line"><span class="keyword">val</span> lines = <span class="type">KafkaUtils</span>.createStream(ssc, zkQuorum, group, topicMap).map(_._2)</span><br><span class="line"></span><br><span class="line"><span class="comment">//遍历RDD</span></span><br><span class="line">lines.foreachRDD(rdd =&gt;&#123;</span><br><span class="line">      println(<span class="string">"---begin---"</span>)</span><br><span class="line">      <span class="comment">//业务</span></span><br><span class="line">      println(<span class="string">"---end---"</span>)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">ssc.start()</span><br><span class="line">ssc.awaitTermination()</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="u542F_u52A8"><a href="#u542F_u52A8" class="headerlink" title="启动"></a><font color="#cf753c">启动</font></h3><ol>
<li><code>sbt clean assembly</code> 打好的jar包在 target/scala-2.10/jinx-spark.jar</li>
<li>启动</li>
</ol>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> YARN_CONF_DIR=/etc/hadoop/conf/ &amp;&amp; nohup /opt/app/spark-<span class="number">1.4</span>.<span class="number">0</span>-bin-hadoop2.<span class="number">3</span>/bin/spark-submit \</span><br><span class="line">--class com.myproject.main.Main \</span><br><span class="line">--master yarn-client \</span><br><span class="line">--num-executors <span class="number">5</span> \</span><br><span class="line">/data/jenkins/workspace/Jinx/target/scala-<span class="number">2.10</span>/jinx-spark.jar &gt;&gt; /data/jenkins/workspace/Jinx/jinx-run.log &amp;</span><br></pre></td></tr></table></figure>
<h3 id="u5173_u95ED"><a href="#u5173_u95ED" class="headerlink" title="关闭"></a><font color="#cf753c">关闭</font></h3><ol>
<li>先找到 yarn 上运行的任务 <code>yarn application -list | grep &lt;keyword&gt;</code></li>
<li><code>yarn application -kill &lt;$applicationId&gt;</code> 关闭掉 yarn应用、执行器</li>
<li>在部署机器上 <code>kill -9 $pid</code> 关闭掉 spark driver</li>
</ol>
<h3 id="u5176_u4E2Djenkins__u53EF_u4EE5_u505Asbt_u9879_u76EE_u7684CI"><a href="#u5176_u4E2Djenkins__u53EF_u4EE5_u505Asbt_u9879_u76EE_u7684CI" class="headerlink" title="其中jenkins 可以做sbt项目的CI"></a><font color="#cf753c">其中jenkins 可以做sbt项目的CI</font></h3><p>我的开发环境脚本如下，可以参考 （记着jenkins 需要设置参数化构建 BUILD_ID=dontKillMe 不然出于保护原因，是不允许后台运行 jenkins所产生的进程的）</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="shebang">#!/usr/bin/env bash</span><br><span class="line"></span></span><br><span class="line"><span class="comment"># kill yarn</span></span><br><span class="line">YARN_ID=`yarn application -list | grep MyTest |  awk <span class="string">'&#123;print $1&#125;'</span>`</span><br><span class="line"><span class="keyword">for</span> yid <span class="keyword">in</span> <span class="variable">$YARN_ID</span></span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">       <span class="built_in">echo</span> <span class="string">"kill yarn <span class="variable">$yid</span>"</span></span><br><span class="line">       yarn application -kill <span class="variable">$yid</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># kill jinx drive</span></span><br><span class="line">ID=`ps -ef | grep <span class="string">'jinx-spark.jar'</span> | awk <span class="string">'&#123;print $2&#125;'</span>`</span><br><span class="line"><span class="keyword">for</span> id <span class="keyword">in</span> <span class="variable">$ID</span></span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">        <span class="built_in">echo</span> <span class="string">"kill process <span class="variable">$id</span>"</span></span><br><span class="line">        <span class="built_in">kill</span> -<span class="number">9</span> <span class="variable">$id</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># start up</span></span><br><span class="line"><span class="built_in">export</span> YARN_CONF_DIR=/etc/hadoop/conf/</span><br><span class="line"></span><br><span class="line">cp /data/jenkins/workspace/Jinx/target/scala-<span class="number">2.10</span>/jinx-spark.jar  /data/workspace/jinx</span><br><span class="line"><span class="built_in">cd</span> /data/workspace/jinx</span><br><span class="line">nohup /opt/app/spark-<span class="number">1.4</span>.<span class="number">0</span>-bin-hadoop2.<span class="number">3</span>/bin/spark-submit --class com.myproject.main.Main --master yarn-client --num-executors <span class="number">5</span> /data/jenkins/workspace/Jinx/target/scala-<span class="number">2.10</span>/jinx-spark.jar &gt;&gt; /data/jenkins/workspace/Jinx/jinx-run.log <span class="number">2</span>&gt;&amp;<span class="number">1</span> &amp;</span><br></pre></td></tr></table></figure>
<h3 id="u53EF_u80FD_u4F1A_u9047_u5230_u7684_u95EE_u9898"><a href="#u53EF_u80FD_u4F1A_u9047_u5230_u7684_u95EE_u9898" class="headerlink" title="可能会遇到的问题"></a>可能会遇到的问题</h3><p>内存OOM，适当加大 driver 程序的内存<br><a href="http://stackoverflow.com/questions/26562033/how-to-set-apache-spark-executor-memory" target="_blank" rel="external"><font color="#528ad5">http://stackoverflow.com/questions/26562033/how-to-set-apache-spark-executor-memory</font></a><br><img src="/images/spark/20150909/1.png" alt=""></p>
]]></content>
    <summary type="html">
    <![CDATA[spark on yarn 部署 及 配置]]>
    
    </summary>
    
      <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
      <category term="spark" scheme="http://yoursite.com/categories/spark/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[install postgresql xl]]></title>
    <link href="http://yoursite.com/blog/2015/08/07/install-postgresql-xl/"/>
    <id>http://yoursite.com/blog/2015/08/07/install-postgresql-xl/</id>
    <published>2015-08-07T08:01:10.000Z</published>
    <updated>2016-01-16T03:34:45.000Z</updated>
    <content type="html"><![CDATA[<p>pgxl 是一个基于 pg 可扩展的 架构，安装部署在几台机器上</p>
<a id="more"></a>
<p>postgresql 有兴趣的可以了解一下，也是一种关系型数据库。postgresql 配合 pg-xl 组件，可以达到类似 hadoop 一样的分布式效果。而且还有分布式事务的特性哦</p>
<hr>
<h2 id="u73AF_u5883_u51C6_u5907"><a href="#u73AF_u5883_u51C6_u5907" class="headerlink" title="环境准备"></a>环境准备</h2><p>PS：本文的研究者为同事 立超 同学，本人仅在测试环境下，根据指导实践了一遍</p>
<p>centos6 机器多台，可选下面的依赖库</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum -y install lrzsz sysstat e4fsprogs ntp readline-devel zlib zlib-devel openssl openssl-devel pam-devel libxml2-devel libxslt-devel python-devel tcl-devel gcc make smartmontools flex bison perl perl-devel perl-ExtUtils* OpenIPMI-tools openldap openldap-devel</span><br></pre></td></tr></table></figure>
<hr>
<h2 id="u5B89_u88C5_u6B65_u9AA4"><a href="#u5B89_u88C5_u6B65_u9AA4" class="headerlink" title="安装步骤"></a>安装步骤</h2><h3 id="u4E0B_u8F7Dpgxl_uFF0C_u5E76_u7F16_u8BD1_u5B89_u88C5"><a href="#u4E0B_u8F7Dpgxl_uFF0C_u5E76_u7F16_u8BD1_u5B89_u88C5" class="headerlink" title="下载pgxl，并编译安装"></a>下载pgxl，并编译安装</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">wget http://jaist.dl.sourceforge.net/project/postgres-xl/Releases/Version_9.<span class="number">2</span>rc/postgres-xl-v9.<span class="number">2</span>-src.tar.gz</span><br><span class="line">tar -zxvf postgres-xl-v9.<span class="number">2</span>-src.tar.gz</span><br><span class="line"></span><br><span class="line"><span class="comment">#解决html.index Error 1的问题</span></span><br><span class="line"><span class="built_in">cd</span> postgres-xl/doc-xc/src/sgml/</span><br><span class="line">rm -rf bookindex.sgmlin &amp;&amp; <span class="built_in">cd</span> ../../..</span><br><span class="line"></span><br><span class="line"><span class="comment">#编译安装</span></span><br><span class="line">./configure --prefix=/home/pgxl/pgxl9.<span class="number">2</span> --with-pgport=<span class="number">11921</span> --with-perl --with-tcl --with-python --with-openssl --with-pam --with-ldap --with-libxml --with-libxslt --enable-thread-safety --with-blocksize=<span class="number">32</span></span><br><span class="line"></span><br><span class="line">make &amp;&amp; make install</span><br></pre></td></tr></table></figure>
<h3 id="u6DFB_u52A0_u73AF_u5883_uFF0C_u7528_u6237"><a href="#u6DFB_u52A0_u73AF_u5883_uFF0C_u7528_u6237" class="headerlink" title="添加环境，用户"></a>添加环境，用户</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">useradd pgxl</span><br><span class="line">su - pgxl</span><br><span class="line">vi .bashrc</span><br><span class="line"><span class="built_in">export</span> PGHOME=/home/pgxl/pgxl9.<span class="number">2</span></span><br><span class="line"><span class="built_in">export</span> LD_LIBRARY_PATH=<span class="variable">$PGHOME</span>/lib:/lib64:/usr/lib64:/usr/<span class="built_in">local</span>/lib64:/lib:/usr/lib:/usr/<span class="built_in">local</span>/lib:<span class="variable">$LD_LIBRARY_PATH</span></span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PGHOME</span>/bin:<span class="variable">$PATH</span>:.</span><br></pre></td></tr></table></figure>
<h3 id="u89D2_u8272_u4ECB_u7ECD_coordinator__u548C_datanode"><a href="#u89D2_u8272_u4ECB_u7ECD_coordinator__u548C_datanode" class="headerlink" title="角色介绍 coordinator 和 datanode"></a>角色介绍 coordinator 和 datanode</h3><ul>
<li>coordinator 作为协调者 ，类似 hdfs Namenode</li>
<li>datanode 作为存储者，类似 hdfs datanode</li>
<li>GTM（global Transcation Manager)</li>
</ul>
<p>在配置时 需要让 端口访问 不仅 datanode 需要互通 ，还要都能访问到 coordinator 互通</p>
<h4 id="u914D_u7F6Ecoordinator"><a href="#u914D_u7F6Ecoordinator" class="headerlink" title="配置coordinator"></a>配置coordinator</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line">initdb -D /data/pgxl/c11921/pg_root --nodename=c11921 -E UTF8 --locale=C -U postgres -W</span><br><span class="line"><span class="built_in">cd</span> /data/pgxl/c11921/pg_root</span><br><span class="line"></span><br><span class="line">vi pg_hba.conf</span><br><span class="line">host all all <span class="number">0.0</span>.<span class="number">0.0</span>/<span class="number">0</span> md5</span><br><span class="line"></span><br><span class="line">vi postgresql.conf</span><br><span class="line">listen_addresses = <span class="string">'*'</span>          <span class="comment"># what IP address(es) to listen on;</span></span><br><span class="line">port = <span class="number">11921</span>                            <span class="comment"># (change requires restart)</span></span><br><span class="line">max_connections = <span class="number">500</span>                   <span class="comment"># (change requires restart)</span></span><br><span class="line">superuser_reserved_connections = <span class="number">13</span>     <span class="comment"># (change requires restart)</span></span><br><span class="line">unix_socket_directory = <span class="string">'.'</span>             <span class="comment"># (change requires restart)</span></span><br><span class="line">unix_socket_permissions = <span class="number">0700</span>          <span class="comment"># begin with 0 to use octal notation</span></span><br><span class="line">tcp_keepalives_idle = <span class="number">60</span>                <span class="comment"># TCP_KEEPIDLE, in seconds;</span></span><br><span class="line">tcp_keepalives_interval = <span class="number">10</span>            <span class="comment"># TCP_KEEPINTVL, in seconds;</span></span><br><span class="line">tcp_keepalives_count = <span class="number">10</span>               <span class="comment"># TCP_KEEPCNT;</span></span><br><span class="line">shared_buffers = <span class="number">2048</span>MB                 <span class="comment"># min 128kB</span></span><br><span class="line">max_prepared_transactions = <span class="number">500</span>         <span class="comment"># zero disables the feature</span></span><br><span class="line">vacuum_cost_delay = <span class="number">10</span>ms                <span class="comment"># 0-100 milliseconds</span></span><br><span class="line">vacuum_cost_<span class="built_in">limit</span> = <span class="number">10000</span>               <span class="comment"># 1-10000 credits</span></span><br><span class="line">bgwriter_delay = <span class="number">10</span>ms                   <span class="comment"># 10-10000ms between rounds</span></span><br><span class="line">shared_queues = <span class="number">64</span>                      <span class="comment"># min 16</span></span><br><span class="line">shared_queue_size = <span class="number">262144</span>               <span class="comment"># min 16KB</span></span><br><span class="line">wal_level = hot_standby                 <span class="comment"># minimal, archive, or hot_standby</span></span><br><span class="line">synchronous_commit = off                <span class="comment"># synchronization level;</span></span><br><span class="line">wal_sync_method = fdatasync             <span class="comment"># the default is the first option</span></span><br><span class="line">wal_buffers = <span class="number">16384</span>kB                   <span class="comment"># min 32kB, -1 sets based on shared_buffers</span></span><br><span class="line">wal_writer_delay = <span class="number">10</span>ms         <span class="comment"># 1-10000 milliseconds</span></span><br><span class="line">checkpoint_segments = <span class="number">128</span>               <span class="comment"># in logfile segments, min 1, 16MB each</span></span><br><span class="line">archive_mode = on               <span class="comment"># allows archiving to be done</span></span><br><span class="line">archive_<span class="built_in">command</span> = <span class="string">'/bin/date'</span>           <span class="comment"># command to use to archive a logfile segment</span></span><br><span class="line">max_wal_senders = <span class="number">32</span>            <span class="comment"># max number of walsender processes</span></span><br><span class="line">hot_standby = on                        <span class="comment"># "on" allows queries during recovery</span></span><br><span class="line">max_standby_archive_delay = <span class="number">300</span>s        <span class="comment"># max delay before canceling queries</span></span><br><span class="line">max_standby_streaming_delay = <span class="number">300</span>s      <span class="comment"># max delay before canceling queries</span></span><br><span class="line">wal_receiver_status_interval = <span class="number">1</span>s       <span class="comment"># send replies at least this often</span></span><br><span class="line">hot_standby_feedback = on               <span class="comment"># send info from standby to prevent</span></span><br><span class="line">remote_query_cost = <span class="number">100.0</span>               <span class="comment"># same scale as above</span></span><br><span class="line">effective_cache_size = <span class="number">96000</span>MB</span><br><span class="line"><span class="built_in">log</span>_destination = <span class="string">'csvlog'</span>              <span class="comment"># Valid values are combinations of</span></span><br><span class="line">logging_collector = on          <span class="comment"># Enable capturing of stderr and csvlog</span></span><br><span class="line"><span class="built_in">log</span>_directory = <span class="string">'pg_log'</span>                <span class="comment"># directory where log files are written,</span></span><br><span class="line"><span class="built_in">log</span>_filename = <span class="string">'postgresql-%Y-%m-%d_%H%M%S.log'</span> <span class="comment"># log file name pattern,</span></span><br><span class="line"><span class="built_in">log</span>_file_mode = <span class="number">0600</span>                    <span class="comment"># creation mode for log files,</span></span><br><span class="line"><span class="built_in">log</span>_truncate_on_rotation = on           <span class="comment"># If on, an existing log file with the</span></span><br><span class="line"><span class="built_in">log</span>_min_duration_statement = <span class="number">1</span>s <span class="comment"># -1 is disabled, 0 logs all statements</span></span><br><span class="line"><span class="built_in">log</span>_checkpoints = on</span><br><span class="line"><span class="built_in">log</span>_connections = on</span><br><span class="line"><span class="built_in">log</span>_disconnections = on</span><br><span class="line"><span class="built_in">log</span>_error_verbosity = verbose           <span class="comment"># terse, default, or verbose messages</span></span><br><span class="line"><span class="built_in">log</span>_lock_waits = on                     <span class="comment"># log lock waits &gt;= deadlock_timeout</span></span><br><span class="line"><span class="built_in">log</span>_statement = <span class="string">'ddl'</span>                   <span class="comment"># none, ddl, mod, all</span></span><br><span class="line"><span class="built_in">log</span>_timezone = <span class="string">'PRC'</span></span><br><span class="line">autovacuum = on                 <span class="comment"># Enable autovacuum subprocess?  'on'</span></span><br><span class="line"><span class="built_in">log</span>_autovacuum_min_duration = <span class="number">0</span> <span class="comment"># -1 disables, 0 logs all actions and</span></span><br><span class="line">autovacuum_vacuum_cost_delay = <span class="number">10</span>ms     <span class="comment"># default vacuum cost delay for</span></span><br><span class="line">datestyle = <span class="string">'iso, mdy'</span></span><br><span class="line">timezone = <span class="string">'PRC'</span></span><br><span class="line">lc_messages = <span class="string">'C'</span>                       <span class="comment"># locale for system error message</span></span><br><span class="line">lc_monetary = <span class="string">'C'</span>                       <span class="comment"># locale for monetary formatting</span></span><br><span class="line">lc_numeric = <span class="string">'C'</span>                        <span class="comment"># locale for number formatting</span></span><br><span class="line">lc_time = <span class="string">'C'</span>                           <span class="comment"># locale for time formatting</span></span><br><span class="line">default_text_search_config = <span class="string">'pg_catalog.english'</span></span><br><span class="line">pooler_port = <span class="number">21921</span>                     <span class="comment"># Pool Manager TCP port</span></span><br><span class="line">max_pool_size = <span class="number">100</span>                     <span class="comment"># Maximum pool size</span></span><br><span class="line">pool_conn_keepalive = <span class="number">60</span>                <span class="comment"># Close connections if they are idle</span></span><br><span class="line">pool_maintenance_timeout = <span class="number">30</span>           <span class="comment"># Launch maintenance routine if pooler</span></span><br><span class="line">max_coordinators = <span class="number">16</span>                   <span class="comment"># Maximum number of Coordinators</span></span><br><span class="line">max_datanodes = <span class="number">16</span>                      <span class="comment"># Maximum number of Datanodes</span></span><br><span class="line">gtm_host = <span class="string">'127.0.0.1'</span>                  <span class="comment"># Host name or address of GTM</span></span><br><span class="line">gtm_port = <span class="number">11926</span>                        <span class="comment"># Port of GTM</span></span><br><span class="line">pgxc_node_name = <span class="string">'c11921'</span>                       <span class="comment"># Coordinator or Datanode name</span></span><br><span class="line">sequence_range = <span class="number">10000</span></span><br></pre></td></tr></table></figure>
<h4 id="u914D_u7F6Edatanode"><a href="#u914D_u7F6Edatanode" class="headerlink" title="配置datanode"></a>配置datanode</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line">initdb -D /data/pgxl/d11922/pg_root --nodename=d11922 -E UTF8 --locale=C -U postgres -W</span><br><span class="line"><span class="built_in">cd</span> /data/pgxl/d11922/pg_root</span><br><span class="line"></span><br><span class="line">vi pg_hba.conf</span><br><span class="line"><span class="built_in">local</span>   all             all                                     trust</span><br><span class="line">host    all             all             <span class="number">127.0</span>.<span class="number">0.1</span>/<span class="number">32</span>            trust</span><br><span class="line">host    all             all             ::<span class="number">1</span>/<span class="number">128</span>                 trust</span><br><span class="line"></span><br><span class="line">vi postgresql.conf</span><br><span class="line">listen_addresses = <span class="string">'*'</span>          <span class="comment"># what IP address(es) to listen on;</span></span><br><span class="line">port = <span class="number">11922</span>                            <span class="comment"># (change requires restart)</span></span><br><span class="line">max_connections = <span class="number">500</span>                   <span class="comment"># (change requires restart)</span></span><br><span class="line">superuser_reserved_connections = <span class="number">13</span>     <span class="comment"># (change requires restart)</span></span><br><span class="line">unix_socket_directory = <span class="string">'.'</span>             <span class="comment"># (change requires restart)</span></span><br><span class="line">unix_socket_permissions = <span class="number">0700</span>          <span class="comment"># begin with 0 to use octal notation</span></span><br><span class="line">tcp_keepalives_idle = <span class="number">60</span>                <span class="comment"># TCP_KEEPIDLE, in seconds;</span></span><br><span class="line">tcp_keepalives_interval = <span class="number">10</span>            <span class="comment"># TCP_KEEPINTVL, in seconds;</span></span><br><span class="line">tcp_keepalives_count = <span class="number">10</span>               <span class="comment"># TCP_KEEPCNT;</span></span><br><span class="line">shared_buffers = <span class="number">2048</span>MB                 <span class="comment"># min 128kB</span></span><br><span class="line">max_prepared_transactions = <span class="number">500</span>         <span class="comment"># zero disables the feature</span></span><br><span class="line">vacuum_cost_delay = <span class="number">10</span>ms                <span class="comment"># 0-100 milliseconds</span></span><br><span class="line">vacuum_cost_<span class="built_in">limit</span> = <span class="number">10000</span>               <span class="comment"># 1-10000 credits</span></span><br><span class="line">bgwriter_delay = <span class="number">10</span>ms                   <span class="comment"># 10-10000ms between rounds</span></span><br><span class="line">shared_queues = <span class="number">64</span>                      <span class="comment"># min 16</span></span><br><span class="line">shared_queue_size = <span class="number">262144</span>               <span class="comment"># min 16KB</span></span><br><span class="line">wal_level = hot_standby                 <span class="comment"># minimal, archive, or hot_standby</span></span><br><span class="line">synchronous_commit = off                <span class="comment"># synchronization level;</span></span><br><span class="line">wal_sync_method = fdatasync             <span class="comment"># the default is the first option</span></span><br><span class="line">wal_buffers = <span class="number">16384</span>kB                   <span class="comment"># min 32kB, -1 sets based on shared_buffers</span></span><br><span class="line">wal_writer_delay = <span class="number">10</span>ms         <span class="comment"># 1-10000 milliseconds</span></span><br><span class="line">checkpoint_segments = <span class="number">128</span>               <span class="comment"># in logfile segments, min 1, 16MB each</span></span><br><span class="line">archive_mode = on               <span class="comment"># allows archiving to be done</span></span><br><span class="line">archive_<span class="built_in">command</span> = <span class="string">'cp %p /home/postgres/archive/%f'</span>           <span class="comment"># command to use to archive a logfile segment</span></span><br><span class="line">max_wal_senders = <span class="number">32</span>            <span class="comment"># max number of walsender processes</span></span><br><span class="line">hot_standby = on                        <span class="comment"># "on" allows queries during recovery</span></span><br><span class="line">max_standby_archive_delay = <span class="number">300</span>s        <span class="comment"># max delay before canceling queries</span></span><br><span class="line">max_standby_streaming_delay = <span class="number">300</span>s      <span class="comment"># max delay before canceling queries</span></span><br><span class="line">wal_receiver_status_interval = <span class="number">1</span>s       <span class="comment"># send replies at least this often</span></span><br><span class="line">hot_standby_feedback = on               <span class="comment"># send info from standby to prevent</span></span><br><span class="line">remote_query_cost = <span class="number">100.0</span>               <span class="comment"># same scale as above</span></span><br><span class="line">effective_cache_size = <span class="number">96000</span>MB</span><br><span class="line"><span class="built_in">log</span>_destination = <span class="string">'csvlog'</span>              <span class="comment"># Valid values are combinations of</span></span><br><span class="line">logging_collector = on          <span class="comment"># Enable capturing of stderr and csvlog</span></span><br><span class="line"><span class="built_in">log</span>_directory = <span class="string">'pg_log'</span>                <span class="comment"># directory where log files are written,</span></span><br><span class="line"><span class="built_in">log</span>_filename = <span class="string">'postgresql-%Y-%m-%d_%H%M%S.log'</span> <span class="comment"># log file name pattern,</span></span><br><span class="line"><span class="built_in">log</span>_file_mode = <span class="number">0600</span>                    <span class="comment"># creation mode for log files,</span></span><br><span class="line"><span class="built_in">log</span>_truncate_on_rotation = on           <span class="comment"># If on, an existing log file with the</span></span><br><span class="line"><span class="built_in">log</span>_min_duration_statement = <span class="number">1</span>s <span class="comment"># -1 is disabled, 0 logs all statements</span></span><br><span class="line"><span class="built_in">log</span>_checkpoints = on</span><br><span class="line"><span class="built_in">log</span>_connections = on</span><br><span class="line"><span class="built_in">log</span>_disconnections = on</span><br><span class="line"><span class="built_in">log</span>_error_verbosity = verbose           <span class="comment"># terse, default, or verbose messages</span></span><br><span class="line"><span class="built_in">log</span>_lock_waits = on                     <span class="comment"># log lock waits &gt;= deadlock_timeout</span></span><br><span class="line"><span class="built_in">log</span>_statement = <span class="string">'ddl'</span>                   <span class="comment"># none, ddl, mod, all</span></span><br><span class="line"><span class="built_in">log</span>_timezone = <span class="string">'PRC'</span></span><br><span class="line">autovacuum = on                 <span class="comment"># Enable autovacuum subprocess?  'on'</span></span><br><span class="line"><span class="built_in">log</span>_autovacuum_min_duration = <span class="number">0</span> <span class="comment"># -1 disables, 0 logs all actions and</span></span><br><span class="line">autovacuum_vacuum_cost_delay = <span class="number">10</span>ms     <span class="comment"># default vacuum cost delay for</span></span><br><span class="line">datestyle = <span class="string">'iso, mdy'</span></span><br><span class="line">timezone = <span class="string">'PRC'</span></span><br><span class="line">lc_messages = <span class="string">'C'</span>                       <span class="comment"># locale for system error message</span></span><br><span class="line">lc_monetary = <span class="string">'C'</span>                       <span class="comment"># locale for monetary formatting</span></span><br><span class="line">lc_numeric = <span class="string">'C'</span>                        <span class="comment"># locale for number formatting</span></span><br><span class="line">lc_time = <span class="string">'C'</span>                           <span class="comment"># locale for time formatting</span></span><br><span class="line">default_text_search_config = <span class="string">'pg_catalog.english'</span></span><br><span class="line">pooler_port = <span class="number">21925</span>                     <span class="comment"># Pool Manager TCP port</span></span><br><span class="line">max_pool_size = <span class="number">100</span>                     <span class="comment"># Maximum pool size</span></span><br><span class="line">pool_conn_keepalive = <span class="number">60</span>                <span class="comment"># Close connections if they are idle</span></span><br><span class="line">pool_maintenance_timeout = <span class="number">30</span>           <span class="comment"># Launch maintenance routine if pooler</span></span><br><span class="line">max_coordinators = <span class="number">16</span>                   <span class="comment"># Maximum number of Coordinators</span></span><br><span class="line">max_datanodes = <span class="number">16</span>                      <span class="comment"># Maximum number of Datanodes</span></span><br><span class="line">gtm_host = <span class="string">'10.100.5.3'</span>                  <span class="comment"># Host name or address of GTM</span></span><br><span class="line">gtm_port = <span class="number">11926</span>                        <span class="comment"># Port of GTM</span></span><br><span class="line">pgxc_node_name = <span class="string">'d_41_11922'</span>                       <span class="comment"># Coordinator or Datanode name</span></span><br></pre></td></tr></table></figure>
<h3 id="u542F_u52A8datanode"><a href="#u542F_u52A8datanode" class="headerlink" title="启动datanode"></a>启动datanode</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pg_ctl start -Z datanode -D /data/pgxl/d11922/pg_root</span><br></pre></td></tr></table></figure>
<h3 id="u914D_u7F6EGTM"><a href="#u914D_u7F6EGTM" class="headerlink" title="配置GTM"></a>配置GTM</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">vi /data/pgxl/g11920/gtm.conf</span><br><span class="line">listen_addresses = <span class="string">'0.0.0.0'</span></span><br><span class="line"><span class="built_in">log</span>_file = <span class="string">'gtm.log'</span></span><br><span class="line"><span class="built_in">log</span>_min_messages = WARNING</span><br><span class="line">nodename = <span class="string">'g11920'</span></span><br><span class="line">port = <span class="number">11920</span></span><br><span class="line">startup = act</span><br></pre></td></tr></table></figure>
<h3 id="u542F_u52A8GTM"><a href="#u542F_u52A8GTM" class="headerlink" title="启动GTM"></a>启动GTM</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gtm_ctl -Z gtm start -D /data/pgxl/g11920/</span><br></pre></td></tr></table></figure>
<h3 id="u8FDE_u63A5datanode"><a href="#u8FDE_u63A5datanode" class="headerlink" title="连接datanode"></a>连接datanode</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">psql -h <span class="number">127.0</span>.<span class="number">0.1</span> -p <span class="number">11922</span> postgres postgres</span><br></pre></td></tr></table></figure>
<h3 id="u6DFB_u52A0_u8282_u70B9"><a href="#u6DFB_u52A0_u8282_u70B9" class="headerlink" title="添加节点"></a>添加节点</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">create node d_41_11922 with (<span class="built_in">type</span>=datanode, host=<span class="string">'10.100.5.41'</span>, port=<span class="number">11922</span>);</span><br><span class="line">select pgxc_pool_reload();</span><br></pre></td></tr></table></figure>
<p>测试环境类似</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">create node c11921 with (<span class="built_in">type</span>=coordinator, host=<span class="string">'10.200.8.47'</span>, port=<span class="number">11921</span>);</span><br><span class="line">create node d_47_11922 with (<span class="built_in">type</span>=datanode, host=<span class="string">'10.200.8.47'</span>, port=<span class="number">11922</span>);</span><br><span class="line">create node d_48_11922 with (<span class="built_in">type</span>=datanode, host=<span class="string">'10.200.8.48'</span>, port=<span class="number">11922</span>);</span><br></pre></td></tr></table></figure>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">postgres=# select * from pgxc_node;&#10; node_name  | node_type | node_port |  node_host  | nodeis_primary | nodeis_preferred |  node_id&#10;------------+-----------+-----------+-------------+----------------+------------------+------------&#10; c11921     | C         |      5432 | localhost   | f              | f                | 1123528383&#10; c_47_11921 | C         |     11921 | 10.200.8.47 | f              | f                | -389791404&#10; d_47_11922 | D         |     11922 | 10.200.8.47 | f              | f                |  851696902&#10; d_48_11922 | D         |     11922 | 10.200.8.48 | f              | f                |  715971365&#10;&#10;drop node c11921;</span><br></pre></td></tr></table></figure>
<p>同理连接 coord<br>psql -h 127.0.0.1 -p 11921 postgres postgres<br>有时删不掉 默认的localhost coord，需要删除自己的，然后alter local 成自己的</p>
<p>pg_hba.conf中，要保证coordinator到各datanode、各datanode之间可以无密码登陆</p>
<h2 id="u540E_u8BB0_3A"><a href="#u540E_u8BB0_3A" class="headerlink" title="后记:"></a>后记:</h2><p>如果机器物理内存小于16G，则启动报错，可以增加swap空间<a href="http://blog.slogra.com/post-246.html" target="_blank" rel="external">http://blog.slogra.com/post-246.html</a></p>
]]></content>
    <summary type="html">
    <![CDATA[pgxl 的部署]]>
    
    </summary>
    
      <category term="pgxl" scheme="http://yoursite.com/tags/pgxl/"/>
    
      <category term="pgxl" scheme="http://yoursite.com/categories/pgxl/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[install storm on yarn]]></title>
    <link href="http://yoursite.com/blog/2015/08/07/install-storm-on-yarn/"/>
    <id>http://yoursite.com/blog/2015/08/07/install-storm-on-yarn/</id>
    <published>2015-08-07T06:31:27.000Z</published>
    <updated>2016-01-16T03:41:01.000Z</updated>
    <content type="html"><![CDATA[<p>storm on yarn 部署</p>
<a id="more"></a>
<p>目前有 yarn ，但 storm 是利用自己资源管理的，如果把 资源统一管理，用 Yarn 或许可以减轻维护成本</p>
<hr>
<h2 id="u73AF_u5883_u51C6_u5907"><a href="#u73AF_u5883_u51C6_u5907" class="headerlink" title="环境准备"></a>环境准备</h2><ul>
<li>多台 centos6 x64 机器<ul>
<li>10.200.8.43  hadoop1 8G 154G 8CPU</li>
<li>10.200.8.44  hadoop2 16G 154G 8CPU</li>
<li>10.200.8.45  hadoop3 16G 1.5T 16CPU</li>
<li>10.200.8.46  hadoop4 16G 260G 16CPU</li>
<li>10.200.8.47  hadoop5 16G 260G 16CPU</li>
<li>10.200.8.48  hadoop6 16G 800G 8CPU</li>
<li>10.200.8.85  hadoop7 64G 42T 8CPU</li>
<li>10.200.8.88  hadoop8 64G 42T 8CPU</li>
</ul>
</li>
<li>maven 3.1.1</li>
<li>jdk7</li>
<li>git</li>
</ul>
<hr>
<h2 id="u6B65_u9AA4"><a href="#u6B65_u9AA4" class="headerlink" title="步骤"></a>步骤</h2><p>1.下载 yahoo 的 storm-on-yarn 工程 得到源代码 目录 storm-yarn-master</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> git@github.com:yahoo/storm-yarn.git</span><br></pre></td></tr></table></figure>
<p>2.解压 <code>storm-yarn-master/lib/storm-0.9.0-wip21.zip</code> 到 storm-yarn-master</p>
<p>最终目录是</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&lt;your dir&gt;/storm-yarn-master</span><br><span class="line">&lt;your dir&gt;/storm-<span class="number">0.9</span>.<span class="number">0</span>-wip21</span><br></pre></td></tr></table></figure>
<p>3.配置环境变量</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> STORM_WORK=&lt;your dir&gt;</span><br><span class="line"><span class="built_in">export</span> STORM_HOME=<span class="variable">$STORM_WORK</span></span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$STORM_WORK</span>/storm-yarn-master/bin:<span class="variable">$STORM_WORK</span>/storm-<span class="number">0.9</span>.<span class="number">0</span>-wip21/bin:<span class="variable">$PATH</span></span><br></pre></td></tr></table></figure>
<p>正常来说还要配置hadoop的环境，但我是用cloudera-manager 管理的，所以下面的配置在 Yarn 里是默认的</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> HADOOP_INSTALL=/opt/hadoop</span><br><span class="line"><span class="built_in">export</span> HADOOP_HOME=<span class="variable">$HADOOP_INSTALL</span></span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$HADOOP_INSTALL</span>/bin</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$HADOOP_INSTALL</span>/sbin</span><br><span class="line"><span class="built_in">export</span> HADOOP_MAPRED_HOME=<span class="variable">$HADOOP_INSTALL</span></span><br><span class="line"><span class="built_in">export</span> HADOOP_COMMON_HOME=<span class="variable">$HADOOP_INSTALL</span></span><br><span class="line"><span class="built_in">export</span> HADOOP_HDFS_HOME=<span class="variable">$HADOOP_INSTALL</span></span><br><span class="line"><span class="built_in">export</span> YARN_HOME=<span class="variable">$HADOOP_INSTALL</span></span><br></pre></td></tr></table></figure>
<p><img src="/images/storm/20150807/1.png" alt=""></p>
<p>4.进入工程代码，修改 pom.xml 中的hadoop版本，编译打包，打包完后会产生 storm-yarn-1.0-alpha.jar ，这个不用动</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="title">properties</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">storm.version</span>&gt;</span>0.9.0-wip21<span class="tag">&lt;/<span class="title">storm.version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">hadoop.version</span>&gt;</span>2.3.0<span class="tag">&lt;/<span class="title">hadoop.version</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--hadoop.version&gt;2.1.0.2.0.5.0-67&lt;/hadoop.version--&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">properties</span>&gt;</span></span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mvn package -DskipTests</span><br></pre></td></tr></table></figure>
<p>然后将 storm-yarn-master/lib/storm.zip 放到 hdfs 中作为 storm 环境 (如果有个性需要，添加Storm工程需要的额外Jar包到storm-0.9.0-wip21的lib下，重新压缩成storm.zip文件，上传至HDFS的指定目录中)</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -mkdir -p /lib/storm/<span class="number">0.9</span>.<span class="number">0</span>-wip21</span><br><span class="line">hadoop fs -put storm.zip /lib/storm/<span class="number">0.9</span>.<span class="number">0</span>-wip21/</span><br></pre></td></tr></table></figure>
<p>网上有说要建 storm 用户主目录，我并没有用 storm 用户下提交任务，所以这里具体是否真的必须，不是很清楚，但还是建了一个目录</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -mkdir -p /user/storm</span><br><span class="line">hadoop fs -chown storm /user/storm</span><br></pre></td></tr></table></figure>
<p>5.修改storm.yaml文件 <code>storm-0.9.0-wip21/conf/storm.yaml</code> (每行开头需要有个空格 ，属性和中划线中间也是)</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">storm.zookeeper.servers:</span><br><span class="line">    - <span class="string">"10.200.8.46"</span></span><br><span class="line">    - <span class="string">"10.200.8.47"</span></span><br><span class="line">    - <span class="string">"10.200.8.48"</span></span><br><span class="line"></span><br><span class="line">supervisor.slots.ports:</span><br><span class="line">   - <span class="number">6700</span></span><br><span class="line">   - <span class="number">6701</span></span><br><span class="line">   - <span class="number">6702</span></span><br><span class="line"></span><br><span class="line">storm.local.dir: /var/lib/hadoop-yarn</span><br><span class="line"></span><br><span class="line">master.initial-num-supervisors: <span class="number">3</span></span><br><span class="line"></span><br><span class="line">master.container.size-mb: <span class="number">128</span></span><br></pre></td></tr></table></figure>
<p>6.运行 <code>storm-yarn launch &lt;your dir&gt;/storm-0.9.0-wip21/conf/storm.yaml</code></p>
<p>提交成功后，会在 yarn 上产生一个 application ，这个就是 storm nimbus 和 ui 的进程</p>
<p><img src="/images/storm/20150807/2.png" alt=""></p>
<p>获取提交后的属性文件</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">storm-yarn getStormConfig -appId application_1438831879245_0021  -output ~/.storm/storm.yaml</span><br><span class="line"><span class="comment"># 通过以下命令得到Nimbus host</span></span><br><span class="line">cat ~/.storm/storm.yaml | grep nimbus.host</span><br></pre></td></tr></table></figure>
<p>然后提交自己的 storm jar 工程</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">storm jar /opt/storm/storm-yarn-master/lib/myStorm-<span class="number">0.0</span>.<span class="number">1</span>-SNAPSHOT-jar-with-dependencies.jar com.myStorm.App WordCountTopology -c nimbus.host=<span class="number">10.200</span>.<span class="number">8.88</span></span><br></pre></td></tr></table></figure>
<p>这时进入 nimbus 的 7070 默认UI端口，就可以看到我们的任务 和 supervisor 进程</p>
<p><img src="/images/storm/20150807/3.png" alt=""></p>
<p>其他命令</p>
<ul>
<li>关闭Topology： <code>storm kill [Topology_name]</code></li>
<li>关闭Storm on yarn集群：<code>storm-yarn shutdown –appId [applicationId]</code></li>
</ul>
<hr>
<h2 id="u9047_u5230_u7684_u95EE_u9898"><a href="#u9047_u5230_u7684_u95EE_u9898" class="headerlink" title="遇到的问题"></a>遇到的问题</h2><h3 id="storm-yarn_launch__u5931_u8D25"><a href="#storm-yarn_launch__u5931_u8D25" class="headerlink" title="storm-yarn launch 失败"></a><font color="#2798a2" size="3"> storm-yarn launch 失败 </font></h3><p>总报错 shell exit code 1 ，yarn 上有 application 的任务</p>
<p>我们可以看 hdfs 上 <code>/tmp/logs/&lt;user&gt;/logs/&lt;application_id&gt;/&lt;机器_8041&gt;</code> 里的内容 会看到 thrift 端口默认在 9000 启动，而9000 被占用了，启动不起来，修改<code>storm-yarn-master/src/main/resource/master_defaults.yaml</code> 里</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#master.thrift.port: 9000</span></span><br><span class="line">master.thrift.port: <span class="number">9009</span></span><br></pre></td></tr></table></figure>
<h3 id="supervisor__u8FDB_u7A0B_u542F_u52A8_u5931_u8D25"><a href="#supervisor__u8FDB_u7A0B_u542F_u52A8_u5931_u8D25" class="headerlink" title="supervisor 进程启动失败"></a><font color="#2798a2" size="3"> supervisor 进程启动失败 </font></h3><p>启动后 nimbus 和 ui 进程正常启动，但 storm web UI 里没有 supervisor 进程。网上99% 的图都是没有 supervisor 进程，都是错的。</p>
<p>原因是这样的，由于 storm nimbus 是作为 yarn 的 application 运行的，所以一开始 是独占资源的，会按照 yarn 上 ResourceManager 中配置的最大值来申请</p>
<p>应该在日志里有 <mem:xxx,vcores:yyy> ，最早 yarn 上的配置 RM 内存可最大申请 6G，16vCores，那么 在 hadoop1-6 由于内存偏小，所以被PASS掉了。只有hadoop7-8满足。</mem:xxx,vcores:yyy></p>
<p>但是申请 vCores 时，只有 hadoop3-4 是16核，其他都是8核，按最大申请，又PASS了其他机器，因此 内存 和 CPU 同时满足的机器，为0</p>
<p>修改 yarn 的内存 和 cpu 即可</p>
<h3 id="u865A_u62DF_u5185_u5B58_u8FC7_u5927__u5BFC_u81F4_storm_on_yarn__u7684_application__u88AB_kill__u6389"><a href="#u865A_u62DF_u5185_u5B58_u8FC7_u5927__u5BFC_u81F4_storm_on_yarn__u7684_application__u88AB_kill__u6389" class="headerlink" title="虚拟内存过大 导致 storm on yarn 的 application 被 kill 掉"></a><font color="#2798a2" size="3"> 虚拟内存过大 导致 storm on yarn 的 application 被 kill 掉 </font></h3><p>运行几个小时后，就挂了。原因不明，现只知道，启动时就会申请 16.8G 的虚拟内存，可以从 top 里看到。原因待查</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Application application_1438831879245_0001 failed <span class="number">2</span> <span class="built_in">times</span> due to AM Container</span><br><span class="line"><span class="keyword">for</span> appattempt_1438831879245_0001_000002 exited with <span class="built_in">exit</span>Code: <span class="number">143</span> due to: Container</span><br><span class="line">[pid=<span class="number">44545</span>,containerID=container_1438831879245_0001_02_000001] is running beyond physical memory limits.</span><br><span class="line">Current usage: <span class="number">1.0</span> GB of <span class="number">1</span> GB physical memory used; <span class="number">19.8</span> GB of <span class="number">2.1</span> GB virtual memory used. Killing container.</span><br></pre></td></tr></table></figure>
<hr>
<p>参考资料：</p>
<p><a href="http://www.tuicool.com/articles/BFr2Yv" target="_blank" rel="external">http://www.tuicool.com/articles/BFr2Yv</a><br><a href="http://blog.csdn.net/jiushuai/article/details/26693311" target="_blank" rel="external">http://blog.csdn.net/jiushuai/article/details/26693311</a><br><a href="http://www.cnblogs.com/prisoner/p/4647461.html" target="_blank" rel="external">http://www.cnblogs.com/prisoner/p/4647461.html</a></p>
]]></content>
    <summary type="html">
    <![CDATA[storm on yarn 部署]]>
    
    </summary>
    
      <category term="strom" scheme="http://yoursite.com/tags/strom/"/>
    
      <category term="storm" scheme="http://yoursite.com/categories/storm/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[sparkR read from hive]]></title>
    <link href="http://yoursite.com/blog/2015/07/28/sparkr-read-from-hive/"/>
    <id>http://yoursite.com/blog/2015/07/28/sparkr-read-from-hive/</id>
    <published>2015-07-28T09:41:26.000Z</published>
    <updated>2016-01-16T03:41:46.000Z</updated>
    <content type="html"><![CDATA[<p>spark 1.4.0 开始支持 自带 R 的接口，可以利用R语法分析数据</p>
<a id="more"></a>
<p>本文示例 用 sparkR on yarn 来分析 hive 表里的数据</p>
<hr>
<h2 id="u73AF_u5883_u51C6_u5907"><a href="#u73AF_u5883_u51C6_u5907" class="headerlink" title="环境准备"></a>环境准备</h2><ul>
<li>scala</li>
<li>hadoop 2.3.0</li>
<li>spark 1.4.0</li>
<li>hive (实验环境 0.12.0)</li>
</ul>
<p>从官网下载二进制包，spark-1.4.0-bin-hadoop2.3 (<a href="http://pan.baidu.com/s/1c0sEXhu" target="_blank" rel="external"><font color="#1c56b8">我的百度云共享</font></a>), <code>tar xvf spark-1.4.0-bin-hadoop2.3.tar</code> 解压。</p>
<hr>
<h2 id="spark_on_yarn"><a href="#spark_on_yarn" class="headerlink" title="spark on yarn"></a>spark on yarn</h2><p>设置环境变量，其中<code>YARN_CONF_DIR</code>是 spark on yarn 需要用到的环境变量</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#scala</span></span><br><span class="line"><span class="built_in">export</span> SCALA_HOME=/opt/app/scala-<span class="number">2.10</span>.<span class="number">5</span></span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$SCALA_HOME</span>/bin</span><br><span class="line"></span><br><span class="line"><span class="comment">#spark</span></span><br><span class="line"><span class="built_in">export</span> SPARK_HOME=/opt/app/spark-<span class="number">1.4</span>.<span class="number">0</span>-bin-hadoop2.<span class="number">3</span></span><br><span class="line"><span class="built_in">export</span> YARN_CONF_DIR=/etc/hadoop/conf/</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$SPARK_HOME</span>/bin:<span class="variable">$PATH</span></span><br></pre></td></tr></table></figure>
<p>sparkR 启动有2种方式，</p>
<p>1.交互模式 , 其中 <code>--num-executors</code> 代表执行机器的个数 , <code>--master yarn-client</code> 固定写死 （一般用 yarn-client 模式执行，yarn-cluster模式一直没成功）</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/opt/app/spark-<span class="number">1.4</span>.<span class="number">0</span>-bin-hadoop2.<span class="number">3</span>/bin/sparkR --num-executors <span class="number">5</span> --master yarn-client</span><br></pre></td></tr></table></figure>
<p>2.提交并执行R文件 ，类似下面命令</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/opt/app/spark-<span class="number">1.4</span>.<span class="number">0</span>-bin-hadoop2.<span class="number">3</span>/bin/spark-submit --master yarn-client --num-executors <span class="number">5</span> /opt/app/spark-<span class="number">1.4</span>.<span class="number">0</span>-bin-hadoop2.<span class="number">3</span>/examples/src/main/r/dataframe.R</span><br></pre></td></tr></table></figure>
<p>由于官网的demo dataframe.R 中，依赖了一个json文件，所以需要预先上传到 hdfs 上，路径根据自己的情况而定</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[yuanxiaolong@hadoop1 ~]$ hadoop fs -ls /opt/app/spark-<span class="number">1.4</span>.<span class="number">0</span>-bin-hadoop2.<span class="number">3</span>/examples/src/main/resources/people.json</span><br><span class="line">Found <span class="number">1</span> items</span><br><span class="line">-rw-r--r--   <span class="number">3</span> yuanxiaolong supergroup         <span class="number">73</span> <span class="number">2015</span>-<span class="number">07</span>-<span class="number">27</span> <span class="number">12</span>:<span class="number">03</span> /opt/app/spark-<span class="number">1.4</span>.<span class="number">0</span>-bin-hadoop2.<span class="number">3</span>/examples/src/main/resources/people.json</span><br></pre></td></tr></table></figure>
<hr>
<h2 id="read_data_from_hive"><a href="#read_data_from_hive" class="headerlink" title="read data from hive"></a>read data from hive</h2><p>1.先把hive的配置，让spark能感知到，即spark知道去连哪个hive metastore , 可以利用创建软连接的方式</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ln <span class="operator">-s</span> /etc/hive/conf/hive-site.xml /opt/app/spark-<span class="number">1.4</span>.<span class="number">0</span>-bin-hadoop2.<span class="number">3</span>/conf/hive-site.xml</span><br></pre></td></tr></table></figure>
<p>2.启动sparkR 交互模式，以便验证。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">/opt/app/spark-<span class="number">1.4</span>.<span class="number">0</span>-bin-hadoop2.<span class="number">3</span>/bin/sparkR</span><br><span class="line"><span class="comment">## 省略</span></span><br><span class="line">Welcome to SparkR!</span><br><span class="line"> Spark context is available as sc, SQL context is available as sqlContext</span><br><span class="line"></span><br><span class="line">&gt; hiveContext &lt;- sparkRHive.init(sc)</span><br><span class="line"><span class="number">15</span>/<span class="number">07</span>/<span class="number">28</span> <span class="number">17</span>:<span class="number">28</span>:<span class="number">29</span> INFO hive.HiveContext: Initializing execution hive, version <span class="number">0.13</span>.<span class="number">1</span></span><br><span class="line"><span class="number">15</span>/<span class="number">07</span>/<span class="number">28</span> <span class="number">17</span>:<span class="number">28</span>:<span class="number">29</span> INFO hive.metastore: Trying to connect to metastore with URI thrift://hadoop4:<span class="number">9083</span></span><br><span class="line"><span class="number">15</span>/<span class="number">07</span>/<span class="number">28</span> <span class="number">17</span>:<span class="number">28</span>:<span class="number">30</span> INFO hive.metastore: Connected to metastore.</span><br><span class="line"><span class="number">15</span>/<span class="number">07</span>/<span class="number">28</span> <span class="number">17</span>:<span class="number">28</span>:<span class="number">30</span> INFO session.SessionState: No Tez session required at this point. hive.execution.engine=mr.</span><br><span class="line"></span><br><span class="line"><span class="comment">## 省略</span></span><br><span class="line">&gt; showDF(sql(hiveContext, <span class="string">"USE data_row"</span>))</span><br><span class="line"><span class="comment">## 省略</span></span><br><span class="line">+------+</span><br><span class="line">|result|</span><br><span class="line">+------+</span><br><span class="line">+------+</span><br><span class="line"></span><br><span class="line">&gt; showDF(sql(hiveContext, <span class="string">"select vid from pcp_vod_temp limit 10"</span>))</span><br><span class="line"><span class="comment">## 省略</span></span><br><span class="line">+----+</span><br><span class="line">| vid|</span><br><span class="line">+----+</span><br><span class="line">|<span class="number">6296</span>|</span><br><span class="line">|<span class="number">5542</span>|</span><br><span class="line">|<span class="number">1962</span>|</span><br><span class="line">|<span class="number">4799</span>|</span><br><span class="line">|<span class="number">6006</span>|</span><br><span class="line">|<span class="number">6296</span>|</span><br><span class="line">|<span class="number">5542</span>|</span><br><span class="line">|<span class="number">1962</span>|</span><br><span class="line">|<span class="number">4799</span>|</span><br><span class="line">|<span class="number">6006</span>|</span><br><span class="line">+----+</span><br><span class="line"><span class="comment">## 省略</span></span><br><span class="line">&gt; rs &lt;- sql(hiveContext, <span class="string">"select vid from pcp_vod_temp limit 10"</span>)</span><br><span class="line">&gt; first(rs)</span><br><span class="line"><span class="comment">## 省略</span></span><br><span class="line">vid</span><br><span class="line"><span class="number">1</span> <span class="number">6296</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## 退出</span></span><br><span class="line">&gt; q()</span><br><span class="line">Save workspace image? [y/n/c]: n</span><br></pre></td></tr></table></figure>
<hr>
<h2 id="RStudio__u548C_SparkR__u7ED3_u5408"><a href="#RStudio__u548C_SparkR__u7ED3_u5408" class="headerlink" title="RStudio 和 SparkR 结合"></a>RStudio 和 SparkR 结合</h2><p>前置条件把 spark 1.4.0 的包，放置到 R中，可以通过软连接的方式</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ln <span class="operator">-s</span> /opt/app/spark-<span class="number">1.4</span>.<span class="number">0</span>-bin-hadoop2.<span class="number">3</span>/R/lib/SparkR /usr/lib64/R/library/</span><br></pre></td></tr></table></figure>
<p>进入RStudio 然后执行 sparkR ，执行（可以自己设置 executor的数量）</p>
<p><a href="https://github.com/apache/spark/tree/master/R" target="_blank" rel="external">https://github.com/apache/spark/tree/master/R</a></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Set this to where Spark is installed</span></span><br><span class="line">Sys.setenv(SPARK_HOME=<span class="string">"/Users/shivaram/spark"</span>)</span><br><span class="line">Sys.setenv(YARN_CONF_DIR=<span class="string">"/etc/hadoop/conf"</span>)</span><br><span class="line">Sys.setenv(SCALA_HOME=<span class="string">"/opt/app/scala-2.10.5"</span>)</span><br><span class="line"><span class="comment"># This line loads SparkR from the installed directory</span></span><br><span class="line">.libPaths(c(file.path(Sys.getenv(<span class="string">"SPARK_HOME"</span>), <span class="string">"R"</span>, <span class="string">"lib"</span>), .libPaths()))</span><br><span class="line">library(SparkR)</span><br><span class="line">sc &lt;- sparkR.init(master=<span class="string">"yarn-client"</span>)</span><br></pre></td></tr></table></figure>
]]></content>
    <summary type="html">
    <![CDATA[sparkR 读取 hive 表里的数据]]>
    
    </summary>
    
      <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
      <category term="spark" scheme="http://yoursite.com/categories/spark/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[flume process partial char]]></title>
    <link href="http://yoursite.com/blog/2015/07/17/flume-process-partial-char/"/>
    <id>http://yoursite.com/blog/2015/07/17/flume-process-partial-char/</id>
    <published>2015-07-17T03:50:06.000Z</published>
    <updated>2016-01-16T03:44:47.000Z</updated>
    <content type="html"><![CDATA[<p>修复了flume的一个BUG</p>
<a id="more"></a>
<p>当flume读取到 特殊字符时，会中断对整个文件的读取，1.5.0 。1.6.0 这里的代码稍微不一样，但是也直接返回 -1 了</p>
<hr>
<h2 id="u73B0_u8C61"><a href="#u73B0_u8C61" class="headerlink" title="现象"></a>现象</h2><p>flume source 为 spooldir 方式，默认会以utf8读取log文件，但是如果写入的数据夹杂了非utf8编码的记录，则flume读取到此非法字符时，就会中止整个文件的读取。</p>
<p>例如log文件中有1000行，500行时有这样一条记录 ，写入时以非utf8写</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;&#34;user_account&#34;:&#34;&#128174;&#27611;&#33151;&#33151;&#128174;&#34;,&#34;user_id&#8221;:&#34;11111111&#34;,&#34;ip&#8221;:&#8221;127.0.0.1&#34;,&#34;time&#34;:&#34;1430755546&#34;,&#34;hour&#34;:&#34;2015050500&#34;&#125;</span><br></pre></td></tr></table></figure>
<p>则flume 只会读取前500行，并传递给source，其中第500行</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;&#34;user_account&#34;:&#34;</span><br></pre></td></tr></table></figure>
<hr>
<h2 id="u5206_u6790_u5B9A_u4F4D"><a href="#u5206_u6790_u5B9A_u4F4D" class="headerlink" title="分析定位"></a>分析定位</h2><p>为什么会产生这样的现象？先一起来看一下 spooldir 读取日志的流程</p>
<p><img src="/images/flume/20150717/1.png" alt=""></p>
<p>flume会把日志里每一行，转换为一个Event 来处理，Event是最小单位。但flume操作的时候，以文件为逻辑单位，所以当遇到特殊字符时，jdk utf8 decoder 解析不出来，flume则认为到了文件末尾，因此就中止了整个文件的读取。</p>
<hr>
<h2 id="u89E3_u51B3"><a href="#u89E3_u51B3" class="headerlink" title="解决"></a>解决</h2><p>知道了原因，那么我们就可以修改flume源码了，修改ResettableFileInputStream.java</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// there may be a partial character in the decoder buffer</span></span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      incrPosition(delta, <span class="keyword">false</span>);</span><br><span class="line">      <span class="keyword">return</span> -<span class="number">1</span>;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>修改为，我们把特殊字符替换成空格（ASCII 32）flume 1.5.0</p>
<p>原理：当解析不出字符时，走到else分支，如果已到文件末尾，则自增全局文件指针 delta （值为1）个，并返回 -1 代表文件结束。如果未到文件末尾，则自增全局指针1下，跳过 “脏字符”，清空缓冲区，再填充，并处理</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// there may be a partial character in the decoder buffer</span></span><br><span class="line">   &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">     <span class="keyword">if</span>(isEndOfInput) &#123;</span><br><span class="line">         incrPosition(delta, <span class="keyword">false</span>);</span><br><span class="line">         logger.info(<span class="string">"End of File."</span>);</span><br><span class="line">         <span class="keyword">return</span> -<span class="number">1</span>;<span class="comment">//end of file</span></span><br><span class="line">     &#125; <span class="keyword">else</span>&#123;</span><br><span class="line">         incrPosition(<span class="number">1</span>, <span class="keyword">false</span>);</span><br><span class="line">         buf.clear();</span><br><span class="line">         buf.flip();</span><br><span class="line">         refillBuf();</span><br><span class="line">         logger.warn(<span class="string">"May have special characters."</span>);</span><br><span class="line">         <span class="keyword">return</span> <span class="number">32</span>;<span class="comment">//a partial character 空格的ASCII</span></span><br><span class="line">     &#125;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure>
<p>flume 1.6.0 <a href="https://github.com/apache/flume/blob/release-1.6.0/flume-ng-core/src/main/java/org/apache/flume/serialization/ResettableFileInputStream.java" target="_blank" rel="external">ResettableFileInputStream.java</a></p>
<hr>
<h2 id="u6784_u5EFAflume"><a href="#u6784_u5EFAflume" class="headerlink" title="构建flume"></a>构建flume</h2><h3 id="u73AF_u5883_u51C6_u5907"><a href="#u73AF_u5883_u51C6_u5907" class="headerlink" title="环境准备"></a>环境准备</h3><ul>
<li>选择一个flume版本，fork到自己的仓库</li>
<li>maven</li>
</ul>
<p>1.修改parent pom.xml，修改<hadoop.version>为自己的版本</hadoop.version></p>
<p>2.注释掉下面子pom.xml的单元测试</p>
<ul>
<li>flume-ng-sinks/flume-hdfs-sink/pom.xml</li>
<li>flume-tools/pom.xml</li>
</ul>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="title">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="title">groupId</span>&gt;</span>org.apache.hbase<span class="tag">&lt;/<span class="title">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="title">artifactId</span>&gt;</span>hbase<span class="tag">&lt;/<span class="title">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="title">version</span>&gt;</span>$&#123;hbase.version&#125;<span class="tag">&lt;/<span class="title">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="title">classifier</span>&gt;</span>tests<span class="tag">&lt;/<span class="title">classifier</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="title">scope</span>&gt;</span>test<span class="tag">&lt;/<span class="title">scope</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="title">dependency</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="title">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="title">groupId</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="title">artifactId</span>&gt;</span>hadoop-test<span class="tag">&lt;/<span class="title">artifactId</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="title">version</span>&gt;</span>$&#123;hadoop.version&#125;<span class="tag">&lt;/<span class="title">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>3.删除 flume-ng-sinks/flume-ng-hbase-sink/src/test 下的单元测试（我这里编译不过，因此删了）</p>
<p>4.获取ua-parser-1.3.0.jar ，并复制到本地仓库，<a href="http://pan.baidu.com/s/1kT69toN" target="_blank" rel="external"><font color="#3573c6">我的百度云共享</font></a></p>
<p>5.执行 <code>mvn install -Phadoop-2 -DskipTests -Dtar</code>  （如果你是hadoop2的话指定为 -Phadoop-2，否则不用添加）</p>
<p>6.flume-ng-dist/target  下就是构建完成的东西，apache-flume-1.5.2-bin.tar.gz 就可以直接用了</p>
<hr>
<h2 id="debug_flume_u6E90_u7801"><a href="#debug_flume_u6E90_u7801" class="headerlink" title="debug flume源码"></a>debug flume源码</h2><h3 id="u914D_u7F6E"><a href="#u914D_u7F6E" class="headerlink" title="配置"></a>配置</h3><p>启动flume命令，其中 –name realtime 是配置文件中自定义的，其中 http://<ip>:34545/metrics 是flume的监控统计信息</ip></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./flume-ng agent --conf ../conf --conf-file ../conf/flume-conf.properties --name realtime -Dflume.monitoring.type=http -Dflume.monitoring.port=<span class="number">34545</span></span><br></pre></td></tr></table></figure>
<p>修改 <code>flume-conf.properties</code> 其中 <code>realtime.sources.fortest.ignorePattern=^.*(?&lt;!\\d{4}\\.log)$</code> 表示只匹配监控 0001.log 、7743.log 这样的文件</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"># globel</span><br><span class="line">realtime.sources=fortest</span><br><span class="line">realtime.channels=fortest</span><br><span class="line">realtime.sinks=fortest</span><br><span class="line"></span><br><span class="line">#source</span><br><span class="line">realtime.sources.fortest.type=spooldir</span><br><span class="line">realtime.sources.fortest.spoolDir=/Users/xiaolongyuan/Downloads/apache-flume-1.5.0.1-bin/data/send</span><br><span class="line">realtime.sources.fortest.channels=fortest</span><br><span class="line">realtime.sources.fortest.batchSize=1000</span><br><span class="line">realtime.sources.fortest.deserializer=LINE</span><br><span class="line">realtime.sources.fortest.deserializer.maxLineLength=20000000</span><br><span class="line">realtime.sources.fortest.bufferMaxLineLength＝2000000</span><br><span class="line">realtime.sources.fortest.decodeErrorPolicy=IGNORE</span><br><span class="line">realtime.sources.fortest.ignorePattern=^.*(?&lt;!\\d&#123;4&#125;\\.log)$</span><br><span class="line"></span><br><span class="line">#channel</span><br><span class="line">realtime.channels.fortest.type = memory</span><br><span class="line">realtime.channels.fortest.capacity = 10000</span><br><span class="line">realtime.channels.fortest.transactionCapacity = 10000</span><br><span class="line">realtime.channels.fortest.byteCapacityBufferPercentage = 20</span><br><span class="line">realtime.channels.fortest.byteCapacity = 800000</span><br><span class="line"></span><br><span class="line">#sink</span><br><span class="line">realtime.sinks.fortest.type=file_roll</span><br><span class="line">realtime.sinks.fortest.sink.directory=/Users/xiaolongyuan/Downloads/apache-flume-1.5.0.1-bin/data/received</span><br><span class="line">realtime.sinks.fortest.channel = fortest</span><br></pre></td></tr></table></figure>
<p>本地读取file，输出到本地file，方便对比2个file的行数</p>
<h3 id="DEBUG"><a href="#DEBUG" class="headerlink" title="DEBUG"></a>DEBUG</h3><p>源码导入到eclipse 或 idea，并修改 <code>flume-env.sh</code></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">JAVA_OPTS=<span class="string">"-Xdebug -Xrunjdwp:transport=dt_socket,address=8001,server=y,suspend=y”</span></span><br></pre></td></tr></table></figure>
<p>在eclipse或idea里新建 remote debug，ip本地，端口8001，连接、打断点、测试。</p>
]]></content>
    <summary type="html">
    <![CDATA[flume bug 修复]]>
    
    </summary>
    
      <category term="flume" scheme="http://yoursite.com/tags/flume/"/>
    
      <category term="flume" scheme="http://yoursite.com/categories/flume/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[install hadoop cluster with cloudera manager]]></title>
    <link href="http://yoursite.com/blog/2015/07/03/install-hadoop-cluster-with-cloudera-manager/"/>
    <id>http://yoursite.com/blog/2015/07/03/install-hadoop-cluster-with-cloudera-manager/</id>
    <published>2015-07-03T01:36:30.000Z</published>
    <updated>2016-01-16T03:49:19.000Z</updated>
    <content type="html"><![CDATA[<p>如何利用 cloudera manager 安装 hadoop cdh 集群</p>
<a id="more"></a>
<p>cloudera manager 是一个可以批量管理hadoop 集群的工具，对于 hadoop cdh 包的安装，十分便捷。</p>
<hr>
<h2 id="u73AF_u5883_u51C6_u5907"><a href="#u73AF_u5883_u51C6_u5907" class="headerlink" title="环境准备"></a>环境准备</h2><ul>
<li>多台 centos6 x64 机器<ul>
<li>10.200.8.43  hadoop1 8G 154G 8CPU</li>
<li>10.200.8.44  hadoop2 16G 154G 8CPU</li>
<li>10.200.8.45  hadoop3 16G 1.5T 16CPU</li>
<li>10.200.8.46  hadoop4 16G 260G 16CPU</li>
<li>10.200.8.47  hadoop5 16G 260G 16CPU</li>
<li>10.200.8.48  hadoop6 16G 800G 8CPU</li>
</ul>
</li>
<li>其中一台部署 mysql 服务，我将 mysql装在 hadoop1 上</li>
<li>选1台机器 hadoop1 执行 <code>yum install cloudera-manager-server-db-2</code> 和 <code>yum install mysql-connector-java</code> 将近900M ，安装较慢</li>
</ul>
<hr>
<h2 id="u73AF_u5883_u914D_u7F6E"><a href="#u73AF_u5883_u914D_u7F6E" class="headerlink" title="环境配置"></a>环境配置</h2><p>1.修改每台机器的hostname，及 /etc/hosts</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sed -i <span class="string">'s/^HOSTNAME=.*$/HOSTNAME=hadoop1/g'</span> /etc/sysconfig/network</span><br><span class="line">hostname hadoop1</span><br><span class="line">service network restart</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">10.200</span>.<span class="number">8.43</span> hadoop1</span><br><span class="line"><span class="number">10.200</span>.<span class="number">8.44</span> hadoop2</span><br><span class="line"><span class="number">10.200</span>.<span class="number">8.45</span> hadoop3</span><br><span class="line"><span class="number">10.200</span>.<span class="number">8.46</span> hadoop4</span><br><span class="line"><span class="number">10.200</span>.<span class="number">8.47</span> hadoop5</span><br><span class="line"><span class="number">10.200</span>.<span class="number">8.48</span> hadoop6</span><br></pre></td></tr></table></figure>
<p>2.在安装mysql的机器hadoop1上，进入mysql，创建 cloudera manager (后面简称 cm )用户，由于cm用户会继续创建 hdfs oozie yarn 等用户，所以需要连带授权</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grant all on *.* to <span class="string">'cm'</span>@<span class="string">'%'</span> identified by <span class="string">'cm'</span> with grant option;</span><br></pre></td></tr></table></figure>
<p>3.初始化元信息到 mysql，会创建 scm数据库 （命令用法可查看 <code>/usr/share/cmf/schema/scm_prepare_database.sh -h</code>）</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/usr/share/cmf/schema/scm_prepare_database.sh -h hadoop1 -u cm -pcm mysql --scm-host hadoop1 scm cm cm</span><br></pre></td></tr></table></figure>
<p>4.检查创建后的mysql元信息配置文件</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop1 yuanxiaolong]<span class="comment"># cat /etc/cloudera-scm-server/db.properties</span></span><br><span class="line"><span class="comment"># Auto-generated by scm_prepare_database.sh on Wed Jul  1 16:09:25 CST 2015</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># For information describing how to configure the Cloudera Manager Server</span></span><br><span class="line"><span class="comment"># to connect to databases, see the "Cloudera Manager Installation Guide."</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line">com.cloudera.cmf.db.type=mysql</span><br><span class="line">com.cloudera.cmf.db.host=hadoop1</span><br><span class="line">com.cloudera.cmf.db.name=scm</span><br><span class="line">com.cloudera.cmf.db.user=cm</span><br><span class="line">com.cloudera.cmf.db.password=cm</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; select user,host from mysql.user;</span><br><span class="line">+------+-----------------------+</span><br><span class="line">| user | host                  |</span><br><span class="line">+------+-----------------------+</span><br><span class="line">| cm   | %                     |</span><br><span class="line">| root | <span class="number">127.0</span>.<span class="number">0.1</span>             |</span><br><span class="line">| root | ::<span class="number">1</span>                   |</span><br><span class="line">| cm   | hadoop1               |</span><br><span class="line">| root | localhost             |</span><br><span class="line">| root | localhost.localdomain |</span><br><span class="line">+------+-----------------------+</span><br><span class="line"><span class="number">6</span> rows <span class="keyword">in</span> <span class="built_in">set</span> (<span class="number">0.00</span> sec)</span><br></pre></td></tr></table></figure>
<p>5.检查进程是否已启动</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/etc/init.d/cloudera-scm-server start</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop1 yuanxiaolong]<span class="comment"># ps -ef | grep cloudera</span></span><br><span class="line">root     <span class="number">12648</span>     <span class="number">1</span>  <span class="number">0</span> <span class="number">16</span>:<span class="number">13</span> pts/<span class="number">2</span>    <span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span> su cloudera-scm <span class="operator">-s</span> /bin/bash -c nohup /usr/sbin/cmf-server</span><br><span class="line"><span class="number">498</span>      <span class="number">12650</span> <span class="number">12648</span> <span class="number">99</span> <span class="number">16</span>:<span class="number">13</span> ?        <span class="number">00</span>:<span class="number">03</span>:<span class="number">09</span> /usr/java/jdk1.<span class="number">7.0</span>_67-cloudera/bin/java -cp .:lib/*:/usr/share/java/mysql-connector-java.jar:/usr/share/java/oracle-connector-java.jar -server -D<span class="built_in">log</span>4j.configuration=file:/etc/cloudera-scm-server/<span class="built_in">log</span>4j.properties -Dfile.encoding=UTF-<span class="number">8</span> -Dcmf.root.logger=INFO,LOGFILE -Dcmf.log.dir=/var/<span class="built_in">log</span>/cloudera-scm-server -Dcmf.log.file=cloudera-scm-server.log -Dcmf.jetty.threshhold=WARN -Dcmf.schema.dir=/usr/share/cmf/schema -Djava.awt.headless=<span class="literal">true</span> -Djava.net.preferIPv4Stack=<span class="literal">true</span> -Dpython.home=/usr/share/cmf/python -XX:+UseConcMarkSweepGC -XX:-CMSConcurrentMTEnabled -XX:+UseParNewGC -XX:+HeapDumpOnOutOfMemoryError -Xmx2G -XX:MaxPermSize=<span class="number">256</span>m -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/tmp -XX:OnOutOfMemoryError=<span class="built_in">kill</span> -<span class="number">9</span> %p com.cloudera.server.cmf.Main</span><br></pre></td></tr></table></figure>
<p>6.进入web界面，第一次登陆会引导创建用户、密码</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">web &#22320;&#22336; &#65306; http://10.200.8.43:7180/cmf/login</span><br></pre></td></tr></table></figure>
<p><img src="/images/cloudera/20150703/1.png" alt=""></p>
<hr>
<h2 id="u914D_u7F6E_u96C6_u7FA4"><a href="#u914D_u7F6E_u96C6_u7FA4" class="headerlink" title="配置集群"></a>配置集群</h2><p>选择版本，由于我用6台做测试集群，因此选择免费版即可。根据自己需要选择 免费 or 收费</p>
<p><img src="/images/cloudera/20150703/2.png" alt=""></p>
<p><img src="/images/cloudera/20150703/3.png" alt=""></p>
<p>按hostname查找主机，正好验证刚才修改的hostname</p>
<p><img src="/images/cloudera/20150703/4.png" alt=""></p>
<p><img src="/images/cloudera/20150703/5.png" alt=""></p>
<p>下面就是安装 cdh 和 cm 了，根据自己需要选择安装类型。我避免线上安装太慢，因此预先下载好数据包，并通过离线安装数据包的方式安装。</p>
<p><a href="http://archive.cloudera.com/cdh5/repo-as-tarball/5.0.1/cdh5.0.1-centos6.tar.gz" target="_blank" rel="external"><font color="#78b00c">cdh5.0.1</font></a></p>
<p><a href="http://archive-primary.cloudera.com/cm5/repo-as-tarball/5.4.1/cm5.4.1-centos6.tar.gz" target="_blank" rel="external"><font color="#78b00c">cm5.4.1</font></a></p>
<p>由于我们 cloudera-manager-server（后面简称cms） 是通过 yum 安装的，因此很可能是最新版本，而最新的 cms 需要最新的 cm 来配合，不然会有兼容问题，安装不成功。到时候自己去网站获取cm的包</p>
<p>下载比较久 可以用 <code>screen</code> 命令 ，开启2个后台窗口，慢慢下载</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">screen -S &lt;name&gt; <span class="comment">#创建</span></span><br><span class="line">screen ctrl+A+D  <span class="comment">#退出</span></span><br><span class="line">screen -r &lt;name&gt; <span class="comment">#进入</span></span><br><span class="line">screen -ls       <span class="comment">#查看</span></span><br><span class="line">screen  ctrl + D <span class="comment">#中止</span></span><br></pre></td></tr></table></figure>
<p>可以在本地，安装一个 ngnix 去代理 静态目录，nginx 可能需要 编译安装 ，安装好后，修改配置</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">location / &#123;</span><br><span class="line">  root /data/clouderaManager/;</span><br><span class="line">  autoindex on;</span><br><span class="line">  index index.html index.htm</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sudo /usr/<span class="built_in">local</span>/nginx/sbin/nginx <span class="operator">-s</span> reload</span><br><span class="line"></span><br><span class="line">sudo /usr/<span class="built_in">local</span>/nginx/sbin/nginx -t</span><br><span class="line">nginx: the configuration file /usr/<span class="built_in">local</span>/nginx/conf/nginx.conf syntax is ok</span><br><span class="line">nginx: configuration file /usr/<span class="built_in">local</span>/nginx/conf/nginx.conf <span class="built_in">test</span> is successful</span><br></pre></td></tr></table></figure>
<p><img src="/images/cloudera/20150703/6.png" alt=""></p>
<p><img src="/images/cloudera/20150703/7.png" alt=""></p>
<hr>
<h2 id="u914D_u7F6Ecdh"><a href="#u914D_u7F6Ecdh" class="headerlink" title="配置cdh"></a>配置cdh</h2><p>安装完成后，就需要配置cdh了，首次安装会自动引导到如下界面。根据不同机器选择不同角色，一般Namenode机器需要选个较好的机器，且跟Mysql服务分开</p>
<p><img src="/images/cloudera/20150703/8.png" alt=""></p>
<p><img src="/images/cloudera/20150703/9.png" alt=""></p>
<p><img src="/images/cloudera/20150703/10.png" alt=""></p>
<p>给每个机器划分完角色（不同的服务，例如 hive oozie yarn 等）后，则需要为这些服务创建 mysql 数据库</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">grant all on *.* to <span class="string">'hive'</span>@<span class="string">'%'</span> identified by <span class="string">'hive'</span> with grant option;</span><br><span class="line">CREATE DATABASE <span class="keyword">if</span> not exists hive DEFAULT CHARACTER SET latin1;</span><br><span class="line"></span><br><span class="line">grant all on *.* to <span class="string">'am'</span>@<span class="string">'%'</span> identified by <span class="string">'am'</span> with grant option;</span><br><span class="line">CREATE DATABASE <span class="keyword">if</span> not exists activitymonitor DEFAULT CHARACTER SET utf8 COLLATE utf8_general_ci;</span><br><span class="line"></span><br><span class="line">grant all on *.* to <span class="string">'rm'</span>@<span class="string">'%'</span> identified by <span class="string">'rm'</span> with grant option;</span><br><span class="line">CREATE DATABASE <span class="keyword">if</span> not exists reportmanager DEFAULT CHARACTER SET utf8 COLLATE utf8_general_ci;</span><br><span class="line"></span><br><span class="line">grant all on *.* to <span class="string">'oozie'</span>@<span class="string">'%'</span> identified by <span class="string">'oozie'</span> with grant option;</span><br><span class="line">CREATE DATABASE <span class="keyword">if</span> not exists oozie DEFAULT CHARACTER SET utf8 COLLATE utf8_general_ci;</span><br></pre></td></tr></table></figure>
<p>由于安装服务的机器，不一定有 mysql-connector-java 的驱动，所以需要在每台机器上 运行 <code>sudo yum install mysql-connector-java</code> (由于cdh的安装跟apache 版本不一样，很多文件夹都是被 “打散” 了 ，当你不清楚文件路径时，这样最方便。如果清楚，则在对应 lib 文件夹下 放置一个 mysql 的驱动jar包即可)</p>
<p>没有驱动会报错<br><img src="/images/cloudera/20150703/11.png" alt=""></p>
<p>安装完成再测试<br><img src="/images/cloudera/20150703/12.png" alt=""></p>
<p>继续配置，一般默认即可<br><img src="/images/cloudera/20150703/13.png" alt=""><br><img src="/images/cloudera/20150703/14.png" alt=""><br><img src="/images/cloudera/20150703/15.png" alt=""></p>
<p>注意：这里关于数据目录需要说明一下，图中cdh检测后给出的是 /data/dfs/dn ，别人安装的时候可能不是这个路径，为什么呢？</p>
<p>因为我的机器只有一个磁盘，当有多个磁盘时，为了让所有磁盘都存放hdfs数据，则这里就是多个目录（以后的角色组就是这样的概念）。<br><img src="/images/cloudera/20150703/16.png" alt=""></p>
<p>继续配置<br><img src="/images/cloudera/20150703/17.png" alt=""></p>
<p><img src="/images/cloudera/20150703/18.png" alt=""></p>
<p>恭喜你，所有安装成功了！</p>
<hr>
<h2 id="u67E5_u6F0F_u8865_u7F3A"><a href="#u67E5_u6F0F_u8865_u7F3A" class="headerlink" title="查漏补缺"></a>查漏补缺</h2><h3 id="u95EE_u9898_u4E00_uFF1Aoozie__u7684_web_UI__u7F3A_u5C11extjs_u800C_u65E0_u6CD5_u8BBF_u95EE_u7684_u95EE_u9898"><a href="#u95EE_u9898_u4E00_uFF1Aoozie__u7684_web_UI__u7F3A_u5C11extjs_u800C_u65E0_u6CD5_u8BBF_u95EE_u7684_u95EE_u9898" class="headerlink" title="问题一：oozie 的 web UI 缺少extjs而无法访问的问题"></a><font color="#cca02e">问题一：oozie 的 web UI 缺少extjs而无法访问的问题</font></h3><p><img src="/images/cloudera/20150703/19.png" alt=""></p>
<p>解决：下载extjs-2.2 解压并放到部署 oozie 机器 /var/lib/oozie 目录下机器 （我的百度云地址 <a href="http://pan.baidu.com/s/1hqInECk" target="_blank" rel="external"><font color="#78b00c">共享extjs 2.2</font></a>）再次访问即可</p>
<p><img src="/images/cloudera/20150703/20.png" alt=""></p>
<h3 id="u95EE_u9898_u4E8C_uFF1Ahive_ddl__u64CD_u4F5C_u5931_u8D25_uFF0C_u65E0_u8BBA_u662F_u5426_u901A_u8FC7HUE"><a href="#u95EE_u9898_u4E8C_uFF1Ahive_ddl__u64CD_u4F5C_u5931_u8D25_uFF0C_u65E0_u8BBA_u662F_u5426_u901A_u8FC7HUE" class="headerlink" title="问题二：hive ddl 操作失败，无论是否通过HUE"></a><font color="#cca02e">问题二：hive ddl 操作失败，无论是否通过HUE</font></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">MetaException(message:javax.jdo.JDODataStoreException: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near &#39;OPTION SQL_SELECT_LIMIT=DEFAULT&#39; at line 1</span><br></pre></td></tr></table></figure>
<p>原因，安装了高版本的 mysql , 使用了低版本的 mysql-connector-java 驱动</p>
<p>解决：下载最新驱动 放到 “Hive Metastore” 的机器路径 <code>/usr/lib/hive/lib/mysql-connector-java-5.1.35.jar</code> 下，然后</p>
<p>重启 hive metastore 和 hive server 2 及 HUE ，即可</p>
<h3 id="u95EE_u9898_u4E09_uFF1A_u7528_u6237_u6743_u9650_u8BBE_u7F6E"><a href="#u95EE_u9898_u4E09_uFF1A_u7528_u6237_u6743_u9650_u8BBE_u7F6E" class="headerlink" title="问题三：用户权限设置"></a><font color="#cca02e">问题三：用户权限设置</font></h3><p>默认hdfs的 超级用户 和 超级组 为 hdfs 和 supergroup ，登录 liunx 机的用户，必须在 supergroup 组里，才能操作 hdfs ，因此需要</p>
<p>1.如果没有supergroup组，则创建supergroup组，在linux机器上 <code>groupadd supergroup</code> ，并查看 /etc/group 来确认</p>
<p>2.添加当前linux机器用户，至supergroup组里 <code>sudo usermod -a -G supergroup yuanxiaolong</code> ，并查看 /etc/passwd 来确认</p>
<p>3.配置hdfs授权用户 “群集”-&gt;”HDFS”-&gt;”配置”-&gt;”服务范围”-&gt;”安全性”<br><img src="/images/cloudera/20150703/21.png" alt=""></p>
<p>4.推送客户端配置</p>
<p>通过hue查询hive，或提交Job 所用的账号名称，是需要在 linux 上真实存在的，hue仅仅是个UI，所有权限认证都是在 Linux 和 hadoop 之间完成的。所以如果hue的用户，在Linux上不存在，但又想<br>操作hadoop，则需要用 <code>adduser &lt;yourname&gt;</code> 添加用户。</p>
<h3 id="u95EE_u9898_u56DB_uFF1A_u65B0_u589E_u8282_u70B9_u65E0_u6CD5_u52A0_u5165_u5230_hdfs__u548C_yarn__u91CC"><a href="#u95EE_u9898_u56DB_uFF1A_u65B0_u589E_u8282_u70B9_u65E0_u6CD5_u52A0_u5165_u5230_hdfs__u548C_yarn__u91CC" class="headerlink" title="问题四：新增节点无法加入到 hdfs 和 yarn 里"></a><font color="#cca02e">问题四：新增节点无法加入到 hdfs 和 yarn 里</font></h3><p>通常需要先在 CM 里执行 “重新授权” ，然后对应 hdfs 或 yarn 执行后，再重启</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfsadmin -refreshNodes</span><br><span class="line">yarn rmadmin -refreshNodes</span><br></pre></td></tr></table></figure>
<hr>
<p>PS：感谢张海波同学的指导<br>参考资料： <a href="http://www.cnblogs.com/jasondan/p/4011153.html" target="_blank" rel="external">http://www.cnblogs.com/jasondan/p/4011153.html</a></p>
]]></content>
    <summary type="html">
    <![CDATA[cloudera manager 安装 hadoop 集群]]>
    
    </summary>
    
      <category term="hadoop" scheme="http://yoursite.com/tags/hadoop/"/>
    
      <category term="hadoop" scheme="http://yoursite.com/categories/hadoop/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[hadoop lzo for hive]]></title>
    <link href="http://yoursite.com/blog/2015/06/28/hadoop-lzo-for-hive/"/>
    <id>http://yoursite.com/blog/2015/06/28/hadoop-lzo-for-hive/</id>
    <published>2015-06-28T13:33:02.000Z</published>
    <updated>2016-01-16T04:16:47.000Z</updated>
    <content type="html"><![CDATA[<p>hadoop hdfs 的压缩</p>
<a id="more"></a>
<p>当集群的hdfs增长速度很快时，需要对hdfs进行压缩，以减少硬件资金开销。</p>
<p>本文结合网上文章、已上某大型互联网公司生产环境同事经验，及本地测试验证，总结出此文。</p>
<hr>
<h2 id="u80CC_u666F"><a href="#u80CC_u666F" class="headerlink" title="背景"></a>背景</h2><p>集群目前有57台机器，共1.2 PB数据，为了减少在硬件上的资金开销，需要对现有的数据进行压缩。</p>
<ul>
<li>对load到hive前的 hdfs 文件，采取保留最近6个月内的数据，6-3个月内的数据 访问概率小，采用 bzip2 压缩</li>
<li>对load到hive里的 hdfs 文件，采取近乎全量lzo压缩</li>
</ul>
<h3 id="u538B_u7F29_u7B97_u6CD5_u662F_u5982_u4F55_u9009_u62E9_u7684_uFF1F"><a href="#u538B_u7F29_u7B97_u6CD5_u662F_u5982_u4F55_u9009_u62E9_u7684_uFF1F" class="headerlink" title="压缩算法是如何选择的？"></a><font color="#20afb4"> 压缩算法是如何选择的？</font></h3><ol>
<li><p>首先，load hive前的数据，使用的较少，一般都是对hive表进行统计操作，但也有可能会有脱离 “正规套路” 的现象，例如外界有某个脚本或客户端，读取整个目录的情况，要尽量“浓缩”且“兼容”，只能bzip2了，可以压缩10倍空间，且可以有Linux命令，解压文件，以便check源文件。</p>
</li>
<li><p>其次，load hive里的数据，使用的较多，压缩率和处理速度是成反比的，因此需要选择一个适中的算法，lzo 和 snappy 都是合适的，但业界一般都用lzo，因此就选择了lzo。（其实如果有充分时间，可以对lzo和snappy做对比的）lzo 压缩后能少 60%-70%</p>
</li>
</ol>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">压缩格式	split	native	压缩率	速度	是否hadoop自带	linux命令	换成压缩格式后，原来的应用程序是否要修改</span><br><span class="line">gzip	否	是	很高	比较快	是，直接使用	有	        和文本处理一样，不需要修改</span><br><span class="line">lzo	是	是	比较高	很快	否，需要安装	有	        需要建索引，还需要指定输入格式</span><br><span class="line">snappy	否	是	比较高	很快	否，需要安装	没有	        和文本处理一样，不需要修改</span><br><span class="line">bzip2	是	否	最高	慢	是，直接使用	有	        和文本处理一样，不需要修改</span><br></pre></td></tr></table></figure>
<h3 id="u751F_u4EA7_u73AF_u5883_u9002_u5408_u7528lzo_u4E48_uFF1F"><a href="#u751F_u4EA7_u73AF_u5883_u9002_u5408_u7528lzo_u4E48_uFF1F" class="headerlink" title="生产环境适合用lzo么？"></a><font color="#20afb4">生产环境适合用lzo么？</font></h3><p>适合，但需要做处理。普通的hdfs文件是textfile，由于lzo不支持spilt，即一个文件不能被多个map并行处理，因此需要对textfile创建lzo索引，以支持spilt。但是生产环境，这种方式不适用，</p>
<p>每天每个hive表的分区目录下，都有很多文件，如果还要维护一个定时任务去创建索引，代价太大，也不方便。因此，我们可以用 sequeneceFile 的方式，用block做存储，就可以split的。</p>
<p>需要做的只有2步，首先，按分区lzo sequenceFile 压缩原始hive表的数据。其次，按sequenceFile创建hive表（STORED AS SEQUENCEFILE），再导入压缩后的数据。是不是更简单？</p>
<hr>
<h2 id="bzip2__u538B_u7F29"><a href="#bzip2__u538B_u7F29" class="headerlink" title="bzip2 压缩"></a>bzip2 压缩</h2><p>建议用 hadoop streaming 的方式来做（下面是HUE中的配置）,方便 代码少</p>
<p><img src="/images/hadoop/20150628/1.png" alt=""><br><img src="/images/hadoop/20150628/2.png" alt=""></p>
<hr>
<h2 id="lzo__u73AF_u5883_u51C6_u5907"><a href="#lzo__u73AF_u5883_u51C6_u5907" class="headerlink" title="lzo 环境准备"></a>lzo 环境准备</h2><h3 id="u4E0B_u8F7D_u3001_u89E3_u538B_u5E76_u7F16_u8BD1lzo_u5305"><a href="#u4E0B_u8F7D_u3001_u89E3_u538B_u5E76_u7F16_u8BD1lzo_u5305" class="headerlink" title="下载、解压并编译lzo包"></a><font color="#c39d2c">下载、解压并编译lzo包</font></h3><p>(我的百度云备用地址 <a href="http://pan.baidu.com/s/1qW7N6ws" target="_blank" rel="external"><font color="#465999">http://pan.baidu.com/s/1qW7N6ws</font></a>)</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">wget http://www.oberhumer.com/opensource/lzo/download/lzo-<span class="number">2.06</span>.tar.gz</span><br><span class="line">tar -zxvf lzo-<span class="number">2.06</span>.tar.gz</span><br><span class="line"><span class="built_in">cd</span> lzo-<span class="number">2.06</span></span><br><span class="line"><span class="built_in">export</span> CFLAGS=-m64</span><br><span class="line">./configure -enable-shared -prefix=/usr/<span class="built_in">local</span>/hadoop/lzo/</span><br><span class="line">make &amp;&amp; sudo make install</span><br></pre></td></tr></table></figure>
<p>编译完lzo包之后，会在<code>/usr/local/hadoop/lzo/</code>生成一些文件，目录结构如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ls <span class="operator">-l</span> /usr/<span class="built_in">local</span>/hadoop/lzo/</span><br><span class="line">drwxr-xr-x <span class="number">3</span> root root <span class="number">4096</span> Mar <span class="number">21</span> <span class="number">17</span>:<span class="number">23</span> include</span><br><span class="line">drwxr-xr-x <span class="number">2</span> root root <span class="number">4096</span> Mar <span class="number">21</span> <span class="number">17</span>:<span class="number">23</span> lib</span><br><span class="line">drwxr-xr-x <span class="number">3</span> root root <span class="number">4096</span> Mar <span class="number">21</span> <span class="number">17</span>:<span class="number">23</span> share</span><br></pre></td></tr></table></figure>
<p>将/usr/local/hadoop/lzo目录下的所有文件打包，并同步到集群中的所有机器上。</p>
<p>在编译lzo包的时候，需要一些环境，可以用下面的命令安装好lzo编译环境<code>yum -y install  lzo-devel zlib-devel  gcc autoconf automake libtool</code></p>
<h3 id="u7F16_u8BD1_u5B89_u88C5Hadoop-LZO"><a href="#u7F16_u8BD1_u5B89_u88C5Hadoop-LZO" class="headerlink" title="编译安装Hadoop-LZO"></a><font color="#c39d2c">编译安装Hadoop-LZO</font></h3><p>下载Twitter hadoop-lzo</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/twitter/hadoop-lzo.git</span><br></pre></td></tr></table></figure>
<p>修改hadoop版本</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="title">properties</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="title">project.build.sourceEncoding</span>&gt;</span>UTF-8<span class="tag">&lt;/<span class="title">project.build.sourceEncoding</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="title">hadoop.current.version</span>&gt;</span>2.3.0<span class="tag">&lt;/<span class="title">hadoop.current.version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="title">hadoop.old.version</span>&gt;</span>1.0.4<span class="tag">&lt;/<span class="title">hadoop.old.version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">properties</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>编译，准备Hadoop lzo 环境</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> CFLAGS=-m64</span><br><span class="line"><span class="built_in">export</span> CXXFLAGS=-m64</span><br><span class="line"><span class="built_in">export</span> C_INCLUDE_PATH=/usr/<span class="built_in">local</span>/hadoop/lzo/include</span><br><span class="line"><span class="built_in">export</span> LIBRARY_PATH=/usr/<span class="built_in">local</span>/hadoop/lzo/lib</span><br><span class="line">mvn clean package -Dmaven.test.skip=<span class="literal">true</span></span><br><span class="line"><span class="built_in">cd</span> target/native/Linux-amd64-<span class="number">64</span></span><br><span class="line">tar -cBf - -C lib . | tar -xBvf - -C ~</span><br><span class="line">cp ~/libgplcompression* <span class="variable">$HADOOP_HOME</span>/lib/native/</span><br><span class="line">cp target/hadoop-lzo-<span class="number">0.4</span>.<span class="number">20</span>-SNAPSHOT.jar  <span class="variable">$HADOOP_HOME</span>/share/hadoop/common/</span><br></pre></td></tr></table></figure>
<p>在<code>tar -cBf - -C lib . | tar -xBvf - -C ~</code>命令之后，会在~目录下生成一下几个文件(即打包目录的东西并解压)</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">ls <span class="operator">-l</span></span><br><span class="line">-rw-r--r--  <span class="number">1</span> libgplcompression.a</span><br><span class="line">-rw-r--r--  <span class="number">1</span> libgplcompression.la</span><br><span class="line">lrwxrwxrwx  <span class="number">1</span> libgplcompression.so -&gt; libgplcompression.so.<span class="number">0.0</span>.<span class="number">0</span></span><br><span class="line">lrwxrwxrwx  <span class="number">1</span> libgplcompression.so.<span class="number">0</span> -&gt; libgplcompression.so.<span class="number">0.0</span>.<span class="number">0</span></span><br><span class="line">-rwxr-xr-x  <span class="number">1</span> libgplcompression.so.<span class="number">0.0</span>.<span class="number">0</span></span><br></pre></td></tr></table></figure>
<p>其中 <code>libgplcompression.so</code> 和 <code>libgplcompression.so.0</code> 是链接文件，指向 <code>libgplcompression.so.0.0.0</code><br>将刚刚生成的 <code>libgplcompression*</code> 和 <code>target/hadoop-lzo-0.4.20-SNAPSHOT.jar</code> 同步到集群中的所有机器对应的目录。</p>
<h3 id="u914D_u7F6EHadoop_u73AF_u5883_u53D8_u91CF"><a href="#u914D_u7F6EHadoop_u73AF_u5883_u53D8_u91CF" class="headerlink" title="配置Hadoop环境变量"></a>配置Hadoop环境变量</h3><p>1、在Hadoop中的<code>$HADOOP_HOME/etc/hadoop/hadoop-env.sh</code>加上配置 <code>export LD_LIBRARY_PATH=/usr/local/hadoop/lzo/lib</code></p>
<p>2、在<code>$HADOOP_HOME/etc/hadoop/core-site.xml</code>加上如下配置：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">name</span>&gt;</span>io.compression.codecs<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">value</span>&gt;</span></span><br><span class="line">     org.apache.hadoop.io.compress.GzipCodec,</span><br><span class="line">     org.apache.hadoop.io.compress.DefaultCodec,</span><br><span class="line">     com.hadoop.compression.lzo.LzoCodec,</span><br><span class="line">     com.hadoop.compression.lzo.LzopCodec,</span><br><span class="line">     org.apache.hadoop.io.compress.BZip2Codec</span><br><span class="line">  <span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">name</span>&gt;</span>io.compression.codec.lzo.class<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">value</span>&gt;</span>com.hadoop.compression.lzo.LzoCodec<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>3、在<code>$HADOOP_HOME/etc/hadoop/mapred-site.xml</code>加上如下配置</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">name</span>&gt;</span>mapred.compress.map.output<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">value</span>&gt;</span>true<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">name</span>&gt;</span>mapred.map.output.compression.codec<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">value</span>&gt;</span>com.hadoop.compression.lzo.LzoCodec<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">name</span>&gt;</span>mapred.child.env<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">value</span>&gt;</span>LD_LIBRARY_PATH=/usr/local/hadoop/lzo/lib<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>将刚刚修改的配置文件全部同步到集群的所有机器上，并重启Hadoop集群（如果用CM来管理，则可以不用重启），这样就可以在Hadoop中使用lzo。</p>
<h3 id="u9700_u8981_u6CE8_u610F_u7684_u51E0_u70B9"><a href="#u9700_u8981_u6CE8_u610F_u7684_u51E0_u70B9" class="headerlink" title="需要注意的几点"></a><font color="#c39d2c">需要注意的几点</font></h3><p>集群机器的环境变量</p>
<p>1.建立<code>mkdir -p /usr/local/hadoop/lzo</code> 文件夹存放library，并修改文件夹权限 <code>chmod -R 777  /usr/local/hadoop/lzo</code></p>
<p>2.cloudera manager 的 hadoop 依赖的 本地库 路径 <code>/usr/lib/hadoop/lib/native/</code> ，并把刚才编译得到的 lzo 本地库 放置 <code>/usr/lib/hadoop/lib/native/libgplcompression*</code><br>并 <code>chmod 777</code>，同理把编译好的 lzo jar 放置到 <code>/usr/lib/hadoop/hadoop-lzo-0.4.20-SNAPSHOT.jar</code> 并修改权限。</p>
<p>3.建立全局变量 ，不能设置到 <code>.bash_profile</code>里，因为此变量不跟用户挂钩.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">"export LD_LIBRARY_PATH=/usr/local/hadoop/lzo/lib"</span> &gt;&gt; /etc/profile</span><br><span class="line"><span class="built_in">source</span> /etc/profile</span><br></pre></td></tr></table></figure>
<p>4.如果集群有 spark on yarn ，则还需要设置 spark 让其知道 lzo 环境</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">"export SPARK_LIBRARY_PATH=<span class="variable">$SPARK_LIBRARY_PATH</span>:/usr/lib/hadoop/lib/native/"</span> &gt;&gt; /etc/profile</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"export SPARK_SUBMIT_LIBRARY_PATH=<span class="variable">$SPARK_SUBMIT_LIBRARY_PATH</span>:/usr/lib/hadoop/lib/native/"</span> &gt;&gt; /etc/profile</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"export SPARK_CLASSPATH=<span class="variable">$SPARK_CLASSPATH</span>:/usr/lib/hadoop/hadoop-lzo-0.4.20-SNAPSHOT.jar"</span> &gt;&gt; /etc/profile</span><br><span class="line"><span class="built_in">source</span> /etc/profile</span><br></pre></td></tr></table></figure>
<p>5.如果是通过 HUE 查询 hive 压缩表数据,则需要配置 HiveServer2 的环境变量, 我是通过 cloudera manager 管理的，所以在 “HiveServer2 环境高级配置代码段” 里配置即可。附cdh issue <a href="https://github.com/cloudera/hue/issues/204" target="_blank" rel="external">地址</a></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">LD_LIBRARY_PATH=/usr/<span class="built_in">local</span>/hadoop/lzo/lib</span><br></pre></td></tr></table></figure>
<hr>
<h2 id="u538B_u7F29_u6838_u5FC3_u4EE3_u7801"><a href="#u538B_u7F29_u6838_u5FC3_u4EE3_u7801" class="headerlink" title="压缩核心代码"></a>压缩核心代码</h2><p>利用 hadoop api 的方式 ，用 hadoop jar <your jar=""> 来进行压缩，不用写 MR 更方便</your></p>
<figure class="highlight xml"><figcaption><span>pom.xml</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br></pre></td><td class="code"><pre><span class="line"><span class="pi">&lt;?xml version="1.0" encoding="UTF-8"?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">project</span> <span class="attribute">xmlns</span>=<span class="value">"http://maven.apache.org/POM/4.0.0"</span></span><br><span class="line">         <span class="attribute">xmlns:xsi</span>=<span class="value">"http://www.w3.org/2001/XMLSchema-instance"</span></span><br><span class="line">         <span class="attribute">xsi:schemaLocation</span>=<span class="value">"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">modelVersion</span>&gt;</span>4.0.0<span class="tag">&lt;/<span class="title">modelVersion</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="title">groupId</span>&gt;</span>com.test<span class="tag">&lt;/<span class="title">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">artifactId</span>&gt;</span>hadoop-lzo<span class="tag">&lt;/<span class="title">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">version</span>&gt;</span>0.0.1<span class="tag">&lt;/<span class="title">version</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="title">packaging</span>&gt;</span>jar<span class="tag">&lt;/<span class="title">packaging</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="title">name</span>&gt;</span>hadoop-lzo<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">description</span>&gt;</span>compress hadoop hdfs and reimport into hive table<span class="tag">&lt;/<span class="title">description</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="title">dependencies</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">&lt;!-- local compile lzo jar --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="title">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="title">groupId</span>&gt;</span>com.test.lzo<span class="tag">&lt;/<span class="title">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="title">artifactId</span>&gt;</span>hadoop-lzo<span class="tag">&lt;/<span class="title">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="title">version</span>&gt;</span>0.4.20<span class="tag">&lt;/<span class="title">version</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="title">scope</span>&gt;</span>system<span class="tag">&lt;/<span class="title">scope</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="title">systemPath</span>&gt;</span>$&#123;project.basedir&#125;/hadoop-lzo-0.4.20-SNAPSHOT.jar<span class="tag">&lt;/<span class="title">systemPath</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="title">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">&lt;!-- hadoop --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="title">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="title">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="title">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="title">artifactId</span>&gt;</span>hadoop-common<span class="tag">&lt;/<span class="title">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="title">version</span>&gt;</span>2.3.0<span class="tag">&lt;/<span class="title">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="title">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="title">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="title">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="title">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="title">artifactId</span>&gt;</span>hadoop-hdfs<span class="tag">&lt;/<span class="title">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="title">version</span>&gt;</span>2.3.0<span class="tag">&lt;/<span class="title">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="title">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="title">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="title">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="title">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="title">artifactId</span>&gt;</span>hadoop-client<span class="tag">&lt;/<span class="title">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="title">version</span>&gt;</span>2.3.0<span class="tag">&lt;/<span class="title">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="title">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">&lt;!-- Logging --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="title">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="title">groupId</span>&gt;</span>org.slf4j<span class="tag">&lt;/<span class="title">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="title">artifactId</span>&gt;</span>slf4j-api<span class="tag">&lt;/<span class="title">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="title">version</span>&gt;</span>1.7.2<span class="tag">&lt;/<span class="title">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="title">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="title">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="title">groupId</span>&gt;</span>log4j<span class="tag">&lt;/<span class="title">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="title">artifactId</span>&gt;</span>log4j<span class="tag">&lt;/<span class="title">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="title">version</span>&gt;</span>1.2.16<span class="tag">&lt;/<span class="title">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="title">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="title">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="title">groupId</span>&gt;</span>org.slf4j<span class="tag">&lt;/<span class="title">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="title">artifactId</span>&gt;</span>slf4j-log4j12<span class="tag">&lt;/<span class="title">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="title">version</span>&gt;</span>1.7.2<span class="tag">&lt;/<span class="title">version</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="title">exclusions</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="title">exclusion</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="title">groupId</span>&gt;</span>log4j<span class="tag">&lt;/<span class="title">groupId</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="title">artifactId</span>&gt;</span>log4j<span class="tag">&lt;/<span class="title">artifactId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="title">exclusion</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="title">exclusions</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="title">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;/<span class="title">dependencies</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="title">build</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--&lt;finalName&gt;hadoop-lzo&lt;/finalName&gt;--&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="title">resources</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="title">resource</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="title">directory</span>&gt;</span>src/main/resources<span class="tag">&lt;/<span class="title">directory</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="title">includes</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="title">include</span>&gt;</span>**/*<span class="tag">&lt;/<span class="title">include</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="title">includes</span>&gt;</span></span><br><span class="line">                <span class="comment">&lt;!-- 允许使用变量替换资源文件 --&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="title">filtering</span>&gt;</span>true<span class="tag">&lt;/<span class="title">filtering</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="title">resource</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="title">resources</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="title">pluginManagement</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="title">plugins</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="title">plugin</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="title">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="title">groupId</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="title">artifactId</span>&gt;</span>maven-compiler-plugin<span class="tag">&lt;/<span class="title">artifactId</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="title">version</span>&gt;</span>2.3.2<span class="tag">&lt;/<span class="title">version</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="title">plugin</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="title">plugin</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="title">artifactId</span>&gt;</span>maven-compiler-plugin<span class="tag">&lt;/<span class="title">artifactId</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="title">configuration</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="title">source</span>&gt;</span>1.6<span class="tag">&lt;/<span class="title">source</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="title">target</span>&gt;</span>1.6<span class="tag">&lt;/<span class="title">target</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="title">encoding</span>&gt;</span>UTF-8<span class="tag">&lt;/<span class="title">encoding</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="title">configuration</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="title">plugin</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="title">plugins</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="title">pluginManagement</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="title">plugins</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="title">plugin</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="title">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="title">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="title">artifactId</span>&gt;</span>maven-surefire-plugin<span class="tag">&lt;/<span class="title">artifactId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="title">configuration</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="title">skipTests</span>&gt;</span>true<span class="tag">&lt;/<span class="title">skipTests</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="title">configuration</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="title">plugin</span>&gt;</span></span><br><span class="line"></span><br><span class="line">            <span class="tag">&lt;<span class="title">plugin</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="title">artifactId</span>&gt;</span>maven-source-plugin<span class="tag">&lt;/<span class="title">artifactId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="title">version</span>&gt;</span>2.1<span class="tag">&lt;/<span class="title">version</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="title">configuration</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="title">attach</span>&gt;</span>true<span class="tag">&lt;/<span class="title">attach</span>&gt;</span></span><br><span class="line">                    <span class="comment">&lt;!--&lt;encoding&gt;UTF-8&lt;/encoding&gt;--&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="title">configuration</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="title">executions</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="title">execution</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="title">phase</span>&gt;</span>compile<span class="tag">&lt;/<span class="title">phase</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="title">goals</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="title">goal</span>&gt;</span>jar<span class="tag">&lt;/<span class="title">goal</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="title">goals</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="title">execution</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="title">executions</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="title">plugin</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="title">plugin</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="title">artifactId</span>&gt;</span>maven-javadoc-plugin<span class="tag">&lt;/<span class="title">artifactId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="title">version</span>&gt;</span>2.1<span class="tag">&lt;/<span class="title">version</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="title">configuration</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="title">attach</span>&gt;</span>true<span class="tag">&lt;/<span class="title">attach</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="title">encoding</span>&gt;</span>UTF-8<span class="tag">&lt;/<span class="title">encoding</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="title">configuration</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="title">executions</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="title">execution</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="title">phase</span>&gt;</span>compile<span class="tag">&lt;/<span class="title">phase</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="title">goals</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="title">goal</span>&gt;</span>jar<span class="tag">&lt;/<span class="title">goal</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="title">goals</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="title">execution</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="title">executions</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="title">plugin</span>&gt;</span></span><br><span class="line"></span><br><span class="line">            <span class="tag">&lt;<span class="title">plugin</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="title">artifactId</span>&gt;</span>maven-assembly-plugin<span class="tag">&lt;/<span class="title">artifactId</span>&gt;</span></span><br><span class="line">                <span class="comment">&lt;!-- 添加上版本,如果去掉此version,则jar包里会出现重复的xml --&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="title">version</span>&gt;</span>2.2-beta-5<span class="tag">&lt;/<span class="title">version</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="title">configuration</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="title">appendAssemblyId</span>&gt;</span>false<span class="tag">&lt;/<span class="title">appendAssemblyId</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="title">descriptorRefs</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="title">descriptorRef</span>&gt;</span>jar-with-dependencies<span class="tag">&lt;/<span class="title">descriptorRef</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="title">descriptorRefs</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="title">archive</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="title">manifest</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="title">mainClass</span>&gt;</span>com.test.Main<span class="tag">&lt;/<span class="title">mainClass</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="title">manifest</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="title">archive</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="title">configuration</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="title">executions</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="title">execution</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="title">id</span>&gt;</span>make-assembly<span class="tag">&lt;/<span class="title">id</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="title">phase</span>&gt;</span>package<span class="tag">&lt;/<span class="title">phase</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="title">goals</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="title">goal</span>&gt;</span>assembly<span class="tag">&lt;/<span class="title">goal</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="title">goals</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="title">execution</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="title">executions</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="title">plugin</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;/<span class="title">plugins</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="title">build</span>&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="title">project</span>&gt;</span></span><br></pre></td></tr></table></figure>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FSDataInputStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileStatus;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.BytesWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IOUtils;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.SequenceFile;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.compress.CompressionCodec;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.ReflectionUtils;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String LZO = <span class="string">"com.hadoop.compression.lzo.LzoCodec"</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> String FS = <span class="string">"hdfs://localhost:8020"</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">compress</span><span class="params">(String inputPath, String outputPath)</span> </span>&#123;</span><br><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">        conf.set(<span class="string">"fs.defaultFS"</span>, FS);</span><br><span class="line">        FSDataInputStream inputStream = <span class="keyword">null</span>;</span><br><span class="line">        SequenceFile.Writer writer = <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            FileSystem fs = FileSystem.get(conf);</span><br><span class="line">            Path input = <span class="keyword">new</span> Path(inputPath);</span><br><span class="line">            inputStream = fs.open(input);</span><br><span class="line">            FileStatus stat = fs.getFileStatus(input);</span><br><span class="line"></span><br><span class="line">            BufferedReader buff = <span class="keyword">new</span> BufferedReader(<span class="keyword">new</span> InputStreamReader(inputStream));</span><br><span class="line"></span><br><span class="line">            BytesWritable EMPTY_KEY = <span class="keyword">new</span> BytesWritable();<span class="comment">//   key</span></span><br><span class="line"><span class="comment">//            Text value = new Text();</span></span><br><span class="line"></span><br><span class="line">            Path seqFile = <span class="keyword">new</span> Path(outputPath);</span><br><span class="line">            CompressionCodec codec = (CompressionCodec) ReflectionUtils.newInstance(Class.forName(LZO), conf);</span><br><span class="line">            writer = SequenceFile.createWriter(conf,</span><br><span class="line">                    SequenceFile.Writer.file(seqFile), SequenceFile.Writer.keyClass(BytesWritable.class),</span><br><span class="line">                    SequenceFile.Writer.valueClass(Text.class),</span><br><span class="line">                    SequenceFile.Writer.compression(SequenceFile.CompressionType.BLOCK, codec));</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">            String str = <span class="string">""</span>;</span><br><span class="line">            System.out.println(<span class="string">"begin"</span>);</span><br><span class="line">            <span class="keyword">while</span> ((str = buff.readLine()) != <span class="keyword">null</span>) &#123;</span><br><span class="line">                writer.append(EMPTY_KEY, <span class="keyword">new</span> Text(str));</span><br><span class="line">            &#125;</span><br><span class="line">            System.out.println(<span class="string">"done"</span>);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">            IOUtils.closeStream(inputStream);</span><br><span class="line">            IOUtils.closeStream(writer);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<h3 id="u6CE8_u610F"><a href="#u6CE8_u610F" class="headerlink" title="注意"></a><font color="#c39d2c">注意</font></h3><p>SequenceFile 是 key-value 型格式，即使 key 不输出任何值，也会 “占位”，所以压缩完文件 可以利用下面命令 看一下，第一列会有一列空占位</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -text &lt;yourlzofile&gt; | head</span><br></pre></td></tr></table></figure>
<p>判断一个文件是否是 SequenceFile ， 可以新建 hive 表 store as SEQUENCEFILE ，然后 类似这样 load 进去</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">load data inpath <span class="string">'hdfs://localhost:8020/data/xiaolong.yuanxl/lzo/'</span> overwrite into table yxl_<span class="built_in">test</span> partition(b=<span class="string">'pcc'</span>,year=<span class="string">'2015'</span>,month=<span class="string">'06'</span>,day=<span class="string">'17'</span>);</span><br></pre></td></tr></table></figure>
<hr>
<h2 id="u6D4B_u8BD5_u7ED3_u679C"><a href="#u6D4B_u8BD5_u7ED3_u679C" class="headerlink" title="测试结果"></a>测试结果</h2><p>数据准备：</p>
<ul>
<li>t1 数据文件 300M  压缩后 118M</li>
<li>t2 数据文件 800M压缩后 305M</li>
</ul>
<p>hql 测试示例：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- HQL(1)</span></span><br><span class="line"><span class="operator"><span class="keyword">select</span> * <span class="keyword">from</span> t2 <span class="keyword">left</span> <span class="keyword">outer</span> <span class="keyword">join</span> t1 <span class="keyword">on</span> t2.uid=t1.uid <span class="keyword">group</span> <span class="keyword">by</span> t2.<span class="keyword">uuid</span>,t2.ip;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">--HQL(2)</span></span><br><span class="line"><span class="operator"><span class="keyword">select</span> <span class="keyword">count</span>(<span class="keyword">distinct</span> t2.deviceid) <span class="keyword">from</span> t2 <span class="keyword">left</span> <span class="keyword">outer</span> <span class="keyword">join</span> t1 <span class="keyword">on</span> t2.uid=t1.uid <span class="keyword">group</span> <span class="keyword">by</span> t2.deviceid,t2.<span class="keyword">uuid</span>,t2.ip ;</span></span><br></pre></td></tr></table></figure>
<p>测试结果：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">表情况	HQL	Mapper 	Time</span><br><span class="line">均压	<span class="number">1</span>	<span class="number">2</span>	Time taken: <span class="number">83.887</span> seconds	 Fetched: <span class="number">920746</span> row(s)</span><br><span class="line">未压t1	<span class="number">1</span>	<span class="number">3</span>	Time taken: <span class="number">78.108</span> seconds	 Fetched: <span class="number">920746</span> row(s)</span><br><span class="line">均未压	<span class="number">1</span>	<span class="number">6</span>	Time taken: <span class="number">98.233</span> seconds	 Fetched: <span class="number">920746</span> row(s)</span><br><span class="line"></span><br><span class="line">均压	<span class="number">2</span>	<span class="number">3</span>	Time taken: <span class="number">100.466</span> seconds	 Fetched: <span class="number">606651</span> row(s)</span><br><span class="line">未压t1	<span class="number">2</span>	<span class="number">3</span>	Time taken: <span class="number">91.218</span> seconds	 Fetched: <span class="number">606651</span> row(s)</span><br><span class="line">均未压	<span class="number">2</span>	<span class="number">5</span>	Time taken: <span class="number">95.349</span> seconds	 Fetched: <span class="number">606651</span> row(s)</span><br></pre></td></tr></table></figure>
<font color="#9f2e4d">注意：数据文件压缩完后，尽量大一些，hadoop 2.3.0 600M的hdfs文件才会启动3个mapper，当然这个也跟环境有关系。</font>


<hr>
<p>参考资料：</p>
<ul>
<li><a href="http://www.iteblog.com/archives/992" target="_blank" rel="external"><font color="#465999"> lzo压缩1——环境准备及压缩 </font></a></li>
<li><a href="http://www.iteblog.com/archives/996" target="_blank" rel="external"><font color="#465999"> lzo压缩2——创建lzo索引 </font></a></li>
</ul>
<p>感谢同事 张龙 分享并指导经验，以至于少走很多弯路。</p>
]]></content>
    <summary type="html">
    <![CDATA[hadoop lzo 压缩]]>
    
    </summary>
    
      <category term="hadoop" scheme="http://yoursite.com/tags/hadoop/"/>
    
      <category term="hadoop" scheme="http://yoursite.com/categories/hadoop/"/>
    
  </entry>
  
</feed>
